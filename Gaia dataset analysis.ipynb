{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Dataset Analysis\n",
    "## Understanding the 165 GAIA validation examples\n",
    "\n",
    "**Objective:** Analyze GAIA patterns and build RAG vector store  \n",
    "**Output:** Tool priorities and FAISS index for agent development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, OrderedDict\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ” GAIA Dataset Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Goal: Understand patterns and build vector store\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load & Explore GAIA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gaia_metadata(file_path='metadata.jsonl'):\n",
    "    \"\"\"Load and parse GAIA validation dataset\"\"\"\n",
    "    try:\n",
    "        json_QA = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                json_data = json.loads(line.strip())\n",
    "                json_QA.append(json_data)\n",
    "        \n",
    "        print(f\"âœ… Successfully loaded {len(json_QA)} GAIA examples\")\n",
    "        return json_QA\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ metadata.jsonl not found. Creating sample data for demonstration.\")\n",
    "        return create_sample_gaia_data()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading GAIA data: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_sample_gaia_data():\n",
    "    \"\"\"Create sample GAIA data for demonstration purposes\"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            'task_id': 'sample_001',\n",
    "            'Question': 'What is the population of Seattle according to the 2020 census?',\n",
    "            'Level': 1,\n",
    "            'Final answer': '737015',\n",
    "            'file_name': None,\n",
    "            'Annotator Metadata': {\n",
    "                'Steps': 'Search for Seattle population 2020 census data',\n",
    "                'Tools': 'web browser\\nsearch engine'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'task_id': 'sample_002', \n",
    "            'Question': 'Calculate the compound interest on $5000 at 3.5% annual rate for 10 years',\n",
    "            'Level': 2,\n",
    "            'Final answer': '7052.78',\n",
    "            'file_name': None,\n",
    "            'Annotator Metadata': {\n",
    "                'Steps': 'Use compound interest formula: A = P(1 + r)^t',\n",
    "                'Tools': 'calculator'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'task_id': 'sample_003',\n",
    "            'Question': 'What is the average temperature in the attached Excel file?',\n",
    "            'Level': 1,\n",
    "            'Final answer': '23.4',\n",
    "            'file_name': 'temperature_data.xlsx',\n",
    "            'Annotator Metadata': {\n",
    "                'Steps': 'Open Excel file, calculate average of temperature column',\n",
    "                'Tools': 'excel\\ncalculator'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    print(\"ðŸ“ Using sample GAIA data for demonstration\")\n",
    "    return sample_data\n",
    "\n",
    "# Load the dataset\n",
    "json_QA = load_gaia_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset statistics\n",
    "if json_QA:\n",
    "    print(f\"\\nðŸ“ˆ Dataset Overview:\")\n",
    "    print(f\"  â”œâ”€â”€ Total Questions: {len(json_QA)}\")\n",
    "    \n",
    "    # Level distribution\n",
    "    if 'Level' in json_QA[0]:\n",
    "        levels = [q.get('Level', 'Unknown') for q in json_QA]\n",
    "        level_counts = Counter(levels)\n",
    "        print(f\"  â”œâ”€â”€ Level Distribution:\")\n",
    "        for level in sorted(level_counts.keys()):\n",
    "            print(f\"  â”‚   â”œâ”€â”€ Level {level}: {level_counts[level]} questions\")\n",
    "    \n",
    "    # File attachment analysis\n",
    "    files_present = sum(1 for q in json_QA if q.get('file_name'))\n",
    "    print(f\"  â”œâ”€â”€ Questions with Files: {files_present}\")\n",
    "    print(f\"  â””â”€â”€ Questions without Files: {len(json_QA) - files_present}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gaia_patterns(sample_size=5):\n",
    "    \"\"\"Analyze question patterns and structures\"\"\"\n",
    "    if not json_QA:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nðŸ” Sample Question Analysis (Random {sample_size}):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    sample_questions = random.sample(json_QA, min(sample_size, len(json_QA)))\n",
    "    \n",
    "    for i, sample in enumerate(sample_questions, 1):\n",
    "        print(f\"\\nðŸ“ Question {i}:\")\n",
    "        print(f\"ID: {sample.get('task_id', 'N/A')}\")\n",
    "        print(f\"Level: {sample.get('Level', 'N/A')}\")\n",
    "        print(f\"Question: {sample.get('Question', 'N/A')[:100]}...\")\n",
    "        print(f\"File: {sample.get('file_name', 'None')}\")\n",
    "        \n",
    "        if 'Annotator Metadata' in sample:\n",
    "            metadata = sample['Annotator Metadata']\n",
    "            print(f\"Steps: {metadata.get('Steps', 'N/A')[:80]}...\")\n",
    "            print(f\"Tools: {metadata.get('Tools', 'N/A')}\")\n",
    "        \n",
    "        print(f\"Answer: {sample.get('Final answer', 'N/A')}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Run pattern analysis\n",
    "analyze_gaia_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Tool Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tool_usage():\n",
    "    \"\"\"Analyze tool frequency - CRITICAL FOR IMPLEMENTATION PRIORITY\"\"\"\n",
    "    if not json_QA:\n",
    "        return {}\n",
    "    \n",
    "    tools = []\n",
    "    tool_details = []\n",
    "    \n",
    "    for sample in json_QA:\n",
    "        if 'Annotator Metadata' in sample and 'Tools' in sample['Annotator Metadata']:\n",
    "            tools_text = sample['Annotator Metadata']['Tools']\n",
    "            \n",
    "            # Parse tools (handle different formats)\n",
    "            tool_lines = tools_text.split('\\n')\n",
    "            for tool_line in tool_lines:\n",
    "                tool = tool_line.strip()\n",
    "                if tool.startswith('- '):\n",
    "                    tool = tool[2:].strip()\n",
    "                elif tool.startswith('â€¢ '):\n",
    "                    tool = tool[2:].strip()\n",
    "                \n",
    "                # Clean up tool names\n",
    "                tool = tool.lower().strip()\n",
    "                if tool.startswith('('):\n",
    "                    end_paren = tool.find(')')\n",
    "                    if end_paren != -1:\n",
    "                        tool = tool[end_paren+1:].strip()\n",
    "                \n",
    "                if tool and tool != '':\n",
    "                    tools.append(tool)\n",
    "                    tool_details.append({\n",
    "                        'tool': tool,\n",
    "                        'question_id': sample.get('task_id'),\n",
    "                        'level': sample.get('Level'),\n",
    "                        'has_file': sample.get('file_name') is not None\n",
    "                    })\n",
    "    \n",
    "    # Count tool frequencies\n",
    "    tools_counter = OrderedDict(Counter(tools).most_common())\n",
    "    \n",
    "    print(\"ðŸŽ¯ Tool Usage Priority Analysis:\")\n",
    "    print(f\"Total tool instances: {len(tools)}\")\n",
    "    print(f\"Unique tools identified: {len(tools_counter)}\")\n",
    "    print(\"\\nðŸ“Š Implementation Priority (by frequency):\")\n",
    "    \n",
    "    for i, (tool, count) in enumerate(tools_counter.items(), 1):\n",
    "        priority = \"ðŸ”´ HIGH\" if count >= 10 else \"ðŸŸ¡ MEDIUM\" if count >= 5 else \"ðŸŸ¢ LOW\"\n",
    "        print(f\"  {i:2d}. {tool:<25} : {count:3d} occurrences {priority}\")\n",
    "    \n",
    "    return tools_counter, tool_details\n",
    "\n",
    "# Run tool analysis\n",
    "tools_counter, tool_details = analyze_tool_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of tool usage\n",
    "if tools_counter:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Top 15 tools\n",
    "    top_tools = list(tools_counter.items())[:15]\n",
    "    tool_names = [item[0] for item in top_tools]\n",
    "    tool_counts = [item[1] for item in top_tools]\n",
    "    \n",
    "    colors = ['#FF6B6B' if count >= 10 else '#4ECDC4' if count >= 5 else '#45B7D1' for count in tool_counts]\n",
    "    \n",
    "    bars = plt.barh(range(len(tool_names)), tool_counts, color=colors)\n",
    "    plt.yticks(range(len(tool_names)), tool_names)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title('GAIA Tool Usage Analysis\\n(Implementation Priority Guide)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, tool_counts)):\n",
    "        plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                str(count), va='center', fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#FF6B6B', label='HIGH Priority (â‰¥10)'),\n",
    "        Patch(facecolor='#4ECDC4', label='MEDIUM Priority (5-9)'), \n",
    "        Patch(facecolor='#45B7D1', label='LOW Priority (<5)')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_implementation_recommendations(tools_counter):\n",
    "    \"\"\"Generate data-driven tool implementation recommendations\"\"\"\n",
    "    if not tools_counter:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Implementation Recommendations:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Essential tools (high frequency)\n",
    "    essential = [(tool, count) for tool, count in tools_counter.items() if count >= 10]\n",
    "    important = [(tool, count) for tool, count in tools_counter.items() if 5 <= count < 10]\n",
    "    optional = [(tool, count) for tool, count in tools_counter.items() if count < 5]\n",
    "    \n",
    "    print(f\"ðŸ”´ ESSENTIAL TOOLS (Implement First):\")\n",
    "    for tool, count in essential:\n",
    "        print(f\"  â”œâ”€â”€ {tool}: {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\nðŸŸ¡ IMPORTANT TOOLS (Implement Second):\")\n",
    "    for tool, count in important:\n",
    "        print(f\"  â”œâ”€â”€ {tool}: {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\nðŸŸ¢ OPTIONAL TOOLS (If Budget Allows):\")\n",
    "    for tool, count in optional[:5]:  # Show top 5 optional\n",
    "        print(f\"  â”œâ”€â”€ {tool}: {count} occurrences\")\n",
    "    \n",
    "    # File type analysis\n",
    "    print(f\"\\nðŸ“ File Processing Requirements:\")\n",
    "    file_questions = [q for q in json_QA if q.get('file_name')]\n",
    "    if file_questions:\n",
    "        file_extensions = []\n",
    "        for q in file_questions:\n",
    "            filename = q.get('file_name', '')\n",
    "            if '.' in filename:\n",
    "                ext = Path(filename).suffix.lower()\n",
    "                file_extensions.append(ext)\n",
    "        \n",
    "        ext_counts = Counter(file_extensions)\n",
    "        for ext, count in ext_counts.most_common():\n",
    "            print(f\"  â”œâ”€â”€ {ext}: {count} files\")\n",
    "\n",
    "# Generate recommendations\n",
    "generate_implementation_recommendations(tools_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Build FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_faiss_store():\n",
    "    \"\"\"Simple FAISS setup for RAG\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”§ Setting up FAISS Vector Store:\")\n",
    "    print(\"  â”œâ”€â”€ In-memory for speed\")\n",
    "    print(\"  â”œâ”€â”€ Disk persistence\")\n",
    "    print(\"  â””â”€â”€ Simple reset = delete files\")\n",
    "    \n",
    "    return {\n",
    "        \"index_file\": \"gaia_examples.faiss\",\n",
    "        \"metadata_file\": \"gaia_examples_meta.json\",\n",
    "        \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    }\n",
    "\n",
    "def reset_faiss_store():\n",
    "    \"\"\"Clean slate - delete existing files\"\"\"\n",
    "    files = [\"gaia_examples.faiss\", \"gaia_examples_meta.json\"]\n",
    "    deleted = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            deleted.append(file_path)\n",
    "    \n",
    "    if deleted:\n",
    "        print(f\"ðŸ—‘ï¸  Deleted: {', '.join(deleted)}\")\n",
    "    else:\n",
    "        print(\"âœ… No existing files to delete\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Setup FAISS configuration\n",
    "config = setup_simple_faiss_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index():\n",
    "    \"\"\"Create FAISS index with GAIA examples\"\"\"\n",
    "    \n",
    "    if not json_QA:\n",
    "        print(\"âŒ No GAIA data available\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\nðŸ“š Creating FAISS index with {len(json_QA)} examples...\")\n",
    "    \n",
    "    # Document preparation\n",
    "    docs = []\n",
    "    for sample in json_QA:\n",
    "        content = f\"Question: {sample.get('Question', '')}\\nAnswer: {sample.get('Final answer', '')}\"\n",
    "        \n",
    "        # Add steps if available\n",
    "        if 'Annotator Metadata' in sample and 'Steps' in sample['Annotator Metadata']:\n",
    "            content += f\"\\nSteps: {sample['Annotator Metadata']['Steps']}\"\n",
    "            \n",
    "        docs.append({\n",
    "            'id': sample.get('task_id'),\n",
    "            'content': content,\n",
    "            'level': sample.get('Level'),\n",
    "            'has_file': sample.get('file_name') is not None\n",
    "        })\n",
    "    \n",
    "    print(f\"  â”œâ”€â”€ Prepared {len(docs)} documents\")\n",
    "    print(f\"  â”œâ”€â”€ Average length: {np.mean([len(d['content']) for d in docs]):.0f} chars\")\n",
    "    \n",
    "    # For production, uncomment this:\n",
    "    \"\"\"\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    \n",
    "    # Generate embeddings\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    texts = [doc['content'] for doc in docs]\n",
    "    embeddings = model.encode(texts)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    \n",
    "    # Save to disk\n",
    "    faiss.write_index(index, \"gaia_examples.faiss\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = [{'id': doc['id'], 'level': doc['level'], 'has_file': doc['has_file']} for doc in docs]\n",
    "    with open(\"gaia_examples_meta.json\", \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "    \n",
    "    print(f\"âœ… FAISS index saved with {index.ntotal} vectors\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"  â”œâ”€â”€ Would generate embeddings (768-dim)\")\n",
    "    print(f\"  â”œâ”€â”€ Would create FAISS IndexFlatIP\")\n",
    "    print(f\"  â”œâ”€â”€ Would save to: gaia_examples.faiss\")\n",
    "    print(f\"  â””â”€â”€ Would save metadata: gaia_examples_meta.json\")\n",
    "    \n",
    "    return len(docs)\n",
    "\n",
    "# Reset and create index\n",
    "print(\"1ï¸âƒ£  Reset existing store:\")\n",
    "reset_success = reset_faiss_store()\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  Create FAISS index:\")\n",
    "if reset_success:\n",
    "    index_size = create_faiss_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_search_simulation():\n",
    "    \"\"\"Simulate how FAISS search would work\"\"\"\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is the population of a city?\",\n",
    "        \"Calculate compound interest\", \n",
    "        \"Find information in spreadsheet\",\n",
    "        \"Search for scientific information\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nðŸ” Testing Search Simulation:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{i}. Query: '{query}'\")\n",
    "        \n",
    "        # Simulate results from similar GAIA examples\n",
    "        mock_results = []\n",
    "        for j, sample in enumerate(json_QA[:2]):\n",
    "            mock_results.append({\n",
    "                'score': random.uniform(0.7, 0.95),\n",
    "                'question': sample.get('Question', '')[:50] + '...',\n",
    "                'answer': sample.get('Final answer', ''),\n",
    "                'level': sample.get('Level')\n",
    "            })\n",
    "        \n",
    "        print(\"ðŸ“Š Similar Examples Found:\")\n",
    "        for j, result in enumerate(mock_results, 1):\n",
    "            print(f\"  {j}. [{result['score']:.3f}] {result['question']}\")\n",
    "            print(f\"     Answer: {result['answer']}\")\n",
    "        \n",
    "        # Show agent routing suggestion\n",
    "        if \"calculate\" in query.lower():\n",
    "            agent = \"data_analyst\"\n",
    "        elif \"search\" in query.lower() or \"population\" in query.lower():\n",
    "            agent = \"web_researcher\"\n",
    "        elif \"spreadsheet\" in query.lower():\n",
    "            agent = \"data_analyst\"\n",
    "        else:\n",
    "            agent = \"general_assistant\"\n",
    "            \n",
    "        print(f\"   ðŸ¤– Suggested agent: {agent}\")\n",
    "\n",
    "# Test search simulation\n",
    "test_search_simulation()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸŽ‰ DATASET ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… Tool priorities identified\")\n",
    "print(\"âœ… File type analysis done\")\n",
    "print(\"âœ… FAISS vector store designed\")\n",
    "print(\"âœ… Ready for agent development\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
