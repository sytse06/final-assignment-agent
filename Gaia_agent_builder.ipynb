{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Agent Builder\n",
    "## Design and test agent architecture\n",
    "\n",
    "**Objective:** Build 4-agent system with proper GAIA formatting  \n",
    "**Output:** Working agents with routing and format compliance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Dependency Check & Import Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency verification and system check\n",
    "import sys\n",
    "import importlib\n",
    "import pkg_resources\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Required dependencies with version checking\n",
    "REQUIRED_PACKAGES = {\n",
    "    'langchain': '>=0.1.0',\n",
    "    'langchain-groq': '>=0.1.0', \n",
    "    'langchain-google-genai': '>=0.1.0',\n",
    "    'langchain-openai': '>=0.1.0',\n",
    "    'langgraph': '>=0.1.0',\n",
    "    'smolagents': '>=0.1.0',\n",
    "    'pandas': '>=1.5.0',\n",
    "    'numpy': '>=1.20.0',\n",
    "    'openai': '>=1.0.0',\n",
    "    'backoff': '>=2.0.0',\n",
    "    'rich': '>=10.0.0'\n",
    "}\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"Check all required dependencies are installed\"\"\"\n",
    "    missing_packages = []\n",
    "    version_issues = []\n",
    "    \n",
    "    for package, min_version in REQUIRED_PACKAGES.items():\n",
    "        try:\n",
    "            importlib.import_module(package.replace('-', '_'))\n",
    "            # Check version if specified\n",
    "            if min_version:\n",
    "                installed_version = pkg_resources.get_distribution(package).version\n",
    "                # Version comparison logic here\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    return missing_packages, version_issues\n",
    "\n",
    "# Check system requirements\n",
    "missing, version_issues = check_dependencies()\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing packages: {missing}\")\n",
    "    print(\"Install with: pip install \" + \" \".join(missing))\n",
    "else:\n",
    "    print(\"‚úÖ All dependencies available\")\n",
    "\n",
    "# Check file system requirements\n",
    "required_files = [\n",
    "    \"gaia_embeddings.csv\",\n",
    "    \"metadata.jsonl\", \n",
    "    \"agent_logic.py\",\n",
    "    \"agent_interface.py\", \n",
    "    \"agent_logging.py\",\n",
    "    \"agent_testing.py\",\n",
    "    \"dev_retriever.py\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìÅ File System Check:\")\n",
    "for file_path in required_files:\n",
    "    if Path(file_path).exists():\n",
    "        print(f\"‚úÖ {file_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file_path} - Missing\")\n",
    "\n",
    "# Environment variables check\n",
    "required_env_vars = [\n",
    "    \"GROQ_API_KEY\",\n",
    "    \"GOOGLE_API_KEY\", \n",
    "    \"OPENROUTER_API_KEY\",\n",
    "    # \"OLLAMA_HOST\" - optional\n",
    "]\n",
    "\n",
    "print(\"\\nüîë Environment Variables:\")\n",
    "for var in required_env_vars:\n",
    "    if os.getenv(var):\n",
    "        print(f\"‚úÖ {var} - Set\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {var} - Not set (provider will be unavailable)\")\n",
    "\n",
    "# Import all core modules\n",
    "try:\n",
    "    from agent_logic import GAIAAgent, GAIAConfig\n",
    "    from agent_interface import create_gaia_agent, get_groq_config\n",
    "    from agent_logging import AgentLoggingSetup\n",
    "    from agent_testing import run_quick_gaia_test\n",
    "    from dev_retriever import load_gaia_retriever\n",
    "    print(\"\\n‚úÖ All core modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency verification and system check based on pyproject.toml\n",
    "import sys\n",
    "import importlib\n",
    "import pkg_resources\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Core dependencies from pyproject.toml + missing critical ones\n",
    "CORE_DEPENDENCIES = {\n",
    "    # From pyproject.toml - Core application\n",
    "    'gradio': '^5.13.2',\n",
    "    'requests': '^2.32.3', \n",
    "    'pandas': '^2.0.0',\n",
    "    'python-dotenv': '^1.0.0',\n",
    "    \n",
    "    # From pyproject.toml - AI/ML framework\n",
    "    'smolagents': '^1.15.0',\n",
    "    'transformers': '^4.40.0',\n",
    "    'huggingface-hub': '^0.30.0',\n",
    "    'torch': '^2.1.0',\n",
    "    'datasets': '^2.14.0',\n",
    "    \n",
    "    # From pyproject.toml - LangChain ecosystem\n",
    "    'langgraph': '^0.4.8',\n",
    "    'langchain-openai': '^0.3.19',\n",
    "    'langchain-ollama': '^0.3.3',\n",
    "    'langchain-groq': '^0.3.2',\n",
    "    'langchain-google-genai': '^2.1.5',\n",
    "    'langchain-community': '^0.3.24',\n",
    "    'langchain-huggingface': '^0.2.0',\n",
    "    'langchain-weaviate': '^0.0.5',\n",
    "    \n",
    "    # From pyproject.toml - Scientific computing\n",
    "    'numpy': '>=2.0.0,<3.0.0',\n",
    "    'matplotlib': '^3.7.0',\n",
    "    'scipy': '^1.11.0',\n",
    "    'seaborn': '^0.13.2',\n",
    "    \n",
    "    # From pyproject.toml - File handling\n",
    "    'beautifulsoup4': '^4.12.0',\n",
    "    'pillow': '^11.0.0',\n",
    "    'openpyxl': '^3.1.0',\n",
    "    'pypdf2': '^3.0.0',\n",
    "    'python-docx': '^1.1.0',\n",
    "    \n",
    "    # From pyproject.toml - Audio processing\n",
    "    'librosa': '^0.10.0',\n",
    "    'soundfile': '^0.12.0',\n",
    "    \n",
    "    # From pyproject.toml - Additional tools\n",
    "    'sentence-transformers': '^4.1.0',\n",
    "    'weaviate-client': '^4.14.4',\n",
    "    'duckduckgo-search': '^8.0.2',\n",
    "    'docling': '^2.36.1',\n",
    "    'backoff': '^2.2.1',\n",
    "    \n",
    "    # MISSING FROM PYPROJECT.TOML but required by our system\n",
    "    'litellm': '^1.72.1',  # Critical for SmolagAgents LiteLLMModel\n",
    "    'rich': '^10.0.0',     # Used in agent_logging.py\n",
    "    'lxml': '>=5.3.0',     # From pyproject.toml but important for web scraping\n",
    "}\n",
    "\n",
    "# Tool-specific dependencies that our system uses\n",
    "TOOL_DEPENDENCIES = {\n",
    "    # Document processing tools (used by ContentRetrieverTool)\n",
    "    'docling': '^2.36.1',           # PDF, DOCX, HTML parsing\n",
    "    'docling-core': '^2.33.1',      # Docling core functionality\n",
    "    'docling-parse': '^4.0.3',      # PDF parsing\n",
    "    'pypdf2': '^3.0.0',             # PDF fallback\n",
    "    'python-docx': '^1.1.0',        # Word documents\n",
    "    'openpyxl': '^3.1.0',           # Excel files\n",
    "    'beautifulsoup4': '^4.12.0',    # HTML parsing\n",
    "    \n",
    "    # Web scraping tools (used by web researchers)\n",
    "    'duckduckgo-search': '^8.0.2',  # Search functionality\n",
    "    'requests': '^2.32.3',          # HTTP requests\n",
    "    'lxml': '>=5.3.0',              # XML/HTML parsing\n",
    "    'selenium': '^4.15.0',          # Browser automation (optional)\n",
    "    \n",
    "    # Audio processing tools\n",
    "    'librosa': '^0.10.0',           # Audio analysis\n",
    "    'soundfile': '^0.12.0',         # Audio I/O\n",
    "    'pydub': '^0.25.1',             # Audio manipulation\n",
    "    \n",
    "    # Image processing tools\n",
    "    'pillow': '^11.0.0',            # Image processing\n",
    "    'opencv-python-headless': '^4.11.0',  # Computer vision\n",
    "    'easyocr': '^1.7.2',            # OCR functionality\n",
    "    \n",
    "    # Embedding and retrieval tools\n",
    "    'sentence-transformers': '^4.1.0',  # Embeddings\n",
    "    'weaviate-client': '^4.14.4',       # Vector database\n",
    "    'langchain-weaviate': '^0.0.5',     # LangChain-Weaviate integration\n",
    "    \n",
    "    # Data processing tools\n",
    "    'numpy': '>=2.0.0,<3.0.0',      # Numerical computing\n",
    "    'pandas': '^2.0.0',             # Data manipulation\n",
    "    'scipy': '^1.11.0',             # Scientific computing\n",
    "}\n",
    "\n",
    "# Optional dependencies that enhance functionality\n",
    "OPTIONAL_DEPENDENCIES = {\n",
    "    'openai': '^1.0.0',           # From pyproject.toml extras\n",
    "    'anthropic': '^0.7.0',        # From pyproject.toml extras  \n",
    "    'selenium': '^4.15.0',        # From pyproject.toml extras\n",
    "    'groq': '^0.26.0',            # Groq API client\n",
    "    'ollama': '^0.5.1',           # Ollama API client\n",
    "}\n",
    "\n",
    "def get_installed_packages():\n",
    "    \"\"\"Get list of actually installed packages\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'list'], \n",
    "                              capture_output=True, text=True)\n",
    "        installed = {}\n",
    "        for line in result.stdout.split('\\n')[2:]:  # Skip header lines\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    package_name = parts[0].lower()\n",
    "                    version = parts[1]\n",
    "                    installed[package_name] = version\n",
    "        return installed\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not get installed packages: {e}\")\n",
    "        return {}\n",
    "\n",
    "def check_package_availability(package_name, installed_packages=None):\n",
    "    \"\"\"Check if a package is available and get version\"\"\"\n",
    "    if installed_packages is None:\n",
    "        installed_packages = {}\n",
    "    \n",
    "    # Check in installed packages first (most reliable)\n",
    "    package_lower = package_name.lower()\n",
    "    if package_lower in installed_packages:\n",
    "        return True, installed_packages[package_lower]\n",
    "    \n",
    "    # Handle package name variations\n",
    "    variations = [\n",
    "        package_name.replace('-', '_'),\n",
    "        package_name.replace('_', '-'),\n",
    "        package_name.replace('-', ''),\n",
    "    ]\n",
    "    \n",
    "    for variation in variations:\n",
    "        if variation.lower() in installed_packages:\n",
    "            return True, installed_packages[variation.lower()]\n",
    "    \n",
    "    # Fallback to import test\n",
    "    try:\n",
    "        import_name = package_name.replace('-', '_')\n",
    "        importlib.import_module(import_name)\n",
    "        \n",
    "        try:\n",
    "            version = pkg_resources.get_distribution(package_name).version\n",
    "            return True, version\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            return True, \"unknown\"\n",
    "    except ImportError:\n",
    "        return False, None\n",
    "\n",
    "print(\"üîç DEPENDENCY ANALYSIS (Based on pyproject.toml + Actual Installation)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get actually installed packages\n",
    "print(\"üì¶ Scanning installed packages...\")\n",
    "installed_packages = get_installed_packages()\n",
    "print(f\"‚úÖ Found {len(installed_packages)} installed packages\")\n",
    "\n",
    "# Check core dependencies\n",
    "missing_core = []\n",
    "available_core = []\n",
    "version_info = {}\n",
    "\n",
    "print(f\"\\nüì¶ CORE DEPENDENCIES ({len(CORE_DEPENDENCIES)} packages):\")\n",
    "for package, version_spec in CORE_DEPENDENCIES.items():\n",
    "    is_available, installed_version = check_package_availability(package, installed_packages)\n",
    "    \n",
    "    if is_available:\n",
    "        available_core.append(package)\n",
    "        version_info[package] = installed_version\n",
    "        print(f\"‚úÖ {package:<25} {installed_version}\")\n",
    "    else:\n",
    "        missing_core.append(package)\n",
    "        print(f\"‚ùå {package:<25} NOT INSTALLED\")\n",
    "\n",
    "# Check tool-specific dependencies\n",
    "print(f\"\\nüîß TOOL-SPECIFIC DEPENDENCIES ({len(TOOL_DEPENDENCIES)} packages):\")\n",
    "missing_tools = []\n",
    "available_tools = []\n",
    "\n",
    "for package, version_spec in TOOL_DEPENDENCIES.items():\n",
    "    is_available, installed_version = check_package_availability(package, installed_packages)\n",
    "    \n",
    "    if is_available:\n",
    "        available_tools.append(package)\n",
    "        print(f\"‚úÖ {package:<30} {installed_version}\")\n",
    "    else:\n",
    "        missing_tools.append(package)\n",
    "        print(f\"‚ùå {package:<30} NOT INSTALLED\")\n",
    "\n",
    "# Check optional dependencies\n",
    "print(f\"\\nüîß OPTIONAL DEPENDENCIES ({len(OPTIONAL_DEPENDENCIES)} packages):\")\n",
    "missing_optional = []\n",
    "available_optional = []\n",
    "\n",
    "for package, version_spec in OPTIONAL_DEPENDENCIES.items():\n",
    "    is_available, installed_version = check_package_availability(package, installed_packages)\n",
    "    \n",
    "    if is_available:\n",
    "        available_optional.append(package)\n",
    "        print(f\"‚úÖ {package:<25} {installed_version}\")\n",
    "    else:\n",
    "        missing_optional.append(package)\n",
    "        print(f\"‚ö†Ô∏è {package:<25} NOT INSTALLED (optional)\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nüìä DEPENDENCY SUMMARY:\")\n",
    "print(f\"‚úÖ Core available: {len(available_core)}/{len(CORE_DEPENDENCIES)}\")\n",
    "print(f\"‚ùå Core missing: {len(missing_core)}\")\n",
    "print(f\"üîß Tools available: {len(available_tools)}/{len(TOOL_DEPENDENCIES)}\")\n",
    "print(f\"‚ùå Tools missing: {len(missing_tools)}\")\n",
    "print(f\"‚öôÔ∏è Optional available: {len(available_optional)}/{len(OPTIONAL_DEPENDENCIES)}\")\n",
    "\n",
    "# Critical missing packages\n",
    "all_missing = missing_core + missing_tools\n",
    "if all_missing:\n",
    "    print(f\"\\n‚ùå MISSING PACKAGES:\")\n",
    "    for package in all_missing:\n",
    "        print(f\"   ‚Ä¢ {package}\")\n",
    "    \n",
    "    print(f\"\\nüí° Install missing packages:\")\n",
    "    print(f\"pip install {' '.join(all_missing)}\")\n",
    "\n",
    "# Tools capability assessment\n",
    "print(f\"\\nüõ†Ô∏è TOOLS CAPABILITY ASSESSMENT:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Document processing capability\n",
    "doc_tools = ['docling', 'docling-core', 'pypdf2', 'python-docx', 'openpyxl', 'beautifulsoup4']\n",
    "doc_available = sum(1 for tool in doc_tools if tool in available_tools)\n",
    "print(f\"üìÑ Document Processing: {doc_available}/{len(doc_tools)} tools available\")\n",
    "\n",
    "# Web scraping capability\n",
    "web_tools = ['duckduckgo-search', 'requests', 'lxml']\n",
    "web_available = sum(1 for tool in web_tools if tool in available_tools)\n",
    "web_selenium = 'selenium' in available_optional\n",
    "print(f\"üåê Web Research: {web_available}/{len(web_tools)} tools available {'+ Selenium' if web_selenium else ''}\")\n",
    "\n",
    "# Audio processing capability\n",
    "audio_tools = ['librosa', 'soundfile', 'pydub']\n",
    "audio_available = sum(1 for tool in audio_tools if tool in available_tools)\n",
    "print(f\"üéµ Audio Processing: {audio_available}/{len(audio_tools)} tools available\")\n",
    "\n",
    "# Image processing capability\n",
    "image_tools = ['pillow', 'opencv-python-headless', 'easyocr']\n",
    "image_available = sum(1 for tool in image_tools if tool in available_tools)\n",
    "print(f\"üñºÔ∏è Image Processing: {image_available}/{len(image_tools)} tools available\")\n",
    "\n",
    "# Embedding and retrieval capability\n",
    "embed_tools = ['sentence-transformers', 'weaviate-client', 'langchain-weaviate']\n",
    "embed_available = sum(1 for tool in embed_tools if tool in available_tools)\n",
    "print(f\"üß† Embeddings/Retrieval: {embed_available}/{len(embed_tools)} tools available\")\n",
    "\n",
    "# Docker/HF Spaces shopping list\n",
    "print(f\"\\nüê≥ DOCKER/HF SPACES INSTALLATION LIST:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"# Add these to your requirements.txt or Dockerfile:\")\n",
    "print()\n",
    "\n",
    "all_packages_dict = {**CORE_DEPENDENCIES, **TOOL_DEPENDENCIES, **OPTIONAL_DEPENDENCIES}\n",
    "for package in sorted(all_packages_dict.keys()):\n",
    "    if package in CORE_DEPENDENCIES:\n",
    "        version = CORE_DEPENDENCIES[package]\n",
    "        print(f\"{package}{version}  # Core - Required\")\n",
    "    elif package in TOOL_DEPENDENCIES:\n",
    "        version = TOOL_DEPENDENCIES[package]\n",
    "        print(f\"{package}{version}  # Tool - Required for functionality\")\n",
    "    else:\n",
    "        version = OPTIONAL_DEPENDENCIES[package] \n",
    "        print(f\"{package}{version}  # Optional - Enhances functionality\")\n",
    "\n",
    "print()\n",
    "print(\"# Additional system dependencies for Docker:\")\n",
    "print(\"# RUN apt-get update && apt-get install -y \\\\\")\n",
    "print(\"#     libsndfile1 \\\\          # For librosa/soundfile\")\n",
    "print(\"#     ffmpeg \\\\              # For audio processing\") \n",
    "print(\"#     poppler-utils \\\\       # For PDF processing\")\n",
    "print(\"#     libxml2-dev \\\\         # For lxml\")\n",
    "print(\"#     libxslt1-dev \\\\        # For lxml\")\n",
    "print(\"#     tesseract-ocr \\\\       # For easyocr\")\n",
    "print(\"#     libglib2.0-0 \\\\       # For opencv\")\n",
    "print(\"#     libsm6 \\\\             # For opencv\")\n",
    "print(\"#     libxext6 \\\\           # For opencv\")\n",
    "print(\"#     libxrender-dev \\\\     # For opencv\")\n",
    "print(\"#     libgomp1 \\\\           # For scientific computing\")\n",
    "print(\"#     && rm -rf /var/lib/apt/lists/*\")\n",
    "\n",
    "print(f\"\\nüîë ENVIRONMENT VARIABLES CHECK:\")\n",
    "required_env_vars = [\n",
    "    \"GROQ_API_KEY\",\n",
    "    \"GOOGLE_API_KEY\", \n",
    "    \"OPENROUTER_API_KEY\",\n",
    "    \"OPENAI_API_KEY\",        # For OpenAI provider\n",
    "    \"ANTHROPIC_API_KEY\",     # For Anthropic provider\n",
    "    \"HF_TOKEN\",              # For HuggingFace\n",
    "    # \"OLLAMA_HOST\",         # Optional for local Ollama\n",
    "]\n",
    "\n",
    "env_status = {}\n",
    "for var in required_env_vars:\n",
    "    is_set = bool(os.getenv(var))\n",
    "    env_status[var] = is_set\n",
    "    status = \"‚úÖ Set\" if is_set else \"‚ö†Ô∏è Not set\"\n",
    "    provider = \"\"\n",
    "    \n",
    "    if \"GROQ\" in var:\n",
    "        provider = \"(Groq provider)\"\n",
    "    elif \"GOOGLE\" in var:\n",
    "        provider = \"(Google provider)\" \n",
    "    elif \"OPENROUTER\" in var:\n",
    "        provider = \"(OpenRouter provider)\"\n",
    "    elif \"OPENAI\" in var:\n",
    "        provider = \"(OpenAI provider)\"\n",
    "    elif \"ANTHROPIC\" in var:\n",
    "        provider = \"(Anthropic provider)\"\n",
    "    elif \"HF_TOKEN\" in var:\n",
    "        provider = \"(HuggingFace)\"\n",
    "    \n",
    "    print(f\"{status:<15} {var:<20} {provider}\")\n",
    "\n",
    "print(f\"\\nüìÅ FILE SYSTEM CHECK:\")\n",
    "required_files = [\n",
    "    \"gaia_embeddings.csv\",\n",
    "    \"metadata.jsonl\", \n",
    "    \"agent_logic.py\",\n",
    "    \"agent_interface.py\", \n",
    "    \"agent_logging.py\",\n",
    "    \"agent_testing.py\",\n",
    "    \"dev_retriever.py\"\n",
    "]\n",
    "\n",
    "file_status = {}\n",
    "for file_path in required_files:\n",
    "    exists = Path(file_path).exists()\n",
    "    file_status[file_path] = exists\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {file_path}\")\n",
    "\n",
    "# Test critical imports\n",
    "print(f\"\\nüß™ CRITICAL IMPORT TEST:\")\n",
    "critical_imports = [\n",
    "    (\"agent_logic\", \"GAIAAgent, GAIAConfig\"),\n",
    "    (\"agent_interface\", \"create_gaia_agent, get_groq_config\"),\n",
    "    (\"agent_logging\", \"AgentLoggingSetup\"),\n",
    "    (\"agent_testing\", \"run_quick_gaia_test\"),\n",
    "    (\"dev_retriever\", \"load_gaia_retriever\"),\n",
    "    (\"smolagents\", \"LiteLLMModel, ToolCallingAgent\"),\n",
    "    (\"langchain_groq\", \"ChatGroq\"),\n",
    "    (\"langchain_google_genai\", \"ChatGoogleGenerativeAI\"),\n",
    "    (\"langgraph.graph\", \"StateGraph\"),\n",
    "    (\"docling\", \"DocumentConverter\"),  # Key for document processing\n",
    "    (\"litellm\", \"completion\"),         # Key for LLM providers\n",
    "]\n",
    "\n",
    "import_status = {}\n",
    "for module, components in critical_imports:\n",
    "    try:\n",
    "        importlib.import_module(module.replace('-', '_'))\n",
    "        import_status[module] = True\n",
    "        print(f\"‚úÖ {module:<25} ({components})\")\n",
    "    except ImportError as e:\n",
    "        import_status[module] = False\n",
    "        print(f\"‚ùå {module:<25} FAILED: {str(e)}\")\n",
    "\n",
    "# Test tool imports specifically\n",
    "print(f\"\\nüîß TOOL-SPECIFIC IMPORT TEST:\")\n",
    "tool_imports = [\n",
    "    (\"docling\", \"Document processing\"),\n",
    "    (\"sentence_transformers\", \"Embeddings\"),\n",
    "    (\"weaviate\", \"Vector database\"),\n",
    "    (\"duckduckgo_search\", \"Web search\"),\n",
    "    (\"librosa\", \"Audio processing\"),\n",
    "    (\"cv2\", \"Computer vision\"),  # opencv-python-headless\n",
    "    (\"easyocr\", \"OCR\"),\n",
    "]\n",
    "\n",
    "tool_import_status = {}\n",
    "for module, description in tool_imports:\n",
    "    try:\n",
    "        importlib.import_module(module)\n",
    "        tool_import_status[module] = True\n",
    "        print(f\"‚úÖ {module:<20} ({description})\")\n",
    "    except ImportError as e:\n",
    "        tool_import_status[module] = False\n",
    "        print(f\"‚ùå {module:<20} FAILED: {description}\")\n",
    "\n",
    "# Final readiness assessment\n",
    "print(f\"\\nüèÜ SYSTEM READINESS ASSESSMENT:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "core_deps_ready = len(missing_core) == 0\n",
    "tools_deps_ready = len(missing_tools) <= 2  # Allow 2 missing tool deps\n",
    "files_ready = all(file_status.values())\n",
    "imports_ready = all(import_status.values())\n",
    "min_env_ready = env_status.get(\"GROQ_API_KEY\", False)  # At least one provider\n",
    "\n",
    "readiness_score = sum([\n",
    "    core_deps_ready * 30,      # Dependencies (30%)\n",
    "    tools_deps_ready * 30,     # Tool dependencies (30%)\n",
    "    files_ready * 20,          # Files (20%)\n",
    "    imports_ready * 15,        # Imports (15%)\n",
    "    min_env_ready * 5          # Environment (5%)\n",
    "])\n",
    "\n",
    "print(f\"üì¶ Core Dependencies: {'‚úÖ' if core_deps_ready else '‚ùå'} ({len(available_core)}/{len(CORE_DEPENDENCIES)})\")\n",
    "print(f\"üîß Tool Dependencies: {'‚úÖ' if tools_deps_ready else '‚ùå'} ({len(available_tools)}/{len(TOOL_DEPENDENCIES)})\")\n",
    "print(f\"üìÅ Required Files: {'‚úÖ' if files_ready else '‚ùå'} ({sum(file_status.values())}/{len(required_files)})\")  \n",
    "print(f\"üß™ Critical Imports: {'‚úÖ' if imports_ready else '‚ùå'} ({sum(import_status.values())}/{len(critical_imports)})\")\n",
    "print(f\"üîë Min Environment: {'‚úÖ' if min_env_ready else '‚ùå'} (At least Groq API key)\")\n",
    "\n",
    "print(f\"\\nüéØ Readiness Score: {readiness_score}/100\")\n",
    "\n",
    "if readiness_score >= 90:\n",
    "    print(\"üöÄ SYSTEM READY - Proceed to component testing\")\n",
    "elif readiness_score >= 70:\n",
    "    print(\"‚ö†Ô∏è MOSTLY READY - Address missing items before proceeding\")\n",
    "elif readiness_score >= 50:\n",
    "    print(\"üîß NEEDS WORK - Significant setup required\")\n",
    "else:\n",
    "    print(\"‚ùå NOT READY - Major setup required\")\n",
    "\n",
    "# Save dependency info for later use\n",
    "dependency_info = {\n",
    "    'core_available': available_core,\n",
    "    'core_missing': missing_core,\n",
    "    'tools_available': available_tools,\n",
    "    'tools_missing': missing_tools,\n",
    "    'optional_available': available_optional,\n",
    "    'optional_missing': missing_optional,\n",
    "    'file_status': file_status,\n",
    "    'env_status': env_status,\n",
    "    'import_status': import_status,\n",
    "    'tool_import_status': tool_import_status,\n",
    "    'readiness_score': readiness_score,\n",
    "    'installed_packages': installed_packages\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Dependency info saved in 'dependency_info' variable for reference\")\n",
    "print(f\"üîç Actual installed packages: {len(installed_packages)} found\")\n",
    "\n",
    "for module, components in critical_imports:\n",
    "    try:\n",
    "        importlib.import_module(module.replace('-', '_'))\n",
    "        import_status[module] = True\n",
    "        print(f\"‚úÖ {module:<25} ({components})\")\n",
    "    except ImportError as e:\n",
    "        import_status[module] = False\n",
    "        print(f\"‚ùå {module:<25} FAILED: {str(e)}\")\n",
    "\n",
    "# Final readiness assessment\n",
    "print(f\"\\nüèÜ SYSTEM READINESS ASSESSMENT:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "core_deps_ready = len(missing_core) == 0\n",
    "files_ready = all(file_status.values())\n",
    "imports_ready = all(import_status.values())\n",
    "min_env_ready = env_status.get(\"GROQ_API_KEY\", False)  # At least one provider\n",
    "\n",
    "readiness_score = sum([\n",
    "    core_deps_ready * 40,      # Dependencies (40%)\n",
    "    files_ready * 30,          # Files (30%)\n",
    "    imports_ready * 20,        # Imports (20%)\n",
    "    min_env_ready * 10         # Environment (10%)\n",
    "])\n",
    "\n",
    "print(f\"üì¶ Dependencies: {'‚úÖ' if core_deps_ready else '‚ùå'} ({len(available_core)}/{len(CORE_DEPENDENCIES)})\")\n",
    "print(f\"üìÅ Required Files: {'‚úÖ' if files_ready else '‚ùå'} ({sum(file_status.values())}/{len(required_files)})\")  \n",
    "print(f\"üß™ Critical Imports: {'‚úÖ' if imports_ready else '‚ùå'} ({sum(import_status.values())}/{len(critical_imports)})\")\n",
    "print(f\"üîë Min Environment: {'‚úÖ' if min_env_ready else '‚ùå'} (At least Groq API key)\")\n",
    "\n",
    "print(f\"\\nüéØ Readiness Score: {readiness_score}/100\")\n",
    "\n",
    "if readiness_score >= 90:\n",
    "    print(\"üöÄ SYSTEM READY - Proceed to component testing\")\n",
    "elif readiness_score >= 70:\n",
    "    print(\"‚ö†Ô∏è MOSTLY READY - Address missing items before proceeding\")\n",
    "elif readiness_score >= 50:\n",
    "    print(\"üîß NEEDS WORK - Significant setup required\")\n",
    "else:\n",
    "    print(\"‚ùå NOT READY - Major setup required\")\n",
    "\n",
    "# Save dependency info for later use\n",
    "dependency_info = {\n",
    "    'core_available': available_core,\n",
    "    'core_missing': missing_core,\n",
    "    'optional_available': available_optional,\n",
    "    'optional_missing': missing_optional,\n",
    "    'file_status': file_status,\n",
    "    'env_status': env_status,\n",
    "    'import_status': import_status,\n",
    "    'readiness_score': readiness_score\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Dependency info saved in 'dependency_info' variable for reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Model Initialization Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all provider configurations systematically\n",
    "from agent_interface import (\n",
    "    get_groq_config, get_google_config, get_openrouter_config, get_ollama_config\n",
    ")\n",
    "\n",
    "# Define test matrix\n",
    "providers_to_test = [\n",
    "    (\"groq\", \"qwen-qwq-32b\", get_groq_config),\n",
    "    (\"groq_fast\", \"llama-3.3-70b-versatile\", lambda: get_groq_config(\"llama-3.3-70b-versatile\")),\n",
    "    (\"google\", \"gemini-2.0-flash-preview\", get_google_config), \n",
    "    (\"google_pro\", \"gemini-1.5-pro-002\", lambda: get_google_config(\"gemini-1.5-pro-002\")),\n",
    "    (\"openrouter\", \"qwen/qwen-2.5-coder-32b-instruct:free\", get_openrouter_config),\n",
    "    (\"ollama\", \"qwen2.5-coder:32b\", get_ollama_config)  # if available\n",
    "]\n",
    "\n",
    "model_test_results = {}\n",
    "\n",
    "for provider_name, model_name, config_func in providers_to_test:\n",
    "    print(f\"\\nüß™ Testing {provider_name} ({model_name})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Get configuration\n",
    "        config = config_func()\n",
    "        print(f\"‚úÖ Configuration loaded\")\n",
    "        \n",
    "        # Test agent creation\n",
    "        agent = create_gaia_agent(config)\n",
    "        print(f\"‚úÖ Agent created successfully\")\n",
    "        \n",
    "        # Test simple inference\n",
    "        test_question = \"What is 2 + 2?\"\n",
    "        result = agent.run_single_question(test_question, task_id=f\"test_{provider_name}\")\n",
    "        \n",
    "        if result and result.get('final_answer'):\n",
    "            print(f\"‚úÖ Inference successful: {result['final_answer']}\")\n",
    "            model_test_results[provider_name] = {\n",
    "                'status': 'success',\n",
    "                'answer': result['final_answer'],\n",
    "                'steps': len(result.get('steps', [])),\n",
    "                'complexity': result.get('complexity', 'unknown')\n",
    "            }\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Inference returned empty result\")\n",
    "            model_test_results[provider_name] = {'status': 'empty_result'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)}\")\n",
    "        model_test_results[provider_name] = {'status': 'failed', 'error': str(e)}\n",
    "\n",
    "# Summary of model testing\n",
    "print(f\"\\nüìä MODEL TESTING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "for provider, result in model_test_results.items():\n",
    "    status = result['status']\n",
    "    if status == 'success':\n",
    "        print(f\"‚úÖ {provider}: Working ({result.get('answer', 'N/A')})\")\n",
    "    elif status == 'failed':\n",
    "        print(f\"‚ùå {provider}: Failed - {result.get('error', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {provider}: Issues detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Testing Single Question Execution...\")\n",
    "\n",
    "# Test questions of different complexity levels\n",
    "test_questions = [\n",
    "    {\n",
    "        \"complexity\": \"Simple\",\n",
    "        \"question\": \"What is 25 + 17?\",\n",
    "        \"expected_strategy\": \"direct_llm\"\n",
    "    },\n",
    "    {\n",
    "        \"complexity\": \"Moderate\", \n",
    "        \"question\": \"Calculate the compound interest on $1000 at 5% annually for 3 years\",\n",
    "        \"expected_strategy\": \"smolag_agent\"\n",
    "    },\n",
    "    {\n",
    "        \"complexity\": \"Complex\",\n",
    "        \"question\": \"Analyze the correlation between these datasets: [1,2,3,4,5] and [2,4,6,8,10]\",\n",
    "        \"expected_strategy\": \"smolag_agent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize agent for testing\n",
    "agent = create_gaia_agent(\"qwen3_32b\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for test_case in test_questions:\n",
    "    print(f\"\\nüîç Testing {test_case['complexity']} Question:\")\n",
    "    print(f\"Q: {test_case['question']}\")\n",
    "    \n",
    "    try:\n",
    "        result = agent.run_single_question(test_case['question'])\n",
    "        \n",
    "        print(f\"A: {result['final_answer']}\")\n",
    "        print(f\"Strategy: {result['selected_strategy']}\")\n",
    "        print(f\"Agent: {result.get('selected_agent', 'N/A')}\")\n",
    "        print(f\"Time: {result.get('execution_time', 0):.2f}s\")\n",
    "        \n",
    "        # Check if strategy matches expectation\n",
    "        strategy_match = result['selected_strategy'] == test_case['expected_strategy']\n",
    "        strategy_status = \"‚úÖ\" if strategy_match else \"‚ö†Ô∏è\"\n",
    "        print(f\"Expected Strategy: {test_case['expected_strategy']} {strategy_status}\")\n",
    "        \n",
    "        test_results.append({\n",
    "            \"complexity\": test_case['complexity'],\n",
    "            \"question\": test_case['question'],\n",
    "            \"answer\": result['final_answer'],\n",
    "            \"strategy_used\": result['selected_strategy'],\n",
    "            \"expected_strategy\": test_case['expected_strategy'],\n",
    "            \"strategy_correct\": strategy_match,\n",
    "            \"execution_time\": result.get('execution_time', 0),\n",
    "            \"similar_examples\": len(result.get('similar_examples', []))\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        test_results.append({\n",
    "            \"complexity\": test_case['complexity'],\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Summary of single question tests\n",
    "print(f\"\\nüìä Single Question Test Summary:\")\n",
    "successful_tests = [r for r in test_results if 'error' not in r]\n",
    "print(f\"Successful tests: {len(successful_tests)}/{len(test_questions)}\")\n",
    "\n",
    "if successful_tests:\n",
    "    strategy_accuracy = sum(r['strategy_correct'] for r in successful_tests) / len(successful_tests)\n",
    "    avg_time = np.mean([r['execution_time'] for r in successful_tests])\n",
    "    print(f\"Strategy selection accuracy: {strategy_accuracy:.2f}\")\n",
    "    print(f\"Average execution time: {avg_time:.2f}s\")\n",
    "\n",
    "agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Component Isolation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each component independently to isolate issues\n",
    "\n",
    "# 2.1 Retriever Testing\n",
    "print(\"üîç Testing RAG Retriever System\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    retriever = load_gaia_retriever(\"gaia_embeddings.csv\")\n",
    "    \n",
    "    if retriever and retriever.is_ready():\n",
    "        print(\"‚úÖ Retriever loaded successfully\")\n",
    "        \n",
    "        # Test retrieval quality\n",
    "        test_queries = [\n",
    "            \"calculate percentage\",\n",
    "            \"analyze spreadsheet data\", \n",
    "            \"image processing question\",\n",
    "            \"complex reasoning task\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            similar_docs = retriever.search(query, k=3)\n",
    "            print(f\"üìö '{query}': Found {len(similar_docs)} similar examples\")\n",
    "            \n",
    "            # Show one example\n",
    "            if similar_docs:\n",
    "                example_content = similar_docs[0].page_content[:100] + \"...\"\n",
    "                print(f\"    Example: {example_content}\")\n",
    "    else:\n",
    "        print(\"‚ùå Retriever failed to initialize\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Retriever error: {e}\")\n",
    "\n",
    "# 2.2 Logging System Testing  \n",
    "print(f\"\\nüìù Testing Logging System\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    logging_setup = AgentLoggingSetup(debug_mode=True)\n",
    "    print(\"‚úÖ Logging system initialized\")\n",
    "    \n",
    "    # Test logging methods\n",
    "    logging_setup.start_task(\"test_task_123\", complexity=\"simple\")\n",
    "    logging_setup.set_routing_path(\"one_shot_llm\")\n",
    "    logging_setup.set_similar_examples_count(3)\n",
    "    logging_setup.log_question_result(\n",
    "        task_id=\"test_task_123\",\n",
    "        question=\"Test question\",\n",
    "        final_answer=\"Test answer\", \n",
    "        total_steps=2,\n",
    "        success=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ All logging methods working\")\n",
    "    print(f\"üìÅ Log files: {logging_setup.current_log_files}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Logging error: {e}\")\n",
    "\n",
    "# 2.3 Tool Loading Testing\n",
    "print(f\"\\nüîß Testing Tool Integration\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    from tools import GetAttachmentTool, ContentRetrieverTool\n",
    "    \n",
    "    # Test tool creation\n",
    "    attachment_tool = GetAttachmentTool()\n",
    "    content_tool = ContentRetrieverTool()\n",
    "    \n",
    "    print(\"‚úÖ Custom GAIA tools loaded\")\n",
    "    print(f\"    GetAttachmentTool: {attachment_tool.name}\")\n",
    "    print(f\"    ContentRetrieverTool: {content_tool.name}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Custom tools not available: {e}\")\n",
    "    print(\"    This is expected if tools.py is not implemented yet\")\n",
    "\n",
    "# Test SmolagAgent base tools\n",
    "try:\n",
    "    from smolagents import GoogleSearchTool, VisitWebpageTool\n",
    "    \n",
    "    search_tool = GoogleSearchTool()\n",
    "    web_tool = VisitWebpageTool()\n",
    "    \n",
    "    print(\"‚úÖ SmolagAgent web tools loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Web tools issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: LangGraph Workflow Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test workflow paths and routing decisions\n",
    "\n",
    "# Create test agent for workflow testing\n",
    "workflow_config = get_ollama_config()\n",
    "workflow_agent = create_gaia_agent(workflow_config)\n",
    "\n",
    "# Test routing decision accuracy\n",
    "routing_test_cases = [\n",
    "    # (question, expected_complexity, expected_route, description)\n",
    "    (\"What is 25% of 400?\", \"simple\", \"one_shot\", \"Simple arithmetic\"),\n",
    "    (\"What are the primary colors?\", \"simple\", \"one_shot\", \"Simple factual\"), \n",
    "    (\"What is the current population of Tokyo?\", \"complex\", \"manager\", \"Needs web search\"),\n",
    "    (\"Analyze the data in the attached Excel file\", \"complex\", \"manager\", \"File processing\"),\n",
    "    (\"Calculate the compound interest on $1000 at 5% for 3 years\", \"simple\", \"one_shot\", \"Math formula\"),\n",
    "    (\"Research recent developments in AI and summarize trends\", \"complex\", \"manager\", \"Complex research\")\n",
    "]\n",
    "\n",
    "print(\"üîÄ Testing Workflow Routing\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "routing_results = []\n",
    "\n",
    "for question, expected_complexity, expected_route, description in routing_test_cases:\n",
    "    print(f\"\\nüìù Test: {description}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    \n",
    "    try:\n",
    "        result = workflow_agent.run_single_question(question, task_id=f\"routing_test_{len(routing_results)}\")\n",
    "        \n",
    "        actual_complexity = result.get('complexity', 'unknown')\n",
    "        actual_steps = result.get('steps', [])\n",
    "        \n",
    "        # Determine actual route from steps\n",
    "        if any('one-shot' in step.lower() for step in actual_steps):\n",
    "            actual_route = \"one_shot\"\n",
    "        elif any('manager' in step.lower() for step in actual_steps):\n",
    "            actual_route = \"manager\"\n",
    "        else:\n",
    "            actual_route = \"unknown\"\n",
    "        \n",
    "        # Check routing accuracy\n",
    "        complexity_correct = actual_complexity == expected_complexity\n",
    "        route_correct = actual_route == expected_route\n",
    "        \n",
    "        print(f\"üß† Complexity: {actual_complexity} (expected: {expected_complexity}) {'‚úÖ' if complexity_correct else '‚ùå'}\")\n",
    "        print(f\"üîÄ Route: {actual_route} (expected: {expected_route}) {'‚úÖ' if route_correct else '‚ùå'}\")\n",
    "        print(f\"üí¨ Answer: {result.get('final_answer', 'No answer')}\")\n",
    "        \n",
    "        routing_results.append({\n",
    "            'question': question,\n",
    "            'expected_complexity': expected_complexity,\n",
    "            'actual_complexity': actual_complexity,\n",
    "            'expected_route': expected_route,\n",
    "            'actual_route': actual_route,\n",
    "            'complexity_correct': complexity_correct,\n",
    "            'route_correct': route_correct,\n",
    "            'description': description\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Workflow error: {e}\")\n",
    "        routing_results.append({\n",
    "            'question': question,\n",
    "            'error': str(e),\n",
    "            'description': description\n",
    "        })\n",
    "\n",
    "# Routing accuracy summary\n",
    "correct_complexity = sum(1 for r in routing_results if r.get('complexity_correct', False))\n",
    "correct_routing = sum(1 for r in routing_results if r.get('route_correct', False))\n",
    "total_tests = len([r for r in routing_results if 'error' not in r])\n",
    "\n",
    "print(f\"\\nüìä ROUTING ACCURACY SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Complexity Detection: {correct_complexity}/{total_tests} ({correct_complexity/total_tests*100:.1f}%)\")\n",
    "print(f\"Route Selection: {correct_routing}/{total_tests} ({correct_routing/total_tests*100:.1f}%)\")\n",
    "\n",
    "if correct_complexity/total_tests < 0.8:\n",
    "    print(\"‚ö†Ô∏è Complexity detection needs improvement\")\n",
    "if correct_routing/total_tests < 0.8:\n",
    "    print(\"‚ö†Ô∏è Route selection logic needs adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Agent Setup Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into specialist agent creation and coordination\n",
    "\n",
    "print(\"ü§ñ Testing Specialist Agent Setup\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test specialist creation individually\n",
    "specialist_configs = [\n",
    "    (\"data_analyst\", \"CodeAgent with Python tools\"),\n",
    "    (\"web_researcher\", \"ToolCallingAgent with web tools\"), \n",
    "    (\"document_processor\", \"ToolCallingAgent with file tools\")\n",
    "]\n",
    "\n",
    "for specialist_name, description in specialist_configs:\n",
    "    print(f\"\\nüîß Testing {specialist_name}\")\n",
    "    print(f\"   {description}\")\n",
    "    \n",
    "    try:\n",
    "        # Access the specialist from workflow_agent\n",
    "        if hasattr(workflow_agent, 'specialists') and specialist_name in workflow_agent.specialists:\n",
    "            specialist = workflow_agent.specialists[specialist_name]\n",
    "            \n",
    "            print(f\"‚úÖ {specialist_name} created successfully\")\n",
    "            print(f\"   Type: {type(specialist).__name__}\")\n",
    "            print(f\"   Tools: {len(getattr(specialist, 'tools', []))} tools available\")\n",
    "            \n",
    "            # Test basic functionality\n",
    "            if specialist_name == \"data_analyst\":\n",
    "                # Test code execution capability\n",
    "                test_code = \"print(f'Data analyst test: 2 + 2 = {2+2}')\"\n",
    "                # Would need to test specialist.run() with code\n",
    "                print(f\"   Code execution: Available\")\n",
    "                \n",
    "            elif specialist_name == \"web_researcher\":\n",
    "                # Test web tools availability\n",
    "                web_tools = getattr(specialist, 'tools', [])\n",
    "                tool_names = [tool.name if hasattr(tool, 'name') else str(tool) for tool in web_tools]\n",
    "                print(f\"   Web tools: {tool_names}\")\n",
    "                \n",
    "            elif specialist_name == \"document_processor\":\n",
    "                # Test document processing tools\n",
    "                doc_tools = getattr(specialist, 'tools', [])\n",
    "                tool_names = [tool.name if hasattr(tool, 'name') else str(tool) for tool in doc_tools]\n",
    "                print(f\"   Document tools: {tool_names}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {specialist_name} not found in agent\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {specialist_name} setup error: {e}\")\n",
    "\n",
    "# Test manager coordination\n",
    "print(f\"\\nüë®‚Äçüíº Testing Manager Agent\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    manager = workflow_agent.manager\n",
    "    print(f\"‚úÖ Manager agent available\")\n",
    "    print(f\"   Type: {type(manager).__name__}\")\n",
    "    print(f\"   Managed agents: {len(getattr(manager, 'managed_agents', []))}\")\n",
    "    \n",
    "    # Test manager can delegate to specialists\n",
    "    manager_tools = getattr(manager, 'tools', [])\n",
    "    print(f\"   Manager tools: {len(manager_tools)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Manager setup error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: GAIA File Type Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test specific GAIA file types with actual files\n",
    "\n",
    "print(\"üìé Testing GAIA File Type Support\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define test files (these should exist in your test data)\n",
    "test_files = {\n",
    "    '.xlsx': 'sample_spreadsheet.xlsx',\n",
    "    '.csv': 'sample_data.csv', \n",
    "    '.png': 'sample_image.png',\n",
    "    '.pdf': 'sample_document.pdf',\n",
    "    '.txt': 'sample_text.txt',\n",
    "    '.json': 'sample_data.json'\n",
    "}\n",
    "\n",
    "file_processing_results = {}\n",
    "\n",
    "for file_type, file_path in test_files.items():\n",
    "    print(f\"\\nüìÑ Testing {file_type} processing\")\n",
    "    \n",
    "    if Path(file_path).exists():\n",
    "        print(f\"‚úÖ Test file available: {file_path}\")\n",
    "        \n",
    "        # Create test question with file\n",
    "        test_question = f\"Analyze the content in the attached {file_type} file and provide a brief summary.\"\n",
    "        \n",
    "        try:\n",
    "            # Test with document processor specialist\n",
    "            result = workflow_agent.run_single_question(\n",
    "                question=test_question,\n",
    "                task_id=f\"file_test_{file_type.replace('.', '')}\"\n",
    "            )\n",
    "            \n",
    "            if result and result.get('final_answer'):\n",
    "                print(f\"‚úÖ Processing successful\")\n",
    "                print(f\"   Answer length: {len(result['final_answer'])} characters\")\n",
    "                print(f\"   Steps: {len(result.get('steps', []))}\")\n",
    "                \n",
    "                file_processing_results[file_type] = {\n",
    "                    'status': 'success',\n",
    "                    'answer_length': len(result['final_answer']),\n",
    "                    'steps': len(result.get('steps', []))\n",
    "                }\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Processing returned empty result\")\n",
    "                file_processing_results[file_type] = {'status': 'empty'}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Processing failed: {e}\")\n",
    "            file_processing_results[file_type] = {'status': 'failed', 'error': str(e)}\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Test file not available: {file_path}\")\n",
    "        file_processing_results[file_type] = {'status': 'no_file'}\n",
    "\n",
    "# File processing summary\n",
    "print(f\"\\nüìä FILE PROCESSING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "for file_type, result in file_processing_results.items():\n",
    "    status = result['status']\n",
    "    if status == 'success':\n",
    "        print(f\"‚úÖ {file_type}: Processed successfully\")\n",
    "    elif status == 'failed':\n",
    "        print(f\"‚ùå {file_type}: Failed - {result.get('error', 'Unknown')}\")\n",
    "    elif status == 'no_file':\n",
    "        print(f\"‚ö†Ô∏è {file_type}: No test file available\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {file_type}: Issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Mock/Synthetic Data Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with synthetic data before using real GAIA data\n",
    "\n",
    "print(\"üé≠ Testing with Mock/Synthetic Data\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create synthetic test cases that mirror GAIA patterns\n",
    "synthetic_test_cases = [\n",
    "    {\n",
    "        'task_id': 'synthetic_001',\n",
    "        'question': 'Calculate the compound interest on $5000 at 3% annual rate for 2 years.',\n",
    "        'expected_answer': '5304.50',  # Pre-calculated\n",
    "        'level': 1,\n",
    "        'category': 'mathematics'\n",
    "    },\n",
    "    {\n",
    "        'task_id': 'synthetic_002', \n",
    "        'question': 'What are the three primary colors in the RGB color model?',\n",
    "        'expected_answer': 'red, green, blue',\n",
    "        'level': 1,\n",
    "        'category': 'knowledge'\n",
    "    },\n",
    "    {\n",
    "        'task_id': 'synthetic_003',\n",
    "        'question': 'If a dataset has values [10, 15, 20, 25, 30], what is the median?',\n",
    "        'expected_answer': '20',\n",
    "        'level': 1,\n",
    "        'category': 'statistics'\n",
    "    },\n",
    "    {\n",
    "        'task_id': 'synthetic_004',\n",
    "        'question': 'List the chemical symbols for hydrogen, oxygen, and carbon.',\n",
    "        'expected_answer': 'H, O, C',\n",
    "        'level': 1,\n",
    "        'category': 'chemistry'\n",
    "    }\n",
    "]\n",
    "\n",
    "synthetic_results = []\n",
    "\n",
    "for test_case in synthetic_test_cases:\n",
    "    question = test_case['question']\n",
    "    expected = test_case['expected_answer']\n",
    "    \n",
    "    print(f\"\\nüìù Testing: {test_case['category']}\")\n",
    "    print(f\"‚ùì Question: {question}\")\n",
    "    print(f\"üéØ Expected: {expected}\")\n",
    "    \n",
    "    try:\n",
    "        result = workflow_agent.run_single_question(\n",
    "            question=question,\n",
    "            task_id=test_case['task_id']\n",
    "        )\n",
    "        \n",
    "        agent_answer = result.get('final_answer', '').strip()\n",
    "        complexity = result.get('complexity', 'unknown')\n",
    "        steps = len(result.get('steps', []))\n",
    "        \n",
    "        # Simple answer matching (normalize for comparison)\n",
    "        agent_normalized = agent_answer.lower().replace(',', '').replace('$', '').strip()\n",
    "        expected_normalized = expected.lower().replace(',', '').replace('$', '').strip()\n",
    "        \n",
    "        is_correct = agent_normalized == expected_normalized or expected_normalized in agent_normalized\n",
    "        \n",
    "        print(f\"ü§ñ Agent: {agent_answer}\")\n",
    "        print(f\"üéØ Match: {'‚úÖ' if is_correct else '‚ùå'}\")\n",
    "        print(f\"üß† Complexity: {complexity}\")\n",
    "        print(f\"üìä Steps: {steps}\")\n",
    "        \n",
    "        synthetic_results.append({\n",
    "            'test_case': test_case,\n",
    "            'agent_answer': agent_answer,\n",
    "            'is_correct': is_correct,\n",
    "            'complexity': complexity,\n",
    "            'steps': steps\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        synthetic_results.append({\n",
    "            'test_case': test_case,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# Synthetic testing summary\n",
    "correct_answers = sum(1 for r in synthetic_results if r.get('is_correct', False))\n",
    "total_synthetic = len([r for r in synthetic_results if 'error' not in r])\n",
    "\n",
    "print(f\"\\nüìä SYNTHETIC DATA TESTING SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Correct answers: {correct_answers}/{total_synthetic} ({correct_answers/total_synthetic*100:.1f}%)\")\n",
    "\n",
    "for result in synthetic_results:\n",
    "    if 'error' not in result:\n",
    "        category = result['test_case']['category']\n",
    "        correct = '‚úÖ' if result['is_correct'] else '‚ùå'\n",
    "        complexity = result['complexity']\n",
    "        print(f\"{correct} {category}: {complexity} complexity\")\n",
    "\n",
    "if correct_answers/total_synthetic >= 0.8:\n",
    "    print(\"üéâ System ready for real GAIA data testing!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Address issues before proceeding to real data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Development Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary of all tests\n",
    "\n",
    "print(\"üìã DEVELOPMENT TESTING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all test results\n",
    "summary_data = {\n",
    "    'dependencies': len(missing) == 0 if 'missing' in locals() else False,\n",
    "    'model_providers': len([r for r in model_test_results.values() if r.get('status') == 'success']),\n",
    "    'total_providers': len(model_test_results),\n",
    "    'routing_accuracy': correct_routing/total_tests if 'total_tests' in locals() and total_tests > 0 else 0,\n",
    "    'synthetic_accuracy': correct_answers/total_synthetic if 'total_synthetic' in locals() and total_synthetic > 0 else 0,\n",
    "    'file_types_working': len([r for r in file_processing_results.values() if r.get('status') == 'success']),\n",
    "    'total_file_types': len(file_processing_results)\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Dependencies: {'All OK' if summary_data['dependencies'] else 'Issues found'}\")\n",
    "print(f\"ü§ñ Model Providers: {summary_data['model_providers']}/{summary_data['total_providers']} working\")\n",
    "print(f\"üîÄ Routing Accuracy: {summary_data['routing_accuracy']*100:.1f}%\")\n",
    "print(f\"üé≠ Synthetic Test Accuracy: {summary_data['synthetic_accuracy']*100:.1f}%\") \n",
    "print(f\"üìé File Type Support: {summary_data['file_types_working']}/{summary_data['total_file_types']} working\")\n",
    "\n",
    "# Readiness assessment\n",
    "readiness_score = (\n",
    "    summary_data['dependencies'] * 20 +\n",
    "    (summary_data['model_providers'] / summary_data['total_providers']) * 20 +\n",
    "    summary_data['routing_accuracy'] * 20 +\n",
    "    summary_data['synthetic_accuracy'] * 20 +\n",
    "    (summary_data['file_types_working'] / summary_data['total_file_types']) * 20\n",
    ")\n",
    "\n",
    "print(f\"\\nüèÜ Development Readiness Score: {readiness_score:.1f}/100\")\n",
    "\n",
    "if readiness_score >= 80:\n",
    "    print(\"üöÄ System ready for production validation testing!\")\n",
    "elif readiness_score >= 60:\n",
    "    print(\"‚ö†Ô∏è Minor issues to address before production testing\")\n",
    "else:\n",
    "    print(\"‚ùå Significant issues need resolution\")\n",
    "\n",
    "# Next steps recommendations\n",
    "print(f\"\\nüí° Next Steps:\")\n",
    "if summary_data['model_providers'] < summary_data['total_providers']:\n",
    "    print(\"  1. Fix model provider connectivity issues\")\n",
    "if summary_data['routing_accuracy'] < 0.8:\n",
    "    print(\"  2. Improve routing logic and complexity detection\")\n",
    "if summary_data['synthetic_accuracy'] < 0.8:\n",
    "    print(\"  3. Debug answer formatting and processing logic\")\n",
    "if summary_data['file_types_working'] < summary_data['total_file_types']:\n",
    "    print(\"  4. Implement or fix file processing capabilities\")\n",
    "\n",
    "print(\"  ‚Üí Proceed to Production Validator notebook when ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
