{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Agent Builder\n",
    "## Design and test agent architecture\n",
    "\n",
    "**Objective:** Build 4-agent system with proper GAIA formatting  \n",
    "**Output:** Working agents with routing and format compliance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Dependencies and Model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîß Environment Setup Complete\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check for required files\n",
    "required_files = [\"gaia_agent_system.py\", \"dev_retriever.py\", \"gaia_embeddings.csv\", \"metadata.jsonl\"]\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"‚ùå Missing files: {missing_files}\")\n",
    "    print(\"Please ensure all required files are in the current directory\")\n",
    "else:\n",
    "    print(\"‚úÖ All required files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaia_agent_system import (\n",
    "    create_gaia_agent,           # was: create_hybrid_gaia_agent\n",
    "    create_production_gaia_agent,  # this one stays the same\n",
    "    ModelConfigs,                # was: GAIAModelConfigs\n",
    "    GAIAConfig                   # was: HybridGAIAConfig\n",
    ")\n",
    "\n",
    "print(\"üöÄ GAIA Agent Builder - Test Notebook\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show available model configurations\n",
    "print(\"\\nüìã Available Model Configurations:\")\n",
    "configs = ModelConfigs.get_all_configs()  # Updated class name\n",
    "\n",
    "openrouter_configs = ModelConfigs.get_openrouter_configs()  # Updated class name\n",
    "groq_configs = ModelConfigs.get_groq_configs()              # Added Groq configs\n",
    "google_configs = ModelConfigs.get_google_configs()          # Added Google configs\n",
    "ollama_configs = ModelConfigs.get_ollama_configs()          # Updated class name\n",
    "\n",
    "print(\"\\nüåê OpenRouter Models (Free):\")\n",
    "for name, config in openrouter_configs.items():\n",
    "    primary = config['primary_model']\n",
    "    secondary = config.get('secondary_model', 'None')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {name}: {primary} ‚Üí {secondary}\")\n",
    "\n",
    "print(\"\\n‚ö° Groq Models (Fast):\")\n",
    "for name, config in groq_configs.items():\n",
    "    primary = config['primary_model']\n",
    "    secondary = config.get('secondary_model', 'None')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {name}: {primary} ‚Üí {secondary}\")\n",
    "\n",
    "print(\"\\nü§ñ Google Models:\")\n",
    "for name, config in google_configs.items():\n",
    "    primary = config['primary_model']\n",
    "    secondary = config.get('secondary_model', 'None')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {name}: {primary} ‚Üí {secondary}\")\n",
    "\n",
    "print(\"\\nüè† Ollama Models (Local):\")\n",
    "for name, config in ollama_configs.items():\n",
    "    primary = config['primary_model']\n",
    "    secondary = config.get('secondary_model', 'None')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {name}: {primary} ‚Üí {secondary}\")\n",
    "\n",
    "print(f\"\\nTotal configurations available: {len(configs)}\")\n",
    "\n",
    "# Show fallback chain summary\n",
    "print(\"\\nüîÑ Fallback Chain Examples:\")\n",
    "print(\"Primary Model Fails ‚Üí Secondary Model ‚Üí Hardcoded Fallback\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ qwen-qwq-32b ‚Üí llama3-70b-8192 ‚Üí qwen/qwen-2.5-coder-32b-instruct:free\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ gemini-2.5-flash-preview-04-17 ‚Üí gemini-2.5-flash-preview-05-20 ‚Üí qwen/qwen-2.5-coder-32b-instruct:free\")\n",
    "print(\"  ‚îî‚îÄ‚îÄ qwen/qwen3-32b:free ‚Üí qwen/qwen3-14b:free ‚Üí qwen/qwen-2.5-coder-32b-instruct:free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with updated function names\n",
    "print(\"\\nüß™ Quick Test Examples:\")\n",
    "\n",
    "# Test 1: Using Groq with QwQ model\n",
    "print(\"\\n1. Testing Groq QwQ model with fallback:\")\n",
    "try:\n",
    "    agent = create_gaia_agent(\"qwen_qwq_groq\")\n",
    "    result = agent.run_single_question(\"What is 15% of 200?\")\n",
    "    print(f\"   Answer: {result.get('final_answer', 'No answer')}\")\n",
    "    print(f\"   Strategy: {result.get('selected_strategy', 'Unknown')}\")\n",
    "    print(f\"   Model used: {result.get('model_used', 'Unknown')}\")\n",
    "    agent.close()\n",
    "    print(\"   ‚úÖ Groq test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Groq test failed: {e}\")\n",
    "\n",
    "# Test 2: Using Google Gemini with fallback\n",
    "print(\"\\n2. Testing Google Gemini with fallback:\")\n",
    "try:\n",
    "    agent = create_gaia_agent(\"gemini_flash_04\")\n",
    "    result = agent.run_single_question(\"Calculate the square root of 144\")\n",
    "    print(f\"   Answer: {result.get('final_answer', 'No answer')}\")\n",
    "    print(f\"   Strategy: {result.get('selected_strategy', 'Unknown')}\")\n",
    "    print(f\"   Model used: {result.get('model_used', 'Unknown')}\")\n",
    "    agent.close()\n",
    "    print(\"   ‚úÖ Google test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Google test failed: {e}\")\n",
    "\n",
    "# Test 3: Using OpenRouter with new naming\n",
    "print(\"\\n3. Testing OpenRouter with updated naming:\")\n",
    "try:\n",
    "    agent = create_gaia_agent(\"qwen3_32b\")\n",
    "    result = agent.run_single_question(\"What is 25% of 400?\")\n",
    "    print(f\"   Answer: {result.get('final_answer', 'No answer')}\")\n",
    "    print(f\"   Strategy: {result.get('selected_strategy', 'Unknown')}\")\n",
    "    print(f\"   Model used: {result.get('model_used', 'Unknown')}\")\n",
    "    agent.close()\n",
    "    print(\"   ‚úÖ OpenRouter test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå OpenRouter test failed: {e}\")\n",
    "\n",
    "# Method 4: Custom configuration using GAIAConfig with fallback\n",
    "print(\"\\n4. Testing custom configuration with manual fallback:\")\n",
    "try:\n",
    "    custom_config = GAIAConfig(\n",
    "        model_provider=\"groq\",\n",
    "        primary_model=\"qwen-qwq-32b\",\n",
    "        secondary_model=\"llama3-70b-8192\",\n",
    "        temperature=0.2,\n",
    "        enable_model_fallback=True,\n",
    "        debug_mode=True\n",
    "    )\n",
    "    custom_agent = create_gaia_agent(custom_config)\n",
    "    result = custom_agent.run_single_question(\"What is 10 + 25?\")\n",
    "    print(f\"   Answer: {result.get('final_answer', 'No answer')}\")\n",
    "    print(f\"   Fallback used: {'Yes' if result.get('fallback_used') else 'No'}\")\n",
    "    custom_agent.close()\n",
    "    print(\"   ‚úÖ Custom config test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Custom config test failed: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All updated imports and fallback models working correctly!\")\n",
    "print(\"\\nüéØ Available Config Names:\")\n",
    "print(\"OpenRouter:\", list(openrouter_configs.keys()))\n",
    "print(\"Groq:\", list(groq_configs.keys()))\n",
    "print(\"Google:\", list(google_configs.keys()))\n",
    "print(\"Ollama:\", list(ollama_configs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Agent initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ Testing Basic Agent Initialization...\")\n",
    "\n",
    "try:\n",
    "    # Test with free OpenRouter model (most likely to work)\n",
    "    agent = create_gaia_agent(\"qwen3_32b\")\n",
    "    print(\"‚úÖ Agent initialized successfully with qwen_coder_free\")\n",
    "    \n",
    "    # Test retriever\n",
    "    test_search = agent.retriever.search(\"calculate compound interest\", k=2)\n",
    "    print(f\"‚úÖ Retriever working - found {len(test_search)} similar examples\")\n",
    "    \n",
    "    # Test metadata\n",
    "    sample_metadata = agent.metadata_manager.get_test_sample(5)\n",
    "    print(f\"‚úÖ Metadata loaded - {len(sample_metadata)} samples available\")\n",
    "    \n",
    "    # Show tool usage analysis\n",
    "    tool_usage = agent.metadata_manager.analyze_tool_usage()\n",
    "    print(f\"‚úÖ Tool analysis complete - {len(tool_usage)} tool types identified\")\n",
    "    \n",
    "    agent.close()\n",
    "    print(\"‚úÖ Agent closed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Agent initialization failed: {e}\")\n",
    "    print(\"Check your API keys and file paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Testing Single Question Execution...\")\n",
    "\n",
    "# Test questions of different complexity levels\n",
    "test_questions = [\n",
    "    {\n",
    "        \"complexity\": \"Simple\",\n",
    "        \"question\": \"What is 25 + 17?\",\n",
    "        \"expected_strategy\": \"direct_llm\"\n",
    "    },\n",
    "    {\n",
    "        \"complexity\": \"Moderate\", \n",
    "        \"question\": \"Calculate the compound interest on $1000 at 5% annually for 3 years\",\n",
    "        \"expected_strategy\": \"smolag_agent\"\n",
    "    },\n",
    "    {\n",
    "        \"complexity\": \"Complex\",\n",
    "        \"question\": \"Analyze the correlation between these datasets: [1,2,3,4,5] and [2,4,6,8,10]\",\n",
    "        \"expected_strategy\": \"smolag_agent\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize agent for testing\n",
    "agent = create_gaia_agent(\"qwen3_32b\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for test_case in test_questions:\n",
    "    print(f\"\\nüîç Testing {test_case['complexity']} Question:\")\n",
    "    print(f\"Q: {test_case['question']}\")\n",
    "    \n",
    "    try:\n",
    "        result = agent.run_single_question(test_case['question'])\n",
    "        \n",
    "        print(f\"A: {result['final_answer']}\")\n",
    "        print(f\"Strategy: {result['selected_strategy']}\")\n",
    "        print(f\"Agent: {result.get('selected_agent', 'N/A')}\")\n",
    "        print(f\"Time: {result.get('execution_time', 0):.2f}s\")\n",
    "        \n",
    "        # Check if strategy matches expectation\n",
    "        strategy_match = result['selected_strategy'] == test_case['expected_strategy']\n",
    "        strategy_status = \"‚úÖ\" if strategy_match else \"‚ö†Ô∏è\"\n",
    "        print(f\"Expected Strategy: {test_case['expected_strategy']} {strategy_status}\")\n",
    "        \n",
    "        test_results.append({\n",
    "            \"complexity\": test_case['complexity'],\n",
    "            \"question\": test_case['question'],\n",
    "            \"answer\": result['final_answer'],\n",
    "            \"strategy_used\": result['selected_strategy'],\n",
    "            \"expected_strategy\": test_case['expected_strategy'],\n",
    "            \"strategy_correct\": strategy_match,\n",
    "            \"execution_time\": result.get('execution_time', 0),\n",
    "            \"similar_examples\": len(result.get('similar_examples', []))\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        test_results.append({\n",
    "            \"complexity\": test_case['complexity'],\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "# Summary of single question tests\n",
    "print(f\"\\nüìä Single Question Test Summary:\")\n",
    "successful_tests = [r for r in test_results if 'error' not in r]\n",
    "print(f\"Successful tests: {len(successful_tests)}/{len(test_questions)}\")\n",
    "\n",
    "if successful_tests:\n",
    "    strategy_accuracy = sum(r['strategy_correct'] for r in successful_tests) / len(successful_tests)\n",
    "    avg_time = np.mean([r['execution_time'] for r in successful_tests])\n",
    "    print(f\"Strategy selection accuracy: {strategy_accuracy:.2f}\")\n",
    "    print(f\"Average execution time: {avg_time:.2f}s\")\n",
    "\n",
    "agent.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Agent Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Testing RAG Effectiveness...\")\n",
    "\n",
    "# Test different RAG example counts\n",
    "rag_test_counts = [1, 3, 5]\n",
    "rag_test_question = \"Calculate the interest rate needed to double an investment in 10 years\"\n",
    "\n",
    "rag_results = {}\n",
    "\n",
    "for count in rag_test_counts:\n",
    "    print(f\"\\nTesting with {count} RAG examples...\")\n",
    "    \n",
    "    try:\n",
    "        rag_agent = create_gaia_agent({\n",
    "            \"model_provider\": \"openrouter\",\n",
    "            \"primary_model\": \"qwen/qwen-2.5-coder-32b-instruct:free\",\n",
    "            \"rag_examples_count\": count,\n",
    "            \"temperature\": 0.7\n",
    "        })\n",
    "        \n",
    "        result = rag_agent.run_single_question(rag_test_question)\n",
    "        \n",
    "        rag_results[f\"rag_{count}\"] = {\n",
    "            \"examples_count\": count,\n",
    "            \"answer\": result['final_answer'],\n",
    "            \"similar_examples_found\": len(result.get('similar_examples', [])),\n",
    "            \"execution_time\": result.get('execution_time', 0),\n",
    "            \"strategy\": result['selected_strategy']\n",
    "        }\n",
    "        \n",
    "        print(f\"  Examples found: {rag_results[f'rag_{count}']['similar_examples_found']}\")\n",
    "        print(f\"  Answer: {result['final_answer']}\")\n",
    "        print(f\"  Strategy: {result['selected_strategy']}\")\n",
    "        \n",
    "        rag_agent.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error with {count} examples: {e}\")\n",
    "\n",
    "# Display RAG effectiveness\n",
    "if rag_results:\n",
    "    print(f\"\\nüìä RAG Effectiveness Results:\")\n",
    "    rag_df = pd.DataFrame(rag_results).T\n",
    "    print(rag_df[['examples_count', 'similar_examples_found', 'strategy', 'execution_time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ Running Small Batch Evaluation...\")\n",
    "\n",
    "try:\n",
    "    # Use best performing model from previous tests\n",
    "    eval_agent = create_production_gaia_agent(\n",
    "        model_config=\"qwen_coder_free\",\n",
    "        enable_logging=True,\n",
    "        performance_tracking=True\n",
    "    )\n",
    "    \n",
    "    # Run evaluation on small sample\n",
    "    print(\"Running evaluation on 15 questions...\")\n",
    "    results_df = eval_agent.run_batch_evaluation(sample_size=15)\n",
    "    \n",
    "    # Detailed analysis\n",
    "    print(f\"\\nüìà Detailed Analysis:\")\n",
    "    print(f\"Total questions: {len(results_df)}\")\n",
    "    \n",
    "    if 'is_correct' in results_df and results_df['is_correct'].notna().any():\n",
    "        accuracy = results_df['is_correct'].mean()\n",
    "        print(f\"Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"Accuracy: Unable to calculate (no ground truth)\")\n",
    "    \n",
    "    if 'execution_time' in results_df:\n",
    "        avg_time = results_df['execution_time'].mean()\n",
    "        print(f\"Average execution time: {avg_time:.2f}s\")\n",
    "    \n",
    "    # Strategy analysis\n",
    "    if 'strategy_used' in results_df:\n",
    "        strategy_counts = results_df['strategy_used'].value_counts()\n",
    "        print(f\"\\nStrategy Usage:\")\n",
    "        for strategy, count in strategy_counts.items():\n",
    "            percentage = (count / len(results_df)) * 100\n",
    "            print(f\"  ‚îú‚îÄ‚îÄ {strategy}: {count} questions ({percentage:.1f}%)\")\n",
    "            \n",
    "            if 'is_correct' in results_df:\n",
    "                strategy_accuracy = results_df[results_df['strategy_used'] == strategy]['is_correct'].mean()\n",
    "                print(f\"      Accuracy: {strategy_accuracy:.3f}\")\n",
    "    \n",
    "    # Level analysis (if available)\n",
    "    if 'level' in results_df:\n",
    "        level_analysis = results_df.groupby('level').agg({\n",
    "            'is_correct': ['count', 'mean'],\n",
    "            'execution_time': 'mean'\n",
    "        }).round(3)\n",
    "        print(f\"\\nPerformance by GAIA Level:\")\n",
    "        print(level_analysis)\n",
    "    \n",
    "    # Agent usage analysis\n",
    "    if 'selected_agent' in results_df:\n",
    "        smolag_data = results_df[results_df['strategy_used'] == 'smolag_agent']\n",
    "        if len(smolag_data) > 0:\n",
    "            print(f\"\\nSmolagAgent Usage:\")\n",
    "            agent_counts = smolag_data['selected_agent'].value_counts()\n",
    "            for agent, count in agent_counts.items():\n",
    "                if pd.notna(agent):\n",
    "                    print(f\"  ‚îú‚îÄ‚îÄ {agent}: {count} times\")\n",
    "    \n",
    "    eval_agent.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Batch evaluation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Performance Analysis and Recommendations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze all collected data\n",
    "total_tests = len(test_results) if 'test_results' in locals() else 0\n",
    "successful_tests = len([r for r in test_results if 'error' not in r]) if total_tests > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Overall Test Summary:\")\n",
    "print(f\"Single question tests: {successful_tests}/{total_tests}\")\n",
    "print(f\"Model configurations tested: {len(model_comparison) if 'model_comparison' in locals() else 0}\")\n",
    "print(f\"RAG configurations tested: {len(rag_results) if 'rag_results' in locals() else 0}\")\n",
    "\n",
    "# Recommendations based on test results\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "\n",
    "# Model recommendations\n",
    "if 'model_comparison' in locals():\n",
    "    successful_models = [name for name, result in model_comparison.items() if 'error' not in result]\n",
    "    if successful_models:\n",
    "        print(f\"‚úÖ Working models: {', '.join(successful_models)}\")\n",
    "        \n",
    "        # Find fastest model\n",
    "        times = {name: result.get('execution_time', float('inf')) \n",
    "                for name, result in model_comparison.items() if 'execution_time' in result}\n",
    "        if times:\n",
    "            fastest_model = min(times, key=times.get)\n",
    "            print(f\"‚ö° Fastest model: {fastest_model} ({times[fastest_model]:.2f}s)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No models worked successfully - check API keys\")\n",
    "\n",
    "# RAG recommendations\n",
    "if 'rag_results' in locals() and rag_results:\n",
    "    avg_examples_found = np.mean([r['similar_examples_found'] for r in rag_results.values()])\n",
    "    print(f\"üìö Average RAG examples found: {avg_examples_found:.1f}\")\n",
    "    \n",
    "    if avg_examples_found >= 2:\n",
    "        print(\"‚úÖ RAG system working well\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Low RAG retrieval - check vector store\")\n",
    "\n",
    "# Strategy recommendations\n",
    "if 'test_results' in locals() and successful_tests > 0:\n",
    "    strategy_accuracy = sum(r.get('strategy_correct', False) for r in test_results if 'strategy_correct' in r) / successful_tests\n",
    "    if strategy_accuracy >= 0.7:\n",
    "        print(\"‚úÖ Strategy selection working well\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Strategy selection may need tuning\")\n",
    "\n",
    "# Final recommendations\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(\"1. If models are working: Run larger batch evaluation (50+ questions)\")\n",
    "print(\"2. If accuracy is low: Tune complexity_threshold parameter\")\n",
    "print(\"3. If too slow: Use faster models (Groq) or reduce RAG examples\")\n",
    "print(\"4. If SmolagAgents failing: Check tool imports and permissions\")\n",
    "\n",
    "# Export test results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "test_summary = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"single_question_tests\": test_results if 'test_results' in locals() else [],\n",
    "    \"model_comparison\": model_comparison if 'model_comparison' in locals() else {},\n",
    "    \"rag_effectiveness\": rag_results if 'rag_results' in locals() else {},\n",
    "    \"batch_evaluation\": results_df.to_dict() if 'results_df' in locals() else {}\n",
    "}\n",
    "\n",
    "with open(f\"gaia_test_results_{timestamp}.json\", \"w\") as f:\n",
    "    json.dump(test_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Test results saved to: gaia_test_results_{timestamp}.json\")\n",
    "print(\"‚úÖ Testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüé¨ Quick Demo for Showcase\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def get_demo_questions_from_metadata(metadata_path: str = \"metadata.jsonl\", count: int = 3):\n",
    "    \"\"\"Extract real GAIA questions from metadata.jsonl for demo purposes\"\"\"\n",
    "    try:\n",
    "        import json\n",
    "        import random\n",
    "        \n",
    "        # Load metadata\n",
    "        questions = []\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    item = json.loads(line.strip())\n",
    "                    if 'Question' in item:\n",
    "                        # Prefer shorter questions for demo (better showcase)\n",
    "                        question_text = item['Question']\n",
    "                        if len(question_text) < 150:  # Keep demo questions concise\n",
    "                            questions.append({\n",
    "                                'question': question_text,\n",
    "                                'answer': item.get('Final answer', 'Unknown'),\n",
    "                                'level': item.get('Level', 1),\n",
    "                                'task_id': item.get('task_id', 'demo')\n",
    "                            })\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        if not questions:\n",
    "            print(\"‚ö†Ô∏è  No questions found in metadata, using fallback demo questions\")\n",
    "            return [\n",
    "                {'question': \"What is 15% of 240?\", 'answer': '36', 'level': 1},\n",
    "                {'question': \"Calculate the area of a circle with radius 7 meters\", 'answer': '153.94', 'level': 1},\n",
    "                {'question': \"Find the population of Tokyo\", 'answer': 'Unknown', 'level': 2}\n",
    "            ]\n",
    "        \n",
    "        # Randomly select questions with preference for different levels\n",
    "        level_1_questions = [q for q in questions if q['level'] == 1]\n",
    "        level_2_questions = [q for q in questions if q['level'] == 2]\n",
    "        level_3_questions = [q for q in questions if q['level'] == 3]\n",
    "        \n",
    "        demo_questions = []\n",
    "        \n",
    "        # Try to get one question from each level\n",
    "        if level_1_questions and len(demo_questions) < count:\n",
    "            demo_questions.append(random.choice(level_1_questions))\n",
    "        \n",
    "        if level_2_questions and len(demo_questions) < count:\n",
    "            demo_questions.append(random.choice(level_2_questions))\n",
    "        \n",
    "        if level_3_questions and len(demo_questions) < count:\n",
    "            demo_questions.append(random.choice(level_3_questions))\n",
    "        \n",
    "        # Fill remaining slots with random questions\n",
    "        remaining_questions = [q for q in questions if q not in demo_questions]\n",
    "        while len(demo_questions) < count and remaining_questions:\n",
    "            demo_questions.append(random.choice(remaining_questions))\n",
    "            remaining_questions = [q for q in remaining_questions if q not in demo_questions]\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(questions)} real GAIA questions from metadata\")\n",
    "        print(f\"üìã Selected {len(demo_questions)} questions for demo (Levels: {[q['level'] for q in demo_questions]})\")\n",
    "        \n",
    "        return demo_questions\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è  Metadata file '{metadata_path}' not found, using fallback demo questions\")\n",
    "        return [\n",
    "            {'question': \"What is 15% of 240?\", 'answer': '36', 'level': 1},\n",
    "            {'question': \"Calculate the area of a circle with radius 7 meters\", 'answer': '153.94', 'level': 1},\n",
    "            {'question': \"Find the population of Tokyo\", 'answer': 'Unknown', 'level': 2}\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error loading metadata: {e}, using fallback demo questions\")\n",
    "        return [\n",
    "            {'question': \"What is 15% of 240?\", 'answer': '36', 'level': 1},\n",
    "            {'question': \"Calculate the area of a circle with radius 7 meters\", 'answer': '153.94', 'level': 1},\n",
    "            {'question': \"Find the population of Tokyo\", 'answer': 'Unknown', 'level': 2}\n",
    "        ]\n",
    "\n",
    "def run_quick_demo():\n",
    "    \"\"\"Run a quick demo with real GAIA questions for showcase purposes\"\"\"\n",
    "    print(\"\\nüé¨ GAIA Agent Quick Demo\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Get real GAIA questions from metadata\n",
    "    demo_questions = get_demo_questions_from_metadata(count=3)\n",
    "    \n",
    "    try:\n",
    "        # Use proper config name from ModelConfigs\n",
    "        demo_agent = create_gaia_agent(\"qwen3_32b\")  # Use config name from ModelConfigs\n",
    "        \n",
    "        for i, q_data in enumerate(demo_questions, 1):\n",
    "            question = q_data['question']\n",
    "            expected_answer = q_data['answer']\n",
    "            level = q_data['level']\n",
    "            \n",
    "            print(f\"\\nüîç Demo Question {i} (Level {level}):\")\n",
    "            print(f\"Q: {question}\")\n",
    "            \n",
    "            # Run the question through GAIA agent\n",
    "            result = demo_agent.run_single_question(\n",
    "                question=question,\n",
    "                task_id=f\"demo_{i}\",\n",
    "                ground_truth=expected_answer,\n",
    "                level=level\n",
    "            )\n",
    "            \n",
    "            agent_answer = result.get('final_answer', 'No answer')\n",
    "            is_correct = result.get('debug_info', {}).get('is_correct', None)\n",
    "            \n",
    "            print(f\"A: {agent_answer}\")\n",
    "            if expected_answer != 'Unknown':\n",
    "                print(f\"Expected: {expected_answer}\")\n",
    "                if is_correct is not None:\n",
    "                    print(f\"Correct: {'‚úÖ' if is_correct else '‚ùå'}\")\n",
    "            \n",
    "            print(f\"Strategy: {result.get('selected_strategy', 'Unknown')}\")\n",
    "            print(f\"Agent: {result.get('selected_agent', 'N/A')}\")\n",
    "            print(f\"Time: {result.get('execution_time', 0):.2f}s\")\n",
    "            print(f\"Confidence: {result.get('confidence_score', 0):.2f}\")\n",
    "            \n",
    "            # Show any errors or fallbacks\n",
    "            errors = result.get('errors', [])\n",
    "            if errors:\n",
    "                print(f\"Errors: {len(errors)} encountered\")\n",
    "            if result.get('fallback_used', False):\n",
    "                print(\"‚ö†Ô∏è  Fallback strategy used\")\n",
    "        \n",
    "        demo_agent.close()\n",
    "        print(\"\\n‚ú® Demo completed successfully!\")\n",
    "        print(\"üéØ This demo used real GAIA benchmark questions!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Demo failed: {e}\")\n",
    "        print(\"üí° Make sure you have proper API keys configured\")\n",
    "\n",
    "def run_extended_demo(question_count: int = 5):\n",
    "    \"\"\"Run extended demo with more questions\"\"\"\n",
    "    print(f\"\\nüé¨ Extended GAIA Demo ({question_count} questions)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    demo_questions = get_demo_questions_from_metadata(count=question_count)\n",
    "    \n",
    "    try:\n",
    "        demo_agent = create_gaia_agent(\"qwen3_32b\")\n",
    "        \n",
    "        correct_answers = 0\n",
    "        total_questions = len(demo_questions)\n",
    "        \n",
    "        for i, q_data in enumerate(demo_questions, 1):\n",
    "            question = q_data['question']\n",
    "            expected_answer = q_data['answer']\n",
    "            level = q_data['level']\n",
    "            \n",
    "            print(f\"\\nüìù Question {i}/{total_questions} (Level {level}):\")\n",
    "            print(f\"Q: {question[:100]}{'...' if len(question) > 100 else ''}\")\n",
    "            \n",
    "            result = demo_agent.run_single_question(\n",
    "                question=question,\n",
    "                ground_truth=expected_answer,\n",
    "                level=level\n",
    "            )\n",
    "            \n",
    "            agent_answer = result.get('final_answer', 'No answer')\n",
    "            is_correct = result.get('debug_info', {}).get('is_correct', None)\n",
    "            \n",
    "            print(f\"A: {agent_answer}\")\n",
    "            \n",
    "            if is_correct is not None:\n",
    "                print(f\"Result: {'‚úÖ Correct' if is_correct else '‚ùå Incorrect'}\")\n",
    "                if is_correct:\n",
    "                    correct_answers += 1\n",
    "            \n",
    "            print(f\"Time: {result.get('execution_time', 0):.1f}s | Strategy: {result.get('selected_strategy', 'Unknown')}\")\n",
    "        \n",
    "        # Summary\n",
    "        if total_questions > 0:\n",
    "            accuracy = correct_answers / total_questions\n",
    "            print(f\"\\nüìä Extended Demo Results:\")\n",
    "            print(f\"Accuracy: {correct_answers}/{total_questions} ({accuracy:.1%})\")\n",
    "            print(f\"Questions from GAIA benchmark levels: {sorted(set(q['level'] for q in demo_questions))}\")\n",
    "        \n",
    "        demo_agent.close()\n",
    "        print(\"\\nüéâ Extended demo completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extended demo failed: {e}\")\n",
    "\n",
    "# Uncomment to run demo\n",
    "run_quick_demo()\n",
    "\n",
    "# Uncomment for extended demo with more questions\n",
    "# run_extended_demo(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ GAIA Agent Builder Test Notebook Complete!\")\n",
    "print(\"Ready for GAIA benchmark evaluation!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: GAIA Format Testing & Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaia_formatter():\n",
    "    \"\"\"Create GAIA format compliance checker\"\"\"\n",
    "    \n",
    "    def clean_gaia_answer(text):\n",
    "        \"\"\"Clean answer according to GAIA rules\"\"\"\n",
    "        \n",
    "        # Extract final answer if present\n",
    "        if \"FINAL ANSWER:\" in text:\n",
    "            text = text.split(\"FINAL ANSWER:\")[-1].strip()\n",
    "        \n",
    "        # Remove articles (the, a, an)\n",
    "        text = re.sub(r'\\b(the|a|an)\\b\\s*', '', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove commas from numbers\n",
    "        text = re.sub(r'(\\d),(\\d)', r'\\1\\2', text)\n",
    "        \n",
    "        # Handle currency and percentages (remove unless specified)\n",
    "        # This is simplified - real implementation needs context awareness\n",
    "        text = text.replace(', '').replace('%', '')\n",
    "        \n",
    "        # Clean extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def validate_gaia_format(response):\n",
    "        \"\"\"Validate response follows GAIA format\"\"\"\n",
    "        \n",
    "        errors = []\n",
    "        \n",
    "        # Check for FINAL ANSWER\n",
    "        if \"FINAL ANSWER:\" not in response:\n",
    "            errors.append(\"Missing 'FINAL ANSWER:' prefix\")\n",
    "        \n",
    "        # Extract and check final answer\n",
    "        if \"FINAL ANSWER:\" in response:\n",
    "            answer = response.split(\"FINAL ANSWER:\")[-1].strip()\n",
    "            \n",
    "            # Check for common violations\n",
    "            if re.search(r'\\b(the|a|an)\\b', answer, re.IGNORECASE):\n",
    "                errors.append(\"Contains articles (the, a, an)\")\n",
    "            \n",
    "            if re.search(r'\\d,\\d', answer):\n",
    "                errors.append(\"Contains commas in numbers\")\n",
    "            \n",
    "            if len(answer.split()) > 10:  # Arbitrary threshold\n",
    "                errors.append(\"Answer too verbose - should be as few words as possible\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    return clean_gaia_answer, validate_gaia_format\n",
    "\n",
    "def test_gaia_compliance():\n",
    "    \"\"\"Test GAIA formatting with examples\"\"\"\n",
    "    \n",
    "    clean_answer, validate_format = create_gaia_formatter()\n",
    "    \n",
    "    test_cases = [\n",
    "        # (input, expected_output, should_pass)\n",
    "        (\"The answer is 1,234\", \"1234\", True),\n",
    "        (\"Total: $25.50\", \"25.50\", True),  \n",
    "        (\"The city is Paris\", \"Paris\", True),\n",
    "        (\"Cities: New York, Boston\", \"New York,Boston\", True),\n",
    "        (\"I think the answer is definitely 42\", \"I think answer is definitely 42\", False),  # Too verbose\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ GAIA Compliance Testing:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    passed = 0\n",
    "    total = len(test_cases)\n",
    "    \n",
    "    for i, (input_text, expected, should_pass) in enumerate(test_cases, 1):\n",
    "        # Test cleaning\n",
    "        cleaned = clean_answer(input_text)\n",
    "        clean_match = cleaned == expected\n",
    "        \n",
    "        # Test validation\n",
    "        full_response = f\"Let me think... FINAL ANSWER: {input_text}\"\n",
    "        is_valid, errors = validate_format(full_response)\n",
    "        \n",
    "        # Overall pass\n",
    "        test_passed = clean_match and (is_valid == should_pass)\n",
    "        if test_passed:\n",
    "            passed += 1\n",
    "        \n",
    "        status = \"‚úÖ\" if test_passed else \"‚ùå\"\n",
    "        print(f\"{i}. {status} '{input_text}'\")\n",
    "        print(f\"   Cleaned: '{cleaned}' (expected: '{expected}')\")\n",
    "        print(f\"   Valid: {is_valid} | Errors: {errors if errors else 'None'}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"üìä Compliance Results: {passed}/{total} ({passed/total*100:.0f}%) passed\")\n",
    "    \n",
    "    return clean_answer, validate_format\n",
    "\n",
    "# Test GAIA compliance\n",
    "gaia_cleaner, gaia_validator = test_gaia_compliance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_agent_responses():\n",
    "    \"\"\"Create mock responses to test agent routing and formatting\"\"\"\n",
    "    \n",
    "    test_questions = [\n",
    "        {\n",
    "            \"question\": \"Calculate 15% of 2500\",\n",
    "            \"file\": None,\n",
    "            \"expected_agent\": \"data_analyst\",\n",
    "            \"expected_answer\": \"375\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the population of Tokyo in 2023?\",\n",
    "            \"file\": None,\n",
    "            \"expected_agent\": \"web_researcher\",\n",
    "            \"expected_answer\": \"37194000\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Extract the total from this spreadsheet\",\n",
    "            \"file\": \"data.xlsx\",\n",
    "            \"expected_agent\": \"data_analyst\",\n",
    "            \"expected_answer\": \"1250\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Read the text in this PDF\",\n",
    "            \"file\": \"document.pdf\",\n",
    "            \"expected_agent\": \"document_reader\",\n",
    "            \"expected_answer\": \"extracted text content\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Explain the concept of entropy\",\n",
    "            \"file\": None,\n",
    "            \"expected_agent\": \"general_helper\",\n",
    "            \"expected_answer\": \"measure of disorder in system\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    def mock_agent_response(agent_type, question, expected_answer):\n",
    "        \"\"\"Generate mock response for testing\"\"\"\n",
    "        reasoning = f\"I am the {agent_type} agent. Let me process this question: {question[:50]}...\"\n",
    "        \n",
    "        if agent_type == \"data_analyst\":\n",
    "            reasoning += \" I'll use mathematical calculations to solve this.\"\n",
    "        elif agent_type == \"web_researcher\":\n",
    "            reasoning += \" I'll search for the most current information online.\"\n",
    "        elif agent_type == \"document_reader\":\n",
    "            reasoning += \" I'll extract and process the document content.\"\n",
    "        else:\n",
    "            reasoning += \" I'll use general reasoning to explain this concept.\"\n",
    "        \n",
    "        return f\"{reasoning}\\n\\nFINAL ANSWER: {expected_answer}\"\n",
    "    \n",
    "    print(\"üîÑ Testing Agent Routing & Responses:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    routing_correct = 0\n",
    "    format_correct = 0\n",
    "    total_tests = len(test_questions)\n",
    "    \n",
    "    for i, test in enumerate(test_questions, 1):\n",
    "        # Test routing\n",
    "        selected_agent = route_question(test[\"question\"], test[\"file\"])\n",
    "        routing_ok = selected_agent == test[\"expected_agent\"]\n",
    "        if routing_ok:\n",
    "            routing_correct += 1\n",
    "        \n",
    "        # Generate mock response\n",
    "        response = mock_agent_response(selected_agent, test[\"question\"], test[\"expected_answer\"])\n",
    "        \n",
    "        # Test GAIA formatting\n",
    "        is_valid, errors = gaia_validator(response)\n",
    "        if is_valid:\n",
    "            format_correct += 1\n",
    "        \n",
    "        # Results\n",
    "        route_status = \"‚úÖ\" if routing_ok else \"‚ùå\"\n",
    "        format_status = \"‚úÖ\" if is_valid else \"‚ùå\"\n",
    "        \n",
    "        print(f\"{i}. Q: '{test['question'][:40]}...'\")  \n",
    "        print(f\"   File: {test['file'] or 'None'}\")\n",
    "        print(f\"   {route_status} Routing: {selected_agent} (expected: {test['expected_agent']})\")\n",
    "        print(f\"   {format_status} Format: {'Valid' if is_valid else f'Errors: {errors}'}\")\n",
    "        print(f\"   Response: {response[-50:]}...\")  # Show end of response\n",
    "        print()\n",
    "    \n",
    "    print(f\"üìä Test Results:\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ Routing: {routing_correct}/{total_tests} ({routing_correct/total_tests*100:.0f}%)\")\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ Format: {format_correct}/{total_tests} ({format_correct/total_tests*100:.0f}%)\")\n",
    "    print(f\"  ‚îî‚îÄ‚îÄ Overall: {min(routing_correct, format_correct)}/{total_tests} ({min(routing_correct, format_correct)/total_tests*100:.0f}%)\")\n",
    "    \n",
    "    return routing_correct == total_tests and format_correct == total_tests\n",
    "\n",
    "# Run comprehensive testing\n",
    "all_tests_passed = create_mock_agent_responses()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ AGENT CHECKER COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ 4-agent architecture designed\")\n",
    "print(\"‚úÖ GAIA-compliant system prompts created\")\n",
    "print(\"‚úÖ Format compliance tested\")\n",
    "print(\"‚úÖ Routing logic validated\")\n",
    "print(f\"‚úÖ All tests passed: {all_tests_passed}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
