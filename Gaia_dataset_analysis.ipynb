{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Dataset Analysis\n",
    "## Understanding the 165 GAIA validation examples\n",
    "\n",
    "**Objective:** Analyze GAIA patterns and build RAG vector store  \n",
    "**Output:** Tool priorities and FAISS index for agent development\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, OrderedDict\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üîç GAIA Dataset Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Goal: Understand patterns and build vector store\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load & Explore GAIA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gaia_metadata(file_path='metadata.jsonl'):\n",
    "    \"\"\"Load and parse GAIA validation dataset\"\"\"\n",
    "    try:\n",
    "        json_QA = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                json_data = json.loads(line.strip())\n",
    "                json_QA.append(json_data)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully loaded {len(json_QA)} GAIA examples\")\n",
    "        return json_QA\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå metadata.jsonl not found. Creating sample data for demonstration.\")\n",
    "        return create_sample_gaia_data()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading GAIA data: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_sample_gaia_data():\n",
    "    \"\"\"Create sample GAIA data for demonstration purposes\"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            'task_id': 'sample_001',\n",
    "            'Question': 'What is the population of Seattle according to the 2020 census?',\n",
    "            'Level': 1,\n",
    "            'Final answer': '737015',\n",
    "            'file_name': None,\n",
    "            'Annotator Metadata': {\n",
    "                'Steps': 'Search for Seattle population 2020 census data',\n",
    "                'Tools': 'web browser\\nsearch engine'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'task_id': 'sample_002', \n",
    "            'Question': 'Calculate the compound interest on $5000 at 3.5% annual rate for 10 years',\n",
    "            'Level': 2,\n",
    "            'Final answer': '7052.78',\n",
    "            'file_name': None,\n",
    "            'Annotator Metadata': {\n",
    "                'Steps': 'Use compound interest formula: A = P(1 + r)^t',\n",
    "                'Tools': 'calculator'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'task_id': 'sample_003',\n",
    "            'Question': 'What is the average temperature in the attached Excel file?',\n",
    "            'Level': 1,\n",
    "            'Final answer': '23.4',\n",
    "            'file_name': 'temperature_data.xlsx',\n",
    "            'Annotator Metadata': {\n",
    "                'Steps': 'Open Excel file, calculate average of temperature column',\n",
    "                'Tools': 'excel\\ncalculator'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    print(\"üìù Using sample GAIA data for demonstration\")\n",
    "    return sample_data\n",
    "\n",
    "# Load the dataset\n",
    "json_QA = load_gaia_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_qa_patterns_for_retrieval():\n",
    "    \"\"\"\n",
    "    Analyze Q&A patterns relevant for retrieval similarity\n",
    "    Focus: What makes questions similar for vector search\n",
    "    \"\"\"\n",
    "    if not json_QA:\n",
    "        return\n",
    "    \n",
    "    print(\"üìä Q&A RETRIEVAL ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic stats relevant for retrieval\n",
    "    print(f\"Total Q&A pairs: {len(json_QA)}\")\n",
    "    \n",
    "    # Level distribution (affects retrieval quality)\n",
    "    levels = [q.get('Level', 'Unknown') for q in json_QA]\n",
    "    level_counts = Counter(levels)\n",
    "    print(f\"\\nLevel distribution (retrieval diversity):\")\n",
    "    for level in sorted(level_counts.keys()):\n",
    "        print(f\"  Level {level}: {level_counts[level]} questions ({level_counts[level]/len(json_QA)*100:.1f}%)\")\n",
    "    \n",
    "    # Question length analysis (affects embedding quality)\n",
    "    question_lengths = [len(q.get('Question', '')) for q in json_QA]\n",
    "    print(f\"\\nQuestion length analysis:\")\n",
    "    print(f\"  Average length: {np.mean(question_lengths):.0f} characters\")\n",
    "    print(f\"  Min/Max: {min(question_lengths)}/{max(question_lengths)} characters\")\n",
    "    \n",
    "    # Answer format analysis (what the LLM should produce)\n",
    "    answer_lengths = [len(str(q.get('Final answer', ''))) for q in json_QA]\n",
    "    print(f\"\\nAnswer format analysis:\")\n",
    "    print(f\"  Average answer length: {np.mean(answer_lengths):.1f} characters\")\n",
    "    print(f\"  Short answers (<10 chars): {sum(1 for l in answer_lengths if l < 10)}\")\n",
    "    print(f\"  Long answers (>50 chars): {sum(1 for l in answer_lengths if l > 50)}\")\n",
    "    \n",
    "    # File attachment analysis (affects retrieval context)\n",
    "    files_present = sum(1 for q in json_QA if q.get('file_name'))\n",
    "    print(f\"\\nFile attachment distribution:\")\n",
    "    print(f\"  With files: {files_present} ({files_present/len(json_QA)*100:.1f}%)\")\n",
    "    print(f\"  Text-only: {len(json_QA) - files_present} ({(len(json_QA) - files_present)/len(json_QA)*100:.1f}%)\")\n",
    "\n",
    "def sample_qa_for_vector_store():\n",
    "    \"\"\"\n",
    "    Show sample Q&A pairs that will go into vector store\n",
    "    \"\"\"\n",
    "    print(\"\\nüìù SAMPLE Q&A PAIRS FOR VECTOR STORE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"This is exactly what retriever will return:\")\n",
    "    print()\n",
    "    \n",
    "    sample_questions = random.sample(json_QA, min(3, len(json_QA)))\n",
    "    \n",
    "    for i, sample in enumerate(sample_questions, 1):\n",
    "        # This is EXACTLY what goes in the vector store (pure Q&A)\n",
    "        vector_content = f\"Question : {sample['Question']}\\n\\nFinal answer : {sample['Final answer']}\"\n",
    "        \n",
    "        print(f\"Vector Store Entry {i}:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(vector_content)\n",
    "        print(f\"\\nMetadata: {{'source': '{sample.get('task_id', 'N/A')}'}}\")\n",
    "        print(f\"Level: {sample.get('Level', 'N/A')} | File: {sample.get('file_name', 'None')}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Run core analysis\n",
    "analyze_qa_patterns_for_retrieval()\n",
    "sample_qa_for_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Tool Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def analyze_tool_usage_fixed():\n",
    "    \"\"\"Analyze tool frequency with proper normalization\"\"\"\n",
    "    if not json_QA:\n",
    "        return {}\n",
    "    \n",
    "    tools = []\n",
    "    tool_details = []\n",
    "    \n",
    "    def normalize_tool_name(tool):\n",
    "        \"\"\"Normalize tool names to remove duplicates\"\"\"\n",
    "        # Convert to lowercase\n",
    "        tool = tool.lower().strip()\n",
    "        \n",
    "        # Remove numbered prefixes (1., 2., 3., etc.)\n",
    "        tool = re.sub(r'^\\d+\\.\\s*', '', tool)\n",
    "        \n",
    "        # Remove articles (a, an, the)\n",
    "        tool = re.sub(r'^(a|an|the)\\s+', '', tool)\n",
    "        \n",
    "        # Remove parentheses and content inside\n",
    "        tool = re.sub(r'\\([^)]*\\)', '', tool)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        tool = ' '.join(tool.split())\n",
    "        \n",
    "        # Common normalizations\n",
    "        normalizations = {\n",
    "            'web browser': ['browser', 'web browsers', 'internet browser'],\n",
    "            'search engine': ['search engines', 'google search', 'web search'],\n",
    "            'calculator': ['math calculator', 'calculations', 'calculation tool'],\n",
    "            'excel': ['microsoft excel', 'spreadsheet', 'ms excel'],\n",
    "            'pdf viewer': ['pdf reader', 'pdf access', 'pdf'],\n",
    "            'image recognition': ['image recognition tools', 'image analysis', 'image processing'],\n",
    "            'text editor': ['word processor', 'text processing'],\n",
    "            'file manager': ['file explorer', 'file system'],\n",
    "            'audio player': ['music player', 'media player'],\n",
    "            'video player': ['video viewer', 'media player']\n",
    "        }\n",
    "        \n",
    "        # Apply normalizations\n",
    "        for canonical, variants in normalizations.items():\n",
    "            if tool in variants or any(variant in tool for variant in variants):\n",
    "                return canonical\n",
    "        \n",
    "        return tool\n",
    "    \n",
    "    for sample in json_QA:\n",
    "        if 'Annotator Metadata' in sample and 'Tools' in sample['Annotator Metadata']:\n",
    "            tools_text = sample['Annotator Metadata']['Tools']\n",
    "            \n",
    "            # Parse tools (handle different formats)\n",
    "            tool_lines = tools_text.split('\\n')\n",
    "            for tool_line in tool_lines:\n",
    "                tool = tool_line.strip()\n",
    "                \n",
    "                # Skip empty lines\n",
    "                if not tool:\n",
    "                    continue\n",
    "                \n",
    "                # Remove bullet points and list markers\n",
    "                tool = re.sub(r'^[-‚Ä¢*]\\s*', '', tool)\n",
    "                \n",
    "                # Normalize the tool name\n",
    "                normalized_tool = normalize_tool_name(tool)\n",
    "                \n",
    "                if normalized_tool and normalized_tool != 'none':\n",
    "                    tools.append(normalized_tool)\n",
    "                    tool_details.append({\n",
    "                        'tool': normalized_tool,\n",
    "                        'original': tool,\n",
    "                        'question_id': sample.get('task_id'),\n",
    "                        'level': sample.get('Level'),\n",
    "                        'has_file': sample.get('file_name') is not None\n",
    "                    })\n",
    "    \n",
    "    # Count tool frequencies\n",
    "    tools_counter = OrderedDict(Counter(tools).most_common())\n",
    "    \n",
    "    print(\"üéØ Fixed Tool Usage Priority Analysis:\")\n",
    "    print(f\"Total tool instances: {len(tools)}\")\n",
    "    print(f\"Unique tools identified: {len(tools_counter)}\")\n",
    "    print(\"\\nüìä Implementation Priority (by frequency):\")\n",
    "    \n",
    "    for i, (tool, count) in enumerate(tools_counter.items(), 1):\n",
    "        if count >= 20:\n",
    "            priority = \"üî¥ CRITICAL\"\n",
    "        elif count >= 10:\n",
    "            priority = \"üü† HIGH\"\n",
    "        elif count >= 5:\n",
    "            priority = \"üü° MEDIUM\"\n",
    "        else:\n",
    "            priority = \"üü¢ LOW\"\n",
    "        \n",
    "        print(f\"  {i:2d}. {tool:<25} : {count:3d} occurrences {priority}\")\n",
    "        \n",
    "        if i <= 10:  # Show top 10 details\n",
    "            # Show which levels use this tool most\n",
    "            level_usage = {}\n",
    "            for detail in tool_details:\n",
    "                if detail['tool'] == tool:\n",
    "                    level = detail['level']\n",
    "                    level_usage[level] = level_usage.get(level, 0) + 1\n",
    "            \n",
    "            level_str = \", \".join([f\"L{k}:{v}\" for k, v in sorted(level_usage.items())])\n",
    "            print(f\"      ‚îî‚îÄ‚îÄ Level usage: {level_str}\")\n",
    "    \n",
    "    return tools_counter, tool_details\n",
    "\n",
    "def create_implementation_roadmap(tools_counter):\n",
    "    \"\"\"Create implementation roadmap based on tool frequency\"\"\"\n",
    "    \n",
    "    print(\"\\nüöÄ GAIA Agent Implementation Roadmap:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Group tools by implementation priority\n",
    "    critical_tools = []\n",
    "    high_tools = []\n",
    "    medium_tools = []\n",
    "    \n",
    "    for tool, count in tools_counter.items():\n",
    "        if count >= 20:\n",
    "            critical_tools.append((tool, count))\n",
    "        elif count >= 10:\n",
    "            high_tools.append((tool, count))\n",
    "        elif count >= 5:\n",
    "            medium_tools.append((tool, count))\n",
    "    \n",
    "    print(\"üî¥ PHASE 1 - CRITICAL (implement first):\")\n",
    "    for tool, count in critical_tools:\n",
    "        print(f\"  ‚úÖ {tool} ({count} uses)\")\n",
    "    \n",
    "    print(\"\\nüü† PHASE 2 - HIGH PRIORITY:\")\n",
    "    for tool, count in high_tools:\n",
    "        print(f\"  üîß {tool} ({count} uses)\")\n",
    "    \n",
    "    print(\"\\nüü° PHASE 3 - MEDIUM PRIORITY:\")\n",
    "    for tool, count in medium_tools:\n",
    "        print(f\"  ‚öôÔ∏è {tool} ({count} uses)\")\n",
    "    \n",
    "    # Map to actual tool implementations\n",
    "    print(\"\\nüõ†Ô∏è RECOMMENDED TOOL MAPPING:\")\n",
    "    tool_mapping = {\n",
    "        'web browser': 'ContentRetrieverTool + WebDriverTool',\n",
    "        'search engine': 'GoogleSearchTool + SerperTool', \n",
    "        'calculator': 'GAIACalculatorTool + PythonREPL',\n",
    "        'excel': 'GetAttachmentTool + PandasTool',\n",
    "        'pdf viewer': 'ContentRetrieverTool + PyPDFTool',\n",
    "        'image recognition': 'VisionTool + ImageAnalysisTool',\n",
    "        'text editor': 'TextProcessingTool',\n",
    "        'file manager': 'GetAttachmentTool + FileSystemTool'\n",
    "    }\n",
    "    \n",
    "    for tool, count in list(tools_counter.items())[:8]:\n",
    "        implementation = tool_mapping.get(tool, f\"Custom{tool.title().replace(' ', '')}Tool\")\n",
    "        print(f\"  {tool:<20} ‚Üí {implementation}\")\n",
    "\n",
    "# Run the fixed analysis\n",
    "tools_counter, tool_details = analyze_tool_usage_fixed()\n",
    "create_implementation_roadmap(tools_counter)\n",
    "\n",
    "# Show some examples of what was normalized\n",
    "print(\"\\nüîç Normalization Examples:\")\n",
    "unique_originals = {}\n",
    "for detail in tool_details[:20]:  # Show first 20\n",
    "    tool = detail['tool']\n",
    "    original = detail['original']\n",
    "    if tool not in unique_originals:\n",
    "        unique_originals[tool] = []\n",
    "    if original not in unique_originals[tool]:\n",
    "        unique_originals[tool].append(original)\n",
    "\n",
    "for tool, originals in list(unique_originals.items())[:5]:\n",
    "    if len(originals) > 1:\n",
    "        print(f\"  '{tool}' ‚Üê {originals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of tool usage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def create_tool_usage_visualization(tools_counter):\n",
    "    \"\"\"Create an enhanced visualization of tool usage with updated priority levels\"\"\"\n",
    "    \n",
    "    if not tools_counter:\n",
    "        print(\"‚ùå No tools data to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "    \n",
    "    # ===== MAIN BAR CHART =====\n",
    "    # Top 15 tools\n",
    "    top_tools = list(tools_counter.items())[:15]\n",
    "    tool_names = [item[0] for item in top_tools]\n",
    "    tool_counts = [item[1] for item in top_tools]\n",
    "    \n",
    "    # Updated color scheme with new thresholds\n",
    "    colors = []\n",
    "    for count in tool_counts:\n",
    "        if count >= 20:\n",
    "            colors.append('#DC2626')  # Red - CRITICAL\n",
    "        elif count >= 10:\n",
    "            colors.append('#F59E0B')  # Orange - HIGH\n",
    "        elif count >= 5:\n",
    "            colors.append('#10B981')  # Green - MEDIUM\n",
    "        else:\n",
    "            colors.append('#6B7280')  # Gray - LOW\n",
    "    \n",
    "    bars = ax1.barh(range(len(tool_names)), tool_counts, color=colors, alpha=0.8)\n",
    "    ax1.set_yticks(range(len(tool_names)))\n",
    "    ax1.set_yticklabels(tool_names, fontsize=10)\n",
    "    ax1.set_xlabel('Usage Frequency', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('GAIA Tool Usage Analysis\\n(Normalized & Cleaned)', fontsize=14, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, tool_counts)):\n",
    "        # Position label inside bar if bar is wide enough, otherwise outside\n",
    "        label_x = bar.get_width() - 2 if bar.get_width() > 10 else bar.get_width() + 0.5\n",
    "        label_color = 'white' if bar.get_width() > 10 else 'black'\n",
    "        \n",
    "        ax1.text(label_x, bar.get_y() + bar.get_height()/2, \n",
    "                str(count), va='center', ha='right' if bar.get_width() > 10 else 'left',\n",
    "                fontweight='bold', color=label_color, fontsize=9)\n",
    "    \n",
    "    # Add priority zone backgrounds\n",
    "    ax1.axvspan(20, max(tool_counts) + 5, alpha=0.1, color='red', label='Critical Zone')\n",
    "    ax1.axvspan(10, 20, alpha=0.1, color='orange', label='High Zone')\n",
    "    ax1.axvspan(5, 10, alpha=0.1, color='green', label='Medium Zone')\n",
    "    \n",
    "    # Enhanced legend\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#DC2626', label='üî¥ CRITICAL (‚â•20 uses)'),\n",
    "        Patch(facecolor='#F59E0B', label='üü† HIGH (10-19 uses)'),\n",
    "        Patch(facecolor='#10B981', label='üü° MEDIUM (5-9 uses)'),\n",
    "        Patch(facecolor='#6B7280', label='üü¢ LOW (<5 uses)')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='lower right', fontsize=10)\n",
    "    \n",
    "    # ===== IMPLEMENTATION PRIORITY PIE CHART =====\n",
    "    # Calculate priority distribution\n",
    "    critical_count = sum(1 for count in tools_counter.values() if count >= 20)\n",
    "    high_count = sum(1 for count in tools_counter.values() if 10 <= count < 20)\n",
    "    medium_count = sum(1 for count in tools_counter.values() if 5 <= count < 10)\n",
    "    low_count = sum(1 for count in tools_counter.values() if count < 5)\n",
    "    \n",
    "    priority_labels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']\n",
    "    priority_counts = [critical_count, high_count, medium_count, low_count]\n",
    "    priority_colors = ['#DC2626', '#F59E0B', '#10B981', '#6B7280']\n",
    "    \n",
    "    # Only show non-zero segments\n",
    "    non_zero_data = [(label, count, color) for label, count, color in \n",
    "                     zip(priority_labels, priority_counts, priority_colors) if count > 0]\n",
    "    \n",
    "    if non_zero_data:\n",
    "        labels, counts, colors = zip(*non_zero_data)\n",
    "        \n",
    "        wedges, texts, autotexts = ax2.pie(counts, labels=labels, colors=colors, autopct='%1.0f%%',\n",
    "                                          startangle=90, textprops={'fontsize': 10})\n",
    "        \n",
    "        # Enhance pie chart text\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "        \n",
    "        ax2.set_title('Implementation Priority Distribution\\n(Tool Count by Priority)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add total count in center\n",
    "        total_tools = len(tools_counter)\n",
    "        ax2.text(0, 0, f'{total_tools}\\nTotal\\nTools', ha='center', va='center',\n",
    "                fontsize=14, fontweight='bold', \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ===== SUMMARY STATS =====\n",
    "    print(\"\\nüìä VISUALIZATION SUMMARY:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total unique tools: {len(tools_counter)}\")\n",
    "    print(f\"Total tool instances: {sum(tools_counter.values())}\")\n",
    "    print(f\"üî¥ Critical tools (‚â•20): {critical_count}\")\n",
    "    print(f\"üü† High priority (10-19): {high_count}\")\n",
    "    print(f\"üü° Medium priority (5-9): {medium_count}\")\n",
    "    print(f\"üü¢ Low priority (<5): {low_count}\")\n",
    "    \n",
    "    # Show top 5 with percentages\n",
    "    total_instances = sum(tools_counter.values())\n",
    "    print(f\"\\nüéØ TOP 5 TOOLS (% of total usage):\")\n",
    "    for i, (tool, count) in enumerate(list(tools_counter.items())[:5], 1):\n",
    "        percentage = (count / total_instances) * 100\n",
    "        print(f\"  {i}. {tool:<20}: {count:3d} uses ({percentage:5.1f}%)\")\n",
    "\n",
    "def create_level_breakdown_chart(tool_details):\n",
    "    \"\"\"Additional chart showing tool usage by GAIA level\"\"\"\n",
    "    \n",
    "    if not tool_details:\n",
    "        return\n",
    "    \n",
    "    # Analyze tool usage by level\n",
    "    level_tool_usage = {}\n",
    "    for detail in tool_details:\n",
    "        level = detail.get('level', 'Unknown')\n",
    "        tool = detail['tool']\n",
    "        \n",
    "        if level not in level_tool_usage:\n",
    "            level_tool_usage[level] = {}\n",
    "        \n",
    "        level_tool_usage[level][tool] = level_tool_usage[level].get(tool, 0) + 1\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get top 10 tools\n",
    "    from collections import Counter\n",
    "    all_tools = [detail['tool'] for detail in tool_details]\n",
    "    top_10_tools = [tool for tool, _ in Counter(all_tools).most_common(10)]\n",
    "    \n",
    "    # Prepare data for stacked bars\n",
    "    levels = sorted(level_tool_usage.keys())\n",
    "    level_data = {level: [] for level in levels}\n",
    "    \n",
    "    for tool in top_10_tools:\n",
    "        for level in levels:\n",
    "            count = level_tool_usage[level].get(tool, 0)\n",
    "            level_data[level].append(count)\n",
    "    \n",
    "    # Create stacked bars\n",
    "    bottom = np.zeros(len(top_10_tools))\n",
    "    colors_level = ['#EF4444', '#F59E0B', '#10B981']  # Red, Orange, Green for levels 1,2,3\n",
    "    \n",
    "    for i, level in enumerate(levels):\n",
    "        plt.bar(top_10_tools, level_data[level], bottom=bottom, \n",
    "               label=f'Level {level}', color=colors_level[i % len(colors_level)], alpha=0.8)\n",
    "        bottom += level_data[level]\n",
    "    \n",
    "    plt.xlabel('Tools', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Usage Count', fontsize=12, fontweight='bold')\n",
    "    plt.title('Tool Usage by GAIA Difficulty Level\\n(Top 10 Tools)', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='GAIA Level', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage after running the fixed analysis:\n",
    "if 'tools_counter' in globals() and tools_counter:\n",
    "    print(\"üé® Creating enhanced visualizations...\")\n",
    "    create_tool_usage_visualization(tools_counter)\n",
    "    \n",
    "    if 'tool_details' in globals() and tool_details:\n",
    "        create_level_breakdown_chart(tool_details)\n",
    "else:\n",
    "    print(\"‚ùå Run the fixed tool analysis first to generate visualizations\")\n",
    "    print(\"Execute: tools_counter, tool_details = analyze_tool_usage_fixed()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_implementation_recommendations(tools_counter):\n",
    "    \"\"\"Generate data-driven tool implementation recommendations\"\"\"\n",
    "    if not tools_counter:\n",
    "        return\n",
    "    \n",
    "    print(\"\\nüí° Implementation Recommendations:\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Essential tools (high frequency)\n",
    "    essential = [(tool, count) for tool, count in tools_counter.items() if count >= 10]\n",
    "    important = [(tool, count) for tool, count in tools_counter.items() if 5 <= count < 10]\n",
    "    optional = [(tool, count) for tool, count in tools_counter.items() if count < 5]\n",
    "    \n",
    "    print(f\"üî¥ ESSENTIAL TOOLS (Implement First):\")\n",
    "    for tool, count in essential:\n",
    "        print(f\"  ‚îú‚îÄ‚îÄ {tool}: {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\nüü° IMPORTANT TOOLS (Implement Second):\")\n",
    "    for tool, count in important:\n",
    "        print(f\"  ‚îú‚îÄ‚îÄ {tool}: {count} occurrences\")\n",
    "    \n",
    "    print(f\"\\nüü¢ OPTIONAL TOOLS (If Budget Allows):\")\n",
    "    for tool, count in optional[:5]:  # Show top 5 optional\n",
    "        print(f\"  ‚îú‚îÄ‚îÄ {tool}: {count} occurrences\")\n",
    "    \n",
    "    # File type analysis\n",
    "    print(f\"\\nüìÅ File Processing Requirements:\")\n",
    "    file_questions = [q for q in json_QA if q.get('file_name')]\n",
    "    if file_questions:\n",
    "        file_extensions = []\n",
    "        for q in file_questions:\n",
    "            filename = q.get('file_name', '')\n",
    "            if '.' in filename:\n",
    "                ext = Path(filename).suffix.lower()\n",
    "                file_extensions.append(ext)\n",
    "        \n",
    "        ext_counts = Counter(file_extensions)\n",
    "        for ext, count in ext_counts.most_common():\n",
    "            print(f\"  ‚îú‚îÄ‚îÄ {ext}: {count} files\")\n",
    "\n",
    "# Generate recommendations\n",
    "generate_implementation_recommendations(tools_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Load Weaviate Vector Store for question answer samples via retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for running instances of Weaviate\n",
    "def cleanup_weaviate_connections():\n",
    "    \"\"\"Close any existing Weaviate connections\"\"\"\n",
    "    try:\n",
    "        # Try to connect to existing instance and close it\n",
    "        import weaviate\n",
    "        client = weaviate.connect_to_local(port=8080, grpc_port=50051)\n",
    "        if client:\n",
    "            client.close()\n",
    "            print(\"‚úÖ Closed existing Weaviate connection\")\n",
    "    except:\n",
    "        pass  # No existing connection to close\n",
    "\n",
    "# Run cleanup first\n",
    "cleanup_weaviate_connections()\n",
    "\n",
    "# Wait a moment for ports to be released\n",
    "import time\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "from dev_retriever import load_gaia_retriever, DevelopmentGAIARetriever\n",
    "from langchain_core.messages import HumanMessage  # Correct import\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dev_retriever import load_gaia_retriever\n",
    "\n",
    "# Quick setup\n",
    "retriever = load_gaia_retriever()\n",
    "\n",
    "# Test it works\n",
    "retriever.test_retrieval()\n",
    "\n",
    "# Try a search\n",
    "results = retriever.search(\"Calculate compound interest\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_setup():\n",
    "    \"\"\"Test basic Weaviate setup and CSV loading\"\"\"\n",
    "    print(\"üß™ TEST 1: Basic Setup and Connection\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if CSV exists\n",
    "    import os\n",
    "    if not os.path.exists('gaia_embeddings.csv'):\n",
    "        print(\"‚ùå gaia_embeddings.csv not found!\")\n",
    "        print(\"üí° Run 'python build_vectorstore.py' first\")\n",
    "        return False\n",
    "    \n",
    "    # Load CSV and check structure\n",
    "    try:\n",
    "        df = pd.read_csv('gaia_embeddings.csv')\n",
    "        print(f\"‚úÖ CSV loaded: {len(df)} documents\")\n",
    "        \n",
    "        # Check required columns for OPTIMIZED format\n",
    "        required_cols = ['content', 'source', 'embedding_b64']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return False\n",
    "        \n",
    "        print(\"‚úÖ All required columns present\")\n",
    "        \n",
    "        # Test optimized embedding format\n",
    "        sample_embedding_b64 = df.iloc[0]['embedding_b64']\n",
    "        embedding_bytes = base64.b64decode(sample_embedding_b64)\n",
    "        embedding_vector = np.frombuffer(embedding_bytes, dtype=np.float32)\n",
    "        print(f\"‚úÖ Optimized embedding dimension: {len(embedding_vector)}\")\n",
    "        \n",
    "        # Test simplified metadata format\n",
    "        sample_source = df.iloc[0]['source']\n",
    "        print(f\"‚úÖ Metadata format: {{'source': '{sample_source}'}}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CSV test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run basic setup test\n",
    "setup_success = test_basic_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_basic_search(retriever):\n",
    "    \"\"\"Test basic search functionality\"\"\"\n",
    "    print(\"\\nüß™ TEST 2: Basic Search Functionality\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not retriever:\n",
    "        print(\"‚ùå No retriever available\")\n",
    "        return False\n",
    "    \n",
    "    test_queries = [\n",
    "        \"Calculate compound interest\",\n",
    "        \"What is the population of a city?\",\n",
    "        \"Analyze data in Excel file\",\n",
    "        \"Find information about scientific research\"\n",
    "    ]\n",
    "    \n",
    "    search_success = 0\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîç Test Query {i}: '{query}'\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            results = retriever.search(query, k=1)\n",
    "            search_time = time.time() - start_time\n",
    "            \n",
    "            if results:\n",
    "                print(f\"‚úÖ Found result in {search_time:.3f}s\")\n",
    "                print(f\"   Content preview: {results[0].page_content[:100]}...\")\n",
    "                print(f\"   Source: {results[0].metadata.get('source', 'N/A')}\")\n",
    "                search_success += 1\n",
    "            else:\n",
    "                print(\"‚ùå No results found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search error: {e}\")\n",
    "    \n",
    "    success_rate = search_success / len(test_queries)\n",
    "    print(f\"\\nüìä Search Success Rate: {search_success}/{len(test_queries)} ({success_rate:.1%})\")\n",
    "    \n",
    "    return success_rate > 0.5\n",
    "\n",
    "# Run basic search test\n",
    "if retriever:\n",
    "    search_success = test_basic_search(retriever)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping search test - no retriever\")\n",
    "    search_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example_student_interface(retriever):\n",
    "    \"\"\"Test the exact interface used by example student\"\"\"\n",
    "    print(\"\\nüß™ TEST 3: Example Student's Retriever Interface\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not retriever:\n",
    "        print(\"‚ùå No retriever available\")\n",
    "        return False\n",
    "    \n",
    "    # Simulate agent state messages (like example student)\n",
    "    test_questions = [\n",
    "        \"How do I calculate the area of a circle?\",\n",
    "        \"What's the GDP of France?\",\n",
    "        \"How can I analyze this Excel spreadsheet?\"\n",
    "    ]\n",
    "    \n",
    "    interface_success = 0\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\nüîÑ Testing Example Student Interface {i}\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        \n",
    "        try:\n",
    "            # Create state messages like #1 student's agent\n",
    "            state_messages = [HumanMessage(content=question)]\n",
    "            \n",
    "            # Use #1 student's exact retriever node interface\n",
    "            result = retriever.retriever_node(state_messages)\n",
    "            \n",
    "            if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "                # Extract the example message (should be last)\n",
    "                example_msg = result[\"messages\"][-1]\n",
    "                \n",
    "                print(\"‚úÖ Retriever node works\")\n",
    "                print(f\"   Retrieved example length: {len(example_msg.content)} chars\")\n",
    "                \n",
    "                # Verify it contains the expected format\n",
    "                if \"Here I provide a similar question and answer for reference:\" in example_msg.content:\n",
    "                    print(\"‚úÖ Correct example student format\")\n",
    "                    interface_success += 1\n",
    "                else:\n",
    "                    print(\"‚ùå Incorrect format\")\n",
    "            else:\n",
    "                print(\"‚ùå No example returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Interface error: {e}\")\n",
    "    \n",
    "    success_rate = interface_success / len(test_questions)\n",
    "    print(f\"\\nüìä Interface Success Rate: {interface_success}/{len(test_questions)} ({success_rate:.1%})\")\n",
    "    \n",
    "    return success_rate > 0.5\n",
    "\n",
    "# Run example student interface test\n",
    "if retriever:\n",
    "    interface_success = test_example_student_interface(retriever)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping interface test - no retriever\")\n",
    "    interface_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "def test_example_interface(retriever):\n",
    "    \"\"\"Test the exact interface used by example student\"\"\"\n",
    "    print(\"\\nüß™ TEST 4: Example Student's Retriever Interface\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not retriever:\n",
    "        print(\"‚ùå No retriever available\")\n",
    "        return False\n",
    "    \n",
    "    # Simulate agent state messages (like #1 student)\n",
    "    test_questions = [\n",
    "        \"How do I calculate the area of a circle?\",\n",
    "        \"What's the GDP of France?\",\n",
    "        \"How can I analyze this Excel spreadsheet?\"\n",
    "    ]\n",
    "    \n",
    "    interface_success = 0\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\nüîÑ Testing Example Student Interface {i}\")\n",
    "        print(f\"Question: '{question}'\")\n",
    "        \n",
    "        try:\n",
    "            # Create state messages like example student's agent\n",
    "            state_messages = [HumanMessage(content=question)]\n",
    "            \n",
    "            # Use #1 student's exact retriever node interface\n",
    "            result = retriever.retriever_node(state_messages)\n",
    "            \n",
    "            if \"messages\" in result and len(result[\"messages\"]) > 1:\n",
    "                # Extract the example message (should be last)\n",
    "                example_msg = result[\"messages\"][-1]\n",
    "                \n",
    "                print(\"‚úÖ Retriever node works\")\n",
    "                print(f\"   Retrieved example length: {len(example_msg.content)} chars\")\n",
    "                \n",
    "                # Verify it contains the expected format\n",
    "                if \"Here I provide a similar question and answer for reference:\" in example_msg.content:\n",
    "                    print(\"‚úÖ Correct example student format\")\n",
    "                    interface_success += 1\n",
    "                else:\n",
    "                    print(\"‚ùå Incorrect format\")\n",
    "            else:\n",
    "                print(\"‚ùå No example returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Interface error: {e}\")\n",
    "    \n",
    "    success_rate = interface_success / len(test_questions)\n",
    "    print(f\"\\nüìä Interface Success Rate: {interface_success}/{len(test_questions)} ({success_rate:.1%})\")\n",
    "    \n",
    "    return success_rate > 0.5\n",
    "\n",
    "# Run example interface test\n",
    "if retriever:\n",
    "    interface_success = test_example_interface(retriever)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping interface test - no retriever\")\n",
    "    interface_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieval_quality(retriever):\n",
    "    \"\"\"Test quality of retrieved examples\"\"\"\n",
    "    print(\"\\nüß™ TEST 5: Retrieval Quality Assessment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not retriever:\n",
    "        print(\"‚ùå No retriever available\")\n",
    "        return False\n",
    "    \n",
    "    # Load original Q&A for comparison\n",
    "    try:\n",
    "        with open('metadata.jsonl', 'r') as f:\n",
    "            original_qa = [json.loads(line) for line in f]\n",
    "    except:\n",
    "        print(\"‚ùå Cannot load original Q&A for comparison\")\n",
    "        return False\n",
    "    \n",
    "    quality_scores = []\n",
    "    \n",
    "    # Test 1: Same questions (should find exact matches)\n",
    "    print(\"üéØ Test 1: Exact retrieval (using original questions)\")\n",
    "    sample_questions = random.sample(original_qa, min(3, len(original_qa)))\n",
    "    \n",
    "    for i, sample in enumerate(sample_questions, 1):\n",
    "        question = sample['Question']\n",
    "        \n",
    "        print(f\"\\nüîç Exact Test {i}\")\n",
    "        print(f\"Query: {question[:80]}...\")\n",
    "        \n",
    "        try:\n",
    "            results = retriever.search(question, k=1)\n",
    "            \n",
    "            if results:\n",
    "                result_content = results[0].page_content\n",
    "                \n",
    "                # Check for exact match (this is GOOD!)\n",
    "                if question in result_content:\n",
    "                    print(\"‚úÖ Perfect exact match found\")\n",
    "                    quality_scores.append(1.0)\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  No exact match - might be similarity threshold issue\")\n",
    "                    quality_scores.append(0.7)\n",
    "                \n",
    "                print(f\"   Retrieved: {results[0].page_content[:100]}...\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå No results retrieved\")\n",
    "                quality_scores.append(0.0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search error: {e}\")\n",
    "            quality_scores.append(0.0)\n",
    "    \n",
    "    # Test 2: Similar questions (test generalization)\n",
    "    print(\"\\nüéØ Test 2: Similarity retrieval (using modified questions)\")\n",
    "    \n",
    "    similarity_tests = [\n",
    "        {\n",
    "            \"original\": \"Calculate compound interest on $5000 at 3% for 10 years\",\n",
    "            \"modified\": \"How do I compute compound interest for an investment?\",\n",
    "            \"expected_keywords\": [\"interest\", \"calculate\", \"compound\"]\n",
    "        },\n",
    "        {\n",
    "            \"original\": \"What is the population of Seattle?\", \n",
    "            \"modified\": \"How many people live in a major US city?\",\n",
    "            \"expected_keywords\": [\"population\", \"city\", \"people\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test in enumerate(similarity_tests, 1):\n",
    "        print(f\"\\nüîç Similarity Test {i}\")\n",
    "        print(f\"Query: {test['modified']}\")\n",
    "        \n",
    "        try:\n",
    "            results = retriever.search(test['modified'], k=3)\n",
    "            \n",
    "            if results:\n",
    "                # Check if retrieved results are relevant\n",
    "                found_relevant = False\n",
    "                \n",
    "                for result in results:\n",
    "                    result_content = result.page_content.lower()\n",
    "                    \n",
    "                    # Check for keyword relevance\n",
    "                    keyword_matches = sum(1 for keyword in test['expected_keywords'] \n",
    "                                        if keyword.lower() in result_content)\n",
    "                    \n",
    "                    if keyword_matches >= 1:\n",
    "                        found_relevant = True\n",
    "                        break\n",
    "                \n",
    "                if found_relevant:\n",
    "                    print(\"‚úÖ Found relevant similar example\")\n",
    "                    quality_scores.append(1.0)\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Retrieved example seems unrelated\")\n",
    "                    quality_scores.append(0.3)\n",
    "                \n",
    "                print(f\"   Retrieved: {results[0].page_content[:100]}...\")\n",
    "                \n",
    "            else:\n",
    "                print(\"‚ùå No results retrieved\")\n",
    "                quality_scores.append(0.0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search error: {e}\")\n",
    "            quality_scores.append(0.0)\n",
    "    \n",
    "    avg_quality = np.mean(quality_scores) if quality_scores else 0\n",
    "    print(f\"\\nüìä Average Quality Score: {avg_quality:.2f}/1.0\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if avg_quality >= 0.9:\n",
    "        print(\"üéâ Excellent retrieval quality!\")\n",
    "    elif avg_quality >= 0.7:\n",
    "        print(\"‚úÖ Good retrieval quality\")\n",
    "    elif avg_quality >= 0.5:\n",
    "        print(\"‚ö†Ô∏è  Acceptable retrieval quality\")\n",
    "    else:\n",
    "        print(\"‚ùå Poor retrieval quality - needs investigation\")\n",
    "    \n",
    "    return avg_quality > 0.6\n",
    "\n",
    "# Run quality assessment\n",
    "if retriever:\n",
    "    quality_success = test_retrieval_quality(retriever)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping quality test - no retriever\")\n",
    "    quality_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance_benchmarks(retriever):\n",
    "    \"\"\"Test retrieval performance and speed\"\"\"\n",
    "    print(\"\\nüß™ TEST 6: Performance Benchmarks\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not retriever:\n",
    "        print(\"‚ùå No retriever available\")\n",
    "        return False\n",
    "    \n",
    "    # Speed test\n",
    "    test_queries = [\n",
    "        \"Calculate compound interest\",\n",
    "        \"Population data analysis\", \n",
    "        \"Excel spreadsheet processing\",\n",
    "        \"Scientific research information\",\n",
    "        \"Mathematical computation\"\n",
    "    ]\n",
    "    \n",
    "    search_times = []\n",
    "    \n",
    "    print(\"üöÄ Speed Test:\")\n",
    "    for query in test_queries:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            results = retriever.search(query, k=1)\n",
    "            search_time = time.time() - start_time\n",
    "            search_times.append(search_time)\n",
    "            print(f\"  {query[:30]:<30}: {search_time:.3f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {query[:30]:<30}: ERROR ({e})\")\n",
    "    \n",
    "    if search_times:\n",
    "        avg_time = np.mean(search_times)\n",
    "        max_time = max(search_times)\n",
    "        min_time = min(search_times)\n",
    "        \n",
    "        print(f\"\\nüìä Performance Results:\")\n",
    "        print(f\"  Average search time: {avg_time:.3f}s\")\n",
    "        print(f\"  Min/Max time: {min_time:.3f}s / {max_time:.3f}s\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        if avg_time < 0.1:\n",
    "            print(\"‚úÖ Excellent performance (<100ms)\")\n",
    "            return True\n",
    "        elif avg_time < 0.5:\n",
    "            print(\"‚úÖ Good performance (<500ms)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Slow performance (>500ms)\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå No successful searches for performance test\")\n",
    "        return False\n",
    "\n",
    "# Run performance test\n",
    "if retriever:\n",
    "    performance_success = test_performance_benchmarks(retriever)\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping performance test - no retriever\")\n",
    "    performance_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_summary():\n",
    "    \"\"\"Print comprehensive test summary\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ WEAVIATE VECTOR STORE TEST SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    tests = [\n",
    "        (\"CSV Setup\", setup_success if 'setup_success' in globals() else False),\n",
    "        (\"Retriever Init\", retriever is not None if 'retriever' in globals() else False),\n",
    "        (\"Basic Search\", search_success if 'search_success' in globals() else False),\n",
    "        (\"Example Student Interface\", interface_success if 'interface_success' in globals() else False),\n",
    "        (\"Retrieval Quality\", quality_success if 'quality_success' in globals() else False),\n",
    "        (\"Performance\", performance_success if 'performance_success' in globals() else False)\n",
    "    ]\n",
    "    \n",
    "    passed_tests = sum(1 for _, success in tests if success)\n",
    "    total_tests = len(tests)\n",
    "    \n",
    "    print(f\"Test Results: {passed_tests}/{total_tests} passed\")\n",
    "    print()\n",
    "    \n",
    "    for test_name, success in tests:\n",
    "        status = \"‚úÖ PASS\" if success else \"‚ùå FAIL\"\n",
    "        print(f\"  {test_name:<20}: {status}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    if passed_tests == total_tests:\n",
    "        print(\"üéâ ALL TESTS PASSED!\")\n",
    "        print(\"‚úÖ Vector store is ready for agent integration\")\n",
    "        print(\"‚úÖ Compatible with example student's approach\")\n",
    "        print(\"‚úÖ Production ready\")\n",
    "    elif passed_tests >= total_tests * 0.8:\n",
    "        print(\"‚ö†Ô∏è  MOSTLY WORKING\")\n",
    "        print(\"‚úÖ Core functionality works\")\n",
    "        print(\"üí° Minor issues to address\")\n",
    "    else:\n",
    "        print(\"‚ùå SIGNIFICANT ISSUES\")\n",
    "        print(\"üîß Requires debugging before agent integration\")\n",
    "    \n",
    "    print(\"\\nüéØ Ready for agent development!\" if passed_tests >= total_tests * 0.8 else \"\\nüîß Fix issues before proceeding\")\n",
    "\n",
    "# Print final summary\n",
    "print_test_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "if 'retriever' in globals() and retriever:\n",
    "    retriever.close()\n",
    "    print(\"\\nüßπ Retriever closed and cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Creation of test batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gaia_dataset_utils\n",
    "\n",
    "# See all available functions\n",
    "print(\"Available functions:\", [attr for attr in dir(gaia_dataset_utils) if not attr.startswith('_')])\n",
    "\n",
    "# Check if the function exists\n",
    "if hasattr(gaia_dataset_utils, 'quick_dataset_check'):\n",
    "    print(\"‚úÖ quick_dataset_check found!\")\n",
    "else:\n",
    "    print(\"‚ùå quick_dataset_check missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaia_dataset_utils import quick_dataset_check, GAIADatasetManager\n",
    "\n",
    "# Check your dataset structure and file references\n",
    "quick_dataset_check(\"/tests/gaia_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Check what's actually in your metadata.json\n",
    "metadata_path = \"./tests/gaia_data/metadata.json\"\n",
    "\n",
    "if os.path.exists(metadata_path):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Metadata type: {type(data)}\")\n",
    "    print(f\"Metadata keys/length: {list(data.keys()) if isinstance(data, dict) else len(data)}\")\n",
    "    \n",
    "    # Show first item structure\n",
    "    if isinstance(data, dict):\n",
    "        first_key = list(data.keys())[0]\n",
    "        print(f\"First item: {first_key}: {data[first_key]}\")\n",
    "    elif isinstance(data, list):\n",
    "        print(f\"First item: {data[0] if data else 'Empty list'}\")\n",
    "    else:\n",
    "        print(f\"Unexpected data structure: {data}\")\n",
    "else:\n",
    "    print(f\"‚ùå metadata.json not found at {metadata_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
