{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Evaluator\n",
    "## Implementation plan, testing framework, and deployment\n",
    "\n",
    "**Objective:** Complete implementation roadmap and evaluation system  \n",
    "**Target:** 45-55% GAIA accuracy within $10 budget\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Your production system\n",
    "from gaia_agent_system import (\n",
    "    create_gaia_agent, \n",
    "    create_production_gaia_agent,\n",
    "    GAIAConfig,\n",
    "    ModelConfigs,\n",
    "    run_gaia_benchmark,\n",
    "    quick_test,\n",
    "    compare_configs\n",
    ")\n",
    "\n",
    "# Testing framework\n",
    "from hf_production_testing_agent_assignment import (\n",
    "    GAIAProductionTestFramework,\n",
    "    TestConfig,\n",
    "    run_production_test,\n",
    "    quick_agent_validation,\n",
    "    compare_agent_configs,\n",
    "    benchmark_best_config\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"üöÄ GAIA Production Evaluation Environment Ready\")\n",
    "print(f\"üìÖ Session: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Quick system validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that your system is working correctly\n",
    "print(\"üîß Testing Core Components...\")\n",
    "\n",
    "# Test retriever\n",
    "try:\n",
    "    from dev_retriever import load_gaia_retriever\n",
    "    retriever = load_gaia_retriever(\"gaia_embeddings.csv\")\n",
    "    if retriever and retriever.is_ready():\n",
    "        print(\"‚úÖ Retriever: Working\")\n",
    "        \n",
    "        # Test search\n",
    "        results = retriever.search(\"Calculate 15% of 100\", k=2)\n",
    "        print(f\"  ‚îú‚îÄ‚îÄ Retrieved {len(results)} similar examples\")\n",
    "    else:\n",
    "        print(\"‚ùå Retriever: Failed to initialize\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Retriever: Error - {e}\")\n",
    "\n",
    "# Test agent creation\n",
    "try:\n",
    "    agent = create_gaia_agent(\"qwen2.5_coder\")\n",
    "    print(\"‚úÖ Agent Creation: Working\")\n",
    "    agent.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Agent Creation: Error - {e}\")\n",
    "\n",
    "# Test model configurations\n",
    "configs = ModelConfigs.get_all_configs()\n",
    "print(f\"‚úÖ Model Configs: {len(configs)} configurations available\")\n",
    "for provider in ['openrouter', 'groq', 'google', 'ollama']:\n",
    "    provider_configs = [name for name, config in configs.items() \n",
    "                       if config['model_provider'] == provider]\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {provider}: {len(provider_configs)} configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify everything works\n",
    "print(\"\\nüß™ Quick Functionality Test...\")\n",
    "\n",
    "test_questions = [\n",
    "    \"Calculate 25% of 800\",\n",
    "    \"What is 15 + 27?\",\n",
    "    \"Convert 100 fahrenheit to celsius\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"\\nTest {i+1}: {question}\")\n",
    "    try:\n",
    "        result = quick_test(question, \"qwen2.5_coder\")\n",
    "        \n",
    "        if \"error\" not in result:\n",
    "            answer = result.get('final_answer', 'No answer')\n",
    "            strategy = result.get('selected_strategy', 'Unknown')\n",
    "            print(f\"  ‚îú‚îÄ‚îÄ Answer: {answer}\")\n",
    "            print(f\"  ‚îî‚îÄ‚îÄ Strategy: {strategy}\")\n",
    "        else:\n",
    "            print(f\"  ‚îî‚îÄ‚îÄ Error: {result['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚îî‚îÄ‚îÄ Exception: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Quick functionality test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Single Configuration Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into your best performing configuration\n",
    "best_config = \"qwen2.5_coder\"  # Change this to your preferred config\n",
    "\n",
    "print(f\"üîç Deep Dive Analysis: {best_config}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create testing framework\n",
    "test_config = TestConfig(\n",
    "    max_questions_per_config=25,\n",
    "    max_total_budget=3.0,  # Limit budget for this test\n",
    "    enable_performance_tracking=True,\n",
    "    generate_visualizations=True\n",
    ")\n",
    "\n",
    "framework = GAIAProductionTestFramework(test_config)\n",
    "\n",
    "# Run comprehensive test\n",
    "result = framework.test_single_configuration(best_config, 25)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nüìä DETAILED RESULTS FOR {best_config}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Questions: {result.get('total_questions', 0)}\")\n",
    "print(f\"Accuracy: {result.get('accuracy', 0):.3f} ({result.get('accuracy', 0)*100:.1f}%)\")\n",
    "print(f\"Average Time: {result.get('avg_execution_time', 0):.2f}s\")\n",
    "print(f\"Total Cost: ${result.get('total_cost', 0):.3f}\")\n",
    "print(f\"Error Rate: {result.get('error_count', 0) / result.get('total_questions', 1):.3f}\")\n",
    "print(f\"Fallback Usage: {result.get('fallback_usage', 0):.3f}\")\n",
    "\n",
    "# Level breakdown\n",
    "level_performance = result.get('level_breakdown', {})\n",
    "if level_performance:\n",
    "    print(f\"\\nüìà Performance by GAIA Level:\")\n",
    "    for level, stats in level_performance.items():\n",
    "        accuracy = stats.get('accuracy', 0)\n",
    "        total = stats.get('total_questions', 0)\n",
    "        avg_time = stats.get('avg_execution_time', 0)\n",
    "        print(f\"  ‚îú‚îÄ‚îÄ {level}: {accuracy:.3f} accuracy ({total} questions, {avg_time:.2f}s avg)\")\n",
    "\n",
    "# Strategy breakdown\n",
    "strategy_performance = result.get('strategy_breakdown', {})\n",
    "if strategy_performance:\n",
    "    print(f\"\\nüéØ Performance by Strategy:\")\n",
    "    for strategy, stats in strategy_performance.items():\n",
    "        accuracy = stats.get('accuracy', 0)\n",
    "        total = stats.get('total_questions', 0)\n",
    "        print(f\"  ‚îú‚îÄ‚îÄ {strategy}: {accuracy:.3f} accuracy ({total} questions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the detailed results\n",
    "detailed_results = result.get('detailed_results', [])\n",
    "if detailed_results:\n",
    "    df_detailed = pd.DataFrame(detailed_results)\n",
    "    \n",
    "    print(f\"\\nüî¨ Detailed Analysis:\")\n",
    "    print(f\"Total records: {len(df_detailed)}\")\n",
    "    \n",
    "    # Accuracy by level\n",
    "    if 'level' in df_detailed.columns:\n",
    "        level_accuracy = df_detailed.groupby('level')['is_correct'].agg(['mean', 'count'])\n",
    "        print(f\"\\nAccuracy by Level:\")\n",
    "        for level, stats in level_accuracy.iterrows():\n",
    "            print(f\"  Level {level}: {stats['mean']:.3f} ({stats['count']} questions)\")\n",
    "    \n",
    "    # Strategy effectiveness\n",
    "    if 'strategy_used' in df_detailed.columns:\n",
    "        strategy_accuracy = df_detailed.groupby('strategy_used')['is_correct'].agg(['mean', 'count'])\n",
    "        print(f\"\\nStrategy Effectiveness:\")\n",
    "        for strategy, stats in strategy_accuracy.iterrows():\n",
    "            print(f\"  {strategy}: {stats['mean']:.3f} ({stats['count']} uses)\")\n",
    "    \n",
    "    # Error analysis\n",
    "    error_questions = df_detailed[df_detailed['errors'].apply(lambda x: len(x) if isinstance(x, list) else 0) > 0]\n",
    "    if len(error_questions) > 0:\n",
    "        print(f\"\\nError Analysis:\")\n",
    "        print(f\"  Questions with errors: {len(error_questions)}\")\n",
    "        print(f\"  Error rate: {len(error_questions)/len(df_detailed):.3f}\")\n",
    "        \n",
    "        # Show sample errors\n",
    "        print(\"  Sample errors:\")\n",
    "        for i, (_, row) in enumerate(error_questions.head(3).iterrows()):\n",
    "            question = row['question'][:50] + \"...\" if len(row['question']) > 50 else row['question']\n",
    "            errors = row['errors']\n",
    "            print(f\"    {i+1}. {question}\")\n",
    "            if isinstance(errors, list) and errors:\n",
    "                print(f\"       Error: {errors[0]}\")\n",
    "    \n",
    "    # Show some correct and incorrect examples\n",
    "    print(f\"\\n‚úÖ Sample Correct Answers:\")\n",
    "    correct_samples = df_detailed[df_detailed['is_correct'] == True].head(3)\n",
    "    for i, (_, row) in enumerate(correct_samples.iterrows()):\n",
    "        question = row['question'][:50] + \"...\" if len(row['question']) > 50 else row['question']\n",
    "        print(f\"  {i+1}. Q: {question}\")\n",
    "        print(f\"     A: {row['predicted_answer']} (Expected: {row['ground_truth']})\")\n",
    "    \n",
    "    print(f\"\\n‚ùå Sample Incorrect Answers:\")\n",
    "    incorrect_samples = df_detailed[df_detailed['is_correct'] == False].head(3)\n",
    "    for i, (_, row) in enumerate(incorrect_samples.iterrows()):\n",
    "        question = row['question'][:50] + \"...\" if len(row['question']) > 50 else row['question']\n",
    "        print(f\"  {i+1}. Q: {question}\")\n",
    "        print(f\"     A: {row['predicted_answer']} (Expected: {row['ground_truth']})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No detailed results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Multi-Configuration Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare your top configurations\n",
    "print(\"üèÜ Multi-Configuration Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select configurations to compare\n",
    "configs_to_compare = [\n",
    "    \"qwen2.5_coder\",      # OpenRouter - High performance\n",
    "    \"qwen_qwq_groq\",      # Groq - Fast execution\n",
    "    \"gemini_flash_04\",    # Google - Balanced\n",
    "    \"deepseek\"            # OpenRouter - Alternative\n",
    "]\n",
    "\n",
    "print(f\"Comparing {len(configs_to_compare)} configurations:\")\n",
    "for config in configs_to_compare:\n",
    "    model_info = ModelConfigs.get_all_configs().get(config, {})\n",
    "    provider = model_info.get('model_provider', 'unknown')\n",
    "    model = model_info.get('primary_model', 'unknown')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {config}: {provider}/{model}\")\n",
    "\n",
    "# Run comparison with budget management\n",
    "comparison_df = compare_agent_configs(configs_to_compare, sample_size=15)\n",
    "\n",
    "print(f\"\\nüìä COMPARISON RESULTS\")\n",
    "print(\"=\" * 30)\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Find best performers\n",
    "best_accuracy = comparison_df.loc[comparison_df['accuracy'].idxmax()]\n",
    "fastest_model = comparison_df.loc[comparison_df['avg_time'].idxmin()]\n",
    "most_efficient = comparison_df.loc[(comparison_df['accuracy'] / comparison_df['total_cost']).idxmax()]\n",
    "\n",
    "print(f\"\\nüèÜ TOP PERFORMERS\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Best Accuracy: {best_accuracy['config']} ({best_accuracy['accuracy']:.3f})\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Fastest: {fastest_model['config']} ({fastest_model['avg_time']:.2f}s)\")\n",
    "print(f\"‚îî‚îÄ‚îÄ Most Cost-Efficient: {most_efficient['config']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "axes[0,0].bar(comparison_df['config'], comparison_df['accuracy'])\n",
    "axes[0,0].set_title('Accuracy by Configuration')\n",
    "axes[0,0].set_ylabel('Accuracy')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].axhline(y=0.45, color='red', linestyle='--', label='GAIA Target')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(comparison_df['accuracy']):\n",
    "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. Execution time comparison\n",
    "axes[0,1].bar(comparison_df['config'], comparison_df['avg_time'])\n",
    "axes[0,1].set_title('Average Execution Time')\n",
    "axes[0,1].set_ylabel('Time (seconds)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Cost vs Performance\n",
    "axes[1,0].scatter(comparison_df['total_cost'], comparison_df['accuracy'], s=100)\n",
    "for i, config in enumerate(comparison_df['config']):\n",
    "    axes[1,0].annotate(config, \n",
    "                      (comparison_df.iloc[i]['total_cost'], comparison_df.iloc[i]['accuracy']),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[1,0].set_xlabel('Total Cost ($)')\n",
    "axes[1,0].set_ylabel('Accuracy')\n",
    "axes[1,0].set_title('Cost vs Performance')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Error rates\n",
    "axes[1,1].bar(comparison_df['config'], comparison_df['error_rate'])\n",
    "axes[1,1].set_title('Error Rate by Configuration')\n",
    "axes[1,1].set_ylabel('Error Rate')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\nüìã SUMMARY TABLE\")\n",
    "summary_table = comparison_df[['config', 'accuracy', 'avg_time', 'total_cost', 'error_rate']].copy()\n",
    "summary_table['gaia_target_met'] = summary_table['accuracy'] >= 0.45\n",
    "summary_table['cost_efficiency'] = summary_table['accuracy'] / summary_table['total_cost']\n",
    "\n",
    "print(summary_table.round(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Production Readiness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Run comprehensive production test with best configuration\n",
    "print(\"üöÄ Production Readiness Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Identify best configuration from comparison\n",
    "best_config_name = comparison_df.loc[comparison_df['accuracy'].idxmax(), 'config']\n",
    "print(f\"Testing production readiness with: {best_config_name}\")\n",
    "\n",
    "# Configure production test\n",
    "production_config = TestConfig(\n",
    "    max_questions_per_config=50,  # Substantial test\n",
    "    max_total_budget=5.0,         # Reserve budget for final submission\n",
    "    enable_model_comparison=False,\n",
    "    enable_level_analysis=True,\n",
    "    enable_error_analysis=True,\n",
    "    enable_performance_tracking=True,\n",
    "    generate_visualizations=True\n",
    ")\n",
    "\n",
    "# Run production test\n",
    "production_framework = GAIAProductionTestFramework(production_config)\n",
    "production_results = production_framework.run_comprehensive_evaluation([best_config_name], 50)\n",
    "\n",
    "# Extract results\n",
    "eval_results = production_results['evaluation_results']\n",
    "analysis = production_results['analysis']\n",
    "\n",
    "print(f\"\\nüéØ PRODUCTION ASSESSMENT RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Overall metrics\n",
    "summary = analysis.get('summary', {})\n",
    "print(f\"Total Questions: {summary.get('total_questions_tested', 0)}\")\n",
    "print(f\"Overall Accuracy: {summary.get('overall_accuracy', 0):.3f} ({summary.get('overall_accuracy', 0)*100:.1f}%)\")\n",
    "print(f\"Average Response Time: {summary.get('average_execution_time', 0):.2f}s\")\n",
    "print(f\"Total Cost: ${summary.get('total_cost', 0):.3f}\")\n",
    "print(f\"Error Rate: {summary.get('error_rate', 0):.3f}\")\n",
    "\n",
    "# GAIA compliance\n",
    "gaia_compliance = analysis.get('gaia_compliance', {})\n",
    "target_met = gaia_compliance.get('meets_target', False)\n",
    "target_accuracy = gaia_compliance.get('target_accuracy', 0.45)\n",
    "achieved_accuracy = gaia_compliance.get('achieved_accuracy', 0)\n",
    "\n",
    "print(f\"\\nüéØ GAIA Compliance Assessment:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Target Accuracy: {target_accuracy:.1%}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Achieved Accuracy: {achieved_accuracy:.1%}\")\n",
    "print(f\"‚îî‚îÄ‚îÄ Target Met: {'‚úÖ YES' if target_met else '‚ùå NO'}\")\n",
    "\n",
    "# Level performance\n",
    "level_performance = gaia_compliance.get('level_performance_distribution', {})\n",
    "if level_performance:\n",
    "    print(f\"\\nüìà Level Performance:\")\n",
    "    for level_key, perf in level_performance.items():\n",
    "        level_num = level_key.replace('level_', '')\n",
    "        accuracy = perf.get('accuracy', 0)\n",
    "        total = perf.get('total_questions', 0)\n",
    "        print(f\"‚îú‚îÄ‚îÄ Level {level_num}: {accuracy:.1%} ({total} questions)\")\n",
    "\n",
    "# Budget analysis\n",
    "budget_analysis = analysis.get('budget_analysis', {})\n",
    "print(f\"\\nüí∞ Budget Analysis:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Budget Allocated: ${budget_analysis.get('budget_allocated', 0):.2f}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Budget Used: ${budget_analysis.get('budget_used', 0):.2f}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Budget Remaining: ${budget_analysis.get('budget_remaining', 0):.2f}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Cost per Question: ${budget_analysis.get('cost_per_question', 0):.3f}\")\n",
    "print(f\"‚îî‚îÄ‚îÄ Efficiency Score: {budget_analysis.get('efficiency_score', 0):.3f}\")\n",
    "\n",
    "# Recommendations\n",
    "recommendations = analysis.get('recommendations', [])\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "for rec in recommendations:\n",
    "    print(f\"‚îú‚îÄ‚îÄ {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness checklist\n",
    "print(f\"\\n‚úÖ PRODUCTION READINESS CHECKLIST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "readiness_criteria = {\n",
    "    \"Accuracy >= 45%\": achieved_accuracy >= 0.45,\n",
    "    \"Average response time < 60s\": summary.get('average_execution_time', 0) < 60,\n",
    "    \"Error rate < 10%\": summary.get('error_rate', 0) < 0.10,\n",
    "    \"Budget usage reasonable\": budget_analysis.get('budget_used', 0) < budget_analysis.get('budget_allocated', 0) * 0.8,\n",
    "    \"Level 1 accuracy > 60%\": level_performance.get('level_1', {}).get('accuracy', 0) > 0.60,\n",
    "    \"Level 2 accuracy > 30%\": level_performance.get('level_2', {}).get('accuracy', 0) > 0.30,\n",
    "    \"Fallback rate < 20%\": summary.get('fallback_usage_rate', 0) < 0.20\n",
    "}\n",
    "\n",
    "readiness_score = 0\n",
    "total_criteria = len(readiness_criteria)\n",
    "\n",
    "for criterion, passed in readiness_criteria.items():\n",
    "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    print(f\"{status} {criterion}\")\n",
    "    if passed:\n",
    "        readiness_score += 1\n",
    "\n",
    "print(f\"\\nüéØ READINESS SCORE: {readiness_score}/{total_criteria} ({readiness_score/total_criteria*100:.1f}%)\")\n",
    "\n",
    "if readiness_score >= total_criteria * 0.8:\n",
    "    print(\"üü¢ PRODUCTION READY - Agent meets most criteria\")\n",
    "elif readiness_score >= total_criteria * 0.6:\n",
    "    print(\"üü° NEEDS IMPROVEMENT - Agent meets basic criteria but has room for optimization\")\n",
    "else:\n",
    "    print(\"üî¥ NOT READY - Agent needs significant improvements before production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5. Error Analysis and Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors in detail for improvement opportunities\n",
    "print(f\"\\nüîç Detailed Error Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get detailed results from production test\n",
    "detailed_results = []\n",
    "for config_name, config_result in eval_results.items():\n",
    "    if \"detailed_results\" in config_result:\n",
    "        detailed_results.extend(config_result[\"detailed_results\"])\n",
    "\n",
    "if detailed_results:\n",
    "    df_production = pd.DataFrame(detailed_results)\n",
    "    \n",
    "    # Error categorization\n",
    "    error_questions = df_production[df_production['errors'].apply(lambda x: len(x) if isinstance(x, list) else 0) > 0]\n",
    "    incorrect_questions = df_production[df_production['is_correct'] == False]\n",
    "    \n",
    "    print(f\"Total questions analyzed: {len(df_production)}\")\n",
    "    print(f\"Questions with errors: {len(error_questions)}\")\n",
    "    print(f\"Incorrect answers: {len(incorrect_questions)}\")\n",
    "    print(f\"Questions using fallback: {len(df_production[df_production['fallback_used'] == True])}\")\n",
    "    \n",
    "    # Analyze patterns in incorrect answers\n",
    "    if len(incorrect_questions) > 0:\n",
    "        print(f\"\\n‚ùå Analysis of Incorrect Answers:\")\n",
    "        \n",
    "        # By level\n",
    "        incorrect_by_level = incorrect_questions.groupby('level').size()\n",
    "        print(f\"Incorrect answers by level:\")\n",
    "        for level, count in incorrect_by_level.items():\n",
    "            total_for_level = len(df_production[df_production['level'] == level])\n",
    "            error_rate = count / total_for_level if total_for_level > 0 else 0\n",
    "            print(f\"  ‚îú‚îÄ‚îÄ Level {level}: {count}/{total_for_level} ({error_rate:.1%} error rate)\")\n",
    "        \n",
    "        # By strategy\n",
    "        if 'strategy_used' in incorrect_questions.columns:\n",
    "            incorrect_by_strategy = incorrect_questions.groupby('strategy_used').size()\n",
    "            print(f\"\\nIncorrect answers by strategy:\")\n",
    "            for strategy, count in incorrect_by_strategy.items():\n",
    "                total_for_strategy = len(df_production[df_production['strategy_used'] == strategy])\n",
    "                error_rate = count / total_for_strategy if total_for_strategy > 0 else 0\n",
    "                print(f\"  ‚îú‚îÄ‚îÄ {strategy}: {count}/{total_for_strategy} ({error_rate:.1%} error rate)\")\n",
    "        \n",
    "        # Show worst performing questions for improvement\n",
    "        print(f\"\\nüéØ Questions for Improvement (Sample):\")\n",
    "        sample_incorrect = incorrect_questions.sample(min(5, len(incorrect_questions)))\n",
    "        for i, (_, row) in enumerate(sample_incorrect.iterrows()):\n",
    "            question = row['question'][:60] + \"...\" if len(row['question']) > 60 else row['question']\n",
    "            print(f\"  {i+1}. Q: {question}\")\n",
    "            print(f\"     Expected: {row['ground_truth']}\")\n",
    "            print(f\"     Got: {row['predicted_answer']}\")\n",
    "            print(f\"     Level: {row['level']}, Strategy: {row.get('strategy_used', 'Unknown')}\")\n",
    "            if isinstance(row['errors'], list) and row['errors']:\n",
    "                print(f\"     Error: {row['errors'][0]}\")\n",
    "            print()\n",
    "\n",
    "# Performance optimization suggestions\n",
    "print(f\"\\nüí° Performance Optimization Suggestions:\")\n",
    "\n",
    "optimization_suggestions = []\n",
    "\n",
    "# Accuracy improvements\n",
    "if achieved_accuracy < 0.50:\n",
    "    optimization_suggestions.append(\"Consider using higher-performance models for complex questions\")\n",
    "    optimization_suggestions.append(\"Improve RAG context by adding more diverse examples\")\n",
    "    optimization_suggestions.append(\"Enhance prompt engineering with better GAIA formatting instructions\")\n",
    "\n",
    "# Speed improvements\n",
    "avg_time = summary.get('average_execution_time', 0)\n",
    "if avg_time > 30:\n",
    "    optimization_suggestions.append(\"Optimize agent selection logic to reduce unnecessary steps\")\n",
    "    optimization_suggestions.append(\"Consider using faster models for simple questions\")\n",
    "\n",
    "# Cost optimization\n",
    "cost_per_question = budget_analysis.get('cost_per_question', 0)\n",
    "if cost_per_question > 0.10:\n",
    "    optimization_suggestions.append(\"Implement better model tiering to use cheaper models when appropriate\")\n",
    "    optimization_suggestions.append(\"Optimize retry logic to avoid unnecessary API calls\")\n",
    "\n",
    "# Error reduction\n",
    "error_rate = summary.get('error_rate', 0)\n",
    "if error_rate > 0.05:\n",
    "    optimization_suggestions.append(\"Improve error handling and recovery mechanisms\")\n",
    "    optimization_suggestions.append(\"Add input validation to prevent malformed requests\")\n",
    "\n",
    "# Fallback optimization\n",
    "fallback_rate = summary.get('fallback_usage_rate', 0)\n",
    "if fallback_rate > 0.15:\n",
    "    optimization_suggestions.append(\"Investigate and fix root causes of SmolagAgent failures\")\n",
    "    optimization_suggestions.append(\"Improve agent reliability and timeout handling\")\n",
    "\n",
    "if optimization_suggestions:\n",
    "    for i, suggestion in enumerate(optimization_suggestions, 1):\n",
    "        print(f\"  {i}. {suggestion}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ System is performing well across all metrics!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary and final recommendation\n",
    "print(f\"\\nüìã EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate overall grade\n",
    "grade_components = {\n",
    "    \"Accuracy\": (achieved_accuracy, 0.45, 40),  # (actual, target, weight)\n",
    "    \"Speed\": (1 - min(avg_time / 60, 1), 0.5, 20),  # Normalized speed score\n",
    "    \"Cost Efficiency\": (min(budget_analysis.get('efficiency_score', 0) / 10, 1), 0.5, 20),\n",
    "    \"Reliability\": (1 - summary.get('error_rate', 0), 0.95, 20)  # Error rate inverted\n",
    "}\n",
    "\n",
    "overall_score = 0\n",
    "total_weight = 0\n",
    "\n",
    "print(f\"Performance Component Analysis:\")\n",
    "for component, (actual, target, weight) in grade_components.items():\n",
    "    normalized_score = min(actual / target, 1.0) if target > 0 else 0\n",
    "    weighted_score = normalized_score * weight\n",
    "    overall_score += weighted_score\n",
    "    total_weight += weight\n",
    "    \n",
    "    status = \"‚úÖ\" if normalized_score >= 0.8 else \"‚ö†Ô∏è\" if normalized_score >= 0.6 else \"‚ùå\"\n",
    "    print(f\"  {status} {component}: {actual:.3f} (target: {target:.3f}) - Score: {normalized_score:.1%}\")\n",
    "\n",
    "final_grade = overall_score / total_weight\n",
    "letter_grade = \"A\" if final_grade >= 0.9 else \"B\" if final_grade >= 0.8 else \"C\" if final_grade >= 0.7 else \"D\" if final_grade >= 0.6 else \"F\"\n",
    "\n",
    "print(f\"\\nüéØ OVERALL ASSESSMENT:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Final Score: {final_grade:.1%}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Letter Grade: {letter_grade}\")\n",
    "print(f\"‚îî‚îÄ‚îÄ GAIA Target Met: {'‚úÖ YES' if target_met else '‚ùå NO'}\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\nüöÄ PRODUCTION DEPLOYMENT RECOMMENDATION:\")\n",
    "\n",
    "if final_grade >= 0.8 and target_met:\n",
    "    recommendation = \"üü¢ DEPLOY TO PRODUCTION\"\n",
    "    details = [\n",
    "        \"System meets GAIA accuracy targets\",\n",
    "        \"Performance metrics are acceptable\",\n",
    "        \"Ready for production workloads\",\n",
    "        f\"Recommended configuration: {best_config_name}\"\n",
    "    ]\n",
    "elif final_grade >= 0.7:\n",
    "    recommendation = \"üü° DEPLOY WITH MONITORING\"\n",
    "    details = [\n",
    "        \"System shows good performance but needs monitoring\",\n",
    "        \"Consider implementing additional safeguards\",\n",
    "        \"Monitor performance closely in production\",\n",
    "        \"Plan for iterative improvements\"\n",
    "    ]\n",
    "else:\n",
    "    recommendation = \"üî¥ FURTHER DEVELOPMENT NEEDED\"\n",
    "    details = [\n",
    "        \"System requires additional optimization\",\n",
    "        \"Focus on accuracy and reliability improvements\",\n",
    "        \"Consider alternative model configurations\",\n",
    "        \"Conduct additional testing before deployment\"\n",
    "    ]\n",
    "\n",
    "print(f\"\\n{recommendation}\")\n",
    "for detail in details:\n",
    "    print(f\"‚îú‚îÄ‚îÄ {detail}\")\n",
    "\n",
    "# Cost projection for production\n",
    "print(f\"\\nüí∞ Production Cost Projection:\")\n",
    "questions_per_day = 100  # Estimate\n",
    "daily_cost = questions_per_day * budget_analysis.get('cost_per_question', 0)\n",
    "monthly_cost = daily_cost * 30\n",
    "\n",
    "print(f\"‚îú‚îÄ‚îÄ Cost per question: ${budget_analysis.get('cost_per_question', 0):.3f}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ Estimated daily cost (100 questions): ${daily_cost:.2f}\")\n",
    "print(f\"‚îî‚îÄ‚îÄ Estimated monthly cost: ${monthly_cost:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Testing Session Complete!\")\n",
    "print(f\"Results saved in: {production_framework.results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive testing report\n",
    "report_data = {\n",
    "    \"session_metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"configurations_tested\": configs_to_compare,\n",
    "        \"best_configuration\": best_config_name,\n",
    "        \"total_questions_tested\": summary.get('total_questions_tested', 0)\n",
    "    },\n",
    "    \"performance_summary\": {\n",
    "        \"overall_accuracy\": achieved_accuracy,\n",
    "        \"gaia_target_met\": target_met,\n",
    "        \"average_response_time\": avg_time,\n",
    "        \"total_cost\": summary.get('total_cost', 0),\n",
    "        \"error_rate\": summary.get('error_rate', 0),\n",
    "        \"final_grade\": final_grade,\n",
    "        \"letter_grade\": letter_grade\n",
    "    },\n",
    "    \"detailed_analysis\": analysis,\n",
    "    \"readiness_assessment\": {\n",
    "        \"readiness_score\": f\"{readiness_score}/{total_criteria}\",\n",
    "        \"readiness_percentage\": readiness_score/total_criteria*100,\n",
    "        \"criteria_met\": readiness_criteria\n",
    "    },\n",
    "    \"recommendations\": {\n",
    "        \"deployment_recommendation\": recommendation,\n",
    "        \"optimization_suggestions\": optimization_suggestions,\n",
    "        \"production_configuration\": best_config_name\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "report_file = Path(f\"gaia_evaluation_report_{timestamp}.json\")\n",
    "\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(report_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìã Comprehensive report saved: {report_file}\")\n",
    "\n",
    "# Create summary table for easy reference\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"Configuration\": best_config_name,\n",
    "    \"Accuracy\": f\"{achieved_accuracy:.1%}\",\n",
    "    \"GAIA Target Met\": \"‚úÖ\" if target_met else \"‚ùå\",\n",
    "    \"Avg Response Time\": f\"{avg_time:.2f}s\",\n",
    "    \"Cost per Question\": f\"${budget_analysis.get('cost_per_question', 0):.3f}\",\n",
    "    \"Final Grade\": letter_grade,\n",
    "    \"Recommendation\": recommendation.split()[1]  # Extract status emoji\n",
    "}])\n",
    "\n",
    "print(f\"\\nüìä FINAL SUMMARY TABLE:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüéâ GAIA Evaluation Complete!\")\n",
    "print(f\"Your system is ready for the next phase of development.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
