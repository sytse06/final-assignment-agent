{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Validator\n",
    "## Implementation plan, testing framework, and deployment\n",
    "\n",
    "**Objective:** Complete implementation roadmap and evaluation system  \n",
    "**Target:** 45-55% GAIA accuracy within $10 budget\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for correct functioning of agent systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper imports for testing GAIAAgent\n",
    "from agent_logic import GAIAAgent\n",
    "from agent_interface import get_openrouter_config\n",
    "\n",
    "# Now create the agent\n",
    "agent = GAIAAgent(get_openrouter_config())\n",
    "\n",
    "print(\"üîç Context-aware tools analysis:\")\n",
    "for i, tool in enumerate(agent.context_aware_tools):\n",
    "   print(f\"  Tool {i}: {type(tool)}\")\n",
    "   print(f\"    Name: {getattr(tool, 'name', 'NO NAME')}\")\n",
    "   print(f\"    Has name attr: {hasattr(tool, 'name')}\")\n",
    "\n",
    "print(f\"\\nüîç Shared tools:\")\n",
    "for name, tool in agent.shared_tools.items():\n",
    "   print(f\"  {name}: {type(tool)} - {getattr(tool, 'name', 'NO NAME')}\")\n",
    "\n",
    "print(f\"\\nüîç Specialist tool check:\")\n",
    "for spec_name, specialist in agent.specialists.items():\n",
    "   tools = getattr(specialist, 'tools', [])\n",
    "   tool_names = [getattr(tool, 'name', str(tool)[:30]) for tool in tools]\n",
    "   has_attachment = any('get_attachment' in str(name).lower() for name in tool_names)\n",
    "   print(f\"  {spec_name}: {len(tools)} tools, has get_attachment: {has_attachment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.process_question(\"Analyze the data in this spreadsheet\", task_id=\"some_file_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüîç Specialist tool check:\")\n",
    "for spec_name, specialist in agent.specialists.items():\n",
    "    tools = getattr(specialist, 'tools', [])\n",
    "    tool_names = [getattr(tool, 'name', str(tool)[:30]) for tool in tools]\n",
    "    has_attachment = any('get_attachment' in str(name).lower() for name in tool_names)\n",
    "    print(f\"  {spec_name}: {len(tools)} tools, has get_attachment: {has_attachment}\")\n",
    "    print(f\"    Tool names: {tool_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_testing import run_quick_gaia_test\n",
    "\n",
    "print(\"üß™ Testing specialized agents...\")\n",
    "results = run_quick_gaia_test('openrouter')\n",
    "\n",
    "print(f\"\\nüìä Results with specialized agents:\")\n",
    "print(f\"Overall accuracy: {results.get('overall_accuracy', 0)}%\")\n",
    "print(f\"Successful executions: {results.get('successful_executions', 0)}\")\n",
    "\n",
    "# Check if we fixed the template response issue\n",
    "if 'detailed_results' in results:\n",
    "    for result in results['detailed_results']:\n",
    "        answer = result.get('agent_answer', '')\n",
    "        if '{' in answer or '[your answer]' in answer:\n",
    "            print(f\"‚ùå Still getting templates: {answer}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Real answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Status Quo Gaia-benchmark Agent Testing Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import the comprehensive testing framework\n",
    "try:\n",
    "    from agent_testing import (\n",
    "        run_gaia_test, \n",
    "        run_quick_gaia_test, \n",
    "        compare_agent_configs, \n",
    "        run_smart_routing_test,\n",
    "        analyze_failure_patterns,\n",
    "        validate_test_environment\n",
    "    )\n",
    "    TESTING_AVAILABLE = True\n",
    "    print(\"‚úÖ GAIA Testing Framework loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Could not import testing framework: {e}\")\n",
    "    TESTING_AVAILABLE = False\n",
    "\n",
    "class GAIAValidator:\n",
    "    \"\"\"\n",
    "    üöÄ GAIA Production Validator - Notebook Edition\n",
    "    \n",
    "    Clean interface for testing GAIA agents:\n",
    "    1. Run tests ‚Üí 2. Get insights ‚Üí 3. Make decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.last_result = None\n",
    "        self.history = []\n",
    "        \n",
    "        print(\"üéØ GAIA Production Validator (Notebook Edition)\")\n",
    "        \n",
    "        if not TESTING_AVAILABLE:\n",
    "            print(\"‚ùå Testing framework not available!\")\n",
    "            return\n",
    "        \n",
    "        # Quick environment check\n",
    "        try:\n",
    "            env_status = validate_test_environment()\n",
    "            if env_status.get(\"all_dependencies_ready\", False):\n",
    "                print(\"‚úÖ Environment ready!\")\n",
    "                if env_status.get(\"context_bridge_functional\", False):\n",
    "                    print(\"üåâ Hybrid State + Context Bridge verified!\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Some dependencies missing\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  Environment check failed - but may still work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare test methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick(self, config=\"groq\", questions=5):\n",
    "    \"\"\"üöÄ Quick test (5 questions) - perfect for development\"\"\"\n",
    "    print(f\"üöÄ Quick Test: {config} ({questions} questions)\")\n",
    "    \n",
    "    try:\n",
    "        result = run_quick_gaia_test(config, num_questions=questions)\n",
    "        self.last_result = result\n",
    "        self._add_history(\"quick\", config, result)\n",
    "        \n",
    "        if result and 'overall_performance' in result:\n",
    "            acc = result['overall_performance']['accuracy']\n",
    "            print(f\"‚úÖ Quick Test: {acc:.1%} accuracy\")\n",
    "            print(\"üí° Run validator.insights() for detailed analysis\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def full(self, config=\"groq\", questions=20):\n",
    "    \"\"\"üéØ Full test (20+ questions) - for production validation\"\"\"\n",
    "    print(f\"üéØ Full Test: {config} ({questions} questions)\")\n",
    "    \n",
    "    try:\n",
    "        result = run_gaia_test(config, max_questions=questions)\n",
    "        self.last_result = result\n",
    "        self._add_history(\"full\", config, result)\n",
    "        \n",
    "        if result and 'overall_performance' in result:\n",
    "            overall = result['overall_performance']\n",
    "            acc = overall['accuracy']\n",
    "            correct = overall['correct_answers']\n",
    "            total = overall['total_questions']\n",
    "            \n",
    "            print(f\"‚úÖ Full Test Complete:\")\n",
    "            print(f\"   Accuracy: {acc:.1%} ({correct}/{total})\")\n",
    "            print(f\"   GAIA Target: {'‚úÖ MET' if acc >= 0.45 else '‚ùå NOT MET'}\")\n",
    "            print(\"üí° Run validator.insights() for detailed analysis\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def compare(self, configs=[\"groq\", \"google\"], questions=10):\n",
    "    \"\"\"üîÑ Compare configurations\"\"\"\n",
    "    print(f\"üîÑ Comparing: {configs} ({questions} questions each)\")\n",
    "    \n",
    "    try:\n",
    "        result = compare_agent_configs(configs, questions)\n",
    "        self.last_result = result\n",
    "        self._add_history(\"compare\", configs, result)\n",
    "        \n",
    "        comparison = result.get('comparison_results', {})\n",
    "        if comparison:\n",
    "            print(f\"‚úÖ Comparison Complete:\")\n",
    "            # Quick ranking\n",
    "            ranked = sorted(\n",
    "                [(name, data.get('accuracy', 0)) for name, data in comparison.items() \n",
    "                    if 'accuracy' in data],\n",
    "                key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            for i, (config, acc) in enumerate(ranked, 1):\n",
    "                medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
    "                print(f\"   {medal} {config}: {acc:.1%}\")\n",
    "            print(\"üí° Run validator.insights() for detailed analysis\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Comparison failed: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def routing(self, config=\"performance\", questions=15):\n",
    "    \"\"\"üõ§Ô∏è Test smart routing effectiveness\"\"\"\n",
    "    print(f\"üõ§Ô∏è Routing Test: {config} ({questions} questions)\")\n",
    "    \n",
    "    try:\n",
    "        result = run_smart_routing_test(config)\n",
    "        self.last_result = result\n",
    "        self._add_history(\"routing\", config, result)\n",
    "        \n",
    "        if result and 'strategy_analysis' in result:\n",
    "            strategies = result['strategy_analysis']\n",
    "            print(f\"‚úÖ Routing Test Complete:\")\n",
    "            for strategy, stats in strategies.items():\n",
    "                acc = stats.get('accuracy', 0)\n",
    "                count = stats.get('total_questions', 0)\n",
    "                if count > 0:\n",
    "                    print(f\"   {strategy}: {acc:.1%} ({count}q)\")\n",
    "            print(\"üí° Run validator.insights() for detailed analysis\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Routing test failed: {e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Evaluation Insights Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def insights(self, result=None):\n",
    "        \"\"\"‚ú® Get actionable insights from test results\"\"\"\n",
    "        if result is None:\n",
    "            result = self.last_result\n",
    "        \n",
    "        if not result:\n",
    "            print(\"‚ùå No test results! Run a test first.\")\n",
    "            return\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"‚ùå Cannot analyze failed test: {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n‚ú® ACTIONABLE INSIGHTS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Detect test type and analyze accordingly\n",
    "        if 'comparison_results' in result:\n",
    "            self._analyze_comparison(result)\n",
    "        elif self._is_routing_test(result):\n",
    "            self._analyze_routing(result)\n",
    "        else:\n",
    "            self._analyze_performance(result)\n",
    "        \n",
    "        # Generate recommendations\n",
    "        self._generate_recommendations(result)\n",
    "    \n",
    "    def _analyze_performance(self, result):\n",
    "        \"\"\"Analyze single agent performance\"\"\"\n",
    "        overall = result.get('overall_performance', {})\n",
    "        accuracy = overall.get('accuracy', 0)\n",
    "        total = overall.get('total_questions', 0)\n",
    "        correct = overall.get('correct_answers', 0)\n",
    "        \n",
    "        print(f\"üìä PERFORMANCE ANALYSIS\")\n",
    "        print(f\"   Overall: {accuracy:.1%} ({correct}/{total})\")\n",
    "        \n",
    "        # GAIA benchmark assessment\n",
    "        if accuracy >= 0.60:\n",
    "            print(f\"   üèÜ EXCELLENT - Competitive performance!\")\n",
    "        elif accuracy >= 0.45:\n",
    "            print(f\"   ‚úÖ GOOD - Above GAIA threshold\")\n",
    "        elif accuracy >= 0.30:\n",
    "            print(f\"   ‚ö†Ô∏è  FAIR - Below GAIA threshold\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå POOR - Needs significant improvement\")\n",
    "        \n",
    "        # Level breakdown\n",
    "        level_perf = result.get('level_performance', {})\n",
    "        if level_perf:\n",
    "            print(f\"\\nüìà LEVEL BREAKDOWN:\")\n",
    "            for level in sorted(level_perf.keys()):\n",
    "                perf = level_perf[level]\n",
    "                acc = perf['accuracy']\n",
    "                count = perf['total']\n",
    "                print(f\"   Level {level}: {acc:.1%} ({count} questions)\")\n",
    "                \n",
    "                # Level-specific insights\n",
    "                if level == '1' and acc < 0.7:\n",
    "                    print(f\"      ‚ö†Ô∏è  Level 1 should be >70%\")\n",
    "                elif level == '3' and acc > 0.3:\n",
    "                    print(f\"      üéØ Strong Level 3 performance!\")\n",
    "        \n",
    "        # Strategy analysis\n",
    "        strategy_perf = result.get('strategy_analysis', {})\n",
    "        if strategy_perf:\n",
    "            print(f\"\\nüéØ STRATEGY EFFECTIVENESS:\")\n",
    "            for strategy, stats in strategy_perf.items():\n",
    "                acc = stats.get('accuracy', 0)\n",
    "                count = stats.get('total_questions', 0)\n",
    "                if count > 0:\n",
    "                    print(f\"   {strategy}: {acc:.1%} ({count}q)\")\n",
    "        \n",
    "        # Hybrid metrics\n",
    "        hybrid = result.get('hybrid_state_metrics', {})\n",
    "        if hybrid:\n",
    "            context_usage = hybrid.get('context_bridge_usage', {})\n",
    "            if context_usage:\n",
    "                usage = context_usage.get('usage_percentage', 0)\n",
    "                print(f\"\\nüåâ HYBRID STATE:\")\n",
    "                print(f\"   Context Bridge: {usage:.1%} usage\")\n",
    "                \n",
    "                avg_time = hybrid.get('average_execution_time', 0)\n",
    "                if avg_time > 0:\n",
    "                    print(f\"   Avg Time: {avg_time:.2f}s per question\")\n",
    "    \n",
    "    def _analyze_comparison(self, result):\n",
    "        \"\"\"Analyze configuration comparison\"\"\"\n",
    "        comparison = result.get('comparison_results', {})\n",
    "        \n",
    "        print(f\"üîÑ CONFIGURATION COMPARISON\")\n",
    "        \n",
    "        # Sort by accuracy\n",
    "        ranked = sorted(\n",
    "            [(name, data) for name, data in comparison.items() if 'accuracy' in data],\n",
    "            key=lambda x: x[1]['accuracy'], reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"   üìä RANKING:\")\n",
    "        for i, (config, data) in enumerate(ranked, 1):\n",
    "            acc = data['accuracy']\n",
    "            total = data.get('total_questions', 0)\n",
    "            correct = data.get('correct_answers', 0)\n",
    "            \n",
    "            medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
    "            print(f\"   {medal} {config}: {acc:.1%} ({correct}/{total})\")\n",
    "        \n",
    "        # Winner analysis\n",
    "        if ranked:\n",
    "            winner, winner_data = ranked[0]\n",
    "            print(f\"\\nüèÜ WINNER: {winner}\")\n",
    "            print(f\"   Accuracy: {winner_data['accuracy']:.1%}\")\n",
    "            \n",
    "            if len(ranked) > 1:\n",
    "                gap = winner_data['accuracy'] - ranked[1][1]['accuracy']\n",
    "                print(f\"   Lead: {gap:.1%} ahead of 2nd place\")\n",
    "    \n",
    "    def _analyze_routing(self, result):\n",
    "        \"\"\"Analyze smart routing\"\"\"\n",
    "        strategies = result.get('strategy_analysis', {})\n",
    "        \n",
    "        print(f\"üõ§Ô∏è SMART ROUTING ANALYSIS\")\n",
    "        \n",
    "        # Group strategies\n",
    "        one_shot = {k: v for k, v in strategies.items() if 'one_shot' in k.lower()}\n",
    "        complex_strat = {k: v for k, v in strategies.items() if 'manager' in k.lower() or 'agent_' in k.lower()}\n",
    "        \n",
    "        if one_shot:\n",
    "            print(f\"   ‚ö° SIMPLE STRATEGIES:\")\n",
    "            for strategy, stats in one_shot.items():\n",
    "                acc = stats.get('accuracy', 0)\n",
    "                count = stats.get('total_questions', 0)\n",
    "                print(f\"      {strategy}: {acc:.1%} ({count}q)\")\n",
    "        \n",
    "        if complex_strat:\n",
    "            print(f\"   üß† COMPLEX STRATEGIES:\")\n",
    "            for strategy, stats in complex_strat.items():\n",
    "                acc = stats.get('accuracy', 0)\n",
    "                count = stats.get('total_questions', 0)\n",
    "                print(f\"      {strategy}: {acc:.1%} ({count}q)\")\n",
    "        \n",
    "        # Routing effectiveness\n",
    "        if one_shot and complex_strat:\n",
    "            simple_total = sum(s.get('total_questions', 0) for s in one_shot.values())\n",
    "            complex_total = sum(s.get('total_questions', 0) for s in complex_strat.values())\n",
    "            \n",
    "            print(f\"\\nüìä ROUTING BALANCE:\")\n",
    "            print(f\"   Simple: {simple_total} questions\")\n",
    "            print(f\"   Complex: {complex_total} questions\")\n",
    "            \n",
    "            if simple_total > complex_total:\n",
    "                print(f\"   ‚úÖ Good - more simple questions handled efficiently\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Many complex questions - check routing logic\")\n",
    "    \n",
    "    def _generate_recommendations(self, result):\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        print(f\"\\nüí° RECOMMENDATIONS\")\n",
    "        \n",
    "        # Try to get detailed failure analysis\n",
    "        try:\n",
    "            failure_analysis = analyze_failure_patterns(result)\n",
    "            recommendations = failure_analysis.get('recommendations', [])\n",
    "            \n",
    "            if recommendations:\n",
    "                for i, rec in enumerate(recommendations, 1):\n",
    "                    print(f\"   {i}. {rec}\")\n",
    "            else:\n",
    "                print(\"   üéâ No specific issues - performance looks good!\")\n",
    "                \n",
    "        except:\n",
    "            # Fallback simple recommendations\n",
    "            overall = result.get('overall_performance', {})\n",
    "            accuracy = overall.get('accuracy', 0)\n",
    "            \n",
    "            if accuracy < 0.45:\n",
    "                print(\"   1. Focus on reaching GAIA 45% threshold\")\n",
    "                print(\"   2. Check Level 1 performance first\")\n",
    "                print(\"   3. Analyze specific failure cases\")\n",
    "            elif accuracy < 0.60:\n",
    "                print(\"   1. Good performance - optimize consistency\")\n",
    "                print(\"   2. Focus on Level 2 improvements\")\n",
    "                print(\"   3. Consider smart routing tweaks\")\n",
    "            else:\n",
    "                print(\"   1. Excellent! Document this configuration\")\n",
    "                print(\"   2. Test with larger question sets\")\n",
    "                print(\"   3. Monitor performance over time\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # UTILITY METHODS üõ†Ô∏è\n",
    "    # ========================================================================\n",
    "    \n",
    "    def _is_routing_test(self, result):\n",
    "        \"\"\"Check if this is a routing test\"\"\"\n",
    "        strategies = result.get('strategy_analysis', {})\n",
    "        return len(strategies) > 2  # Routing tests have multiple strategies\n",
    "    \n",
    "    def _add_history(self, test_type, config, result):\n",
    "        \"\"\"Add to test history\"\"\"\n",
    "        self.history.append({\n",
    "            \"time\": datetime.now().strftime(\"%H:%M:%S\"),\n",
    "            \"type\": test_type,\n",
    "            \"config\": str(config),\n",
    "            \"accuracy\": result.get('overall_performance', {}).get('accuracy', 0) if 'overall_performance' in result else 0,\n",
    "            \"success\": \"error\" not in result\n",
    "        })\n",
    "    \n",
    "    def status(self):\n",
    "        \"\"\"Show validator status\"\"\"\n",
    "        print(f\"üìä VALIDATOR STATUS\")\n",
    "        print(f\"   Tests run: {len(self.history)}\")\n",
    "        print(f\"   Framework: {'‚úÖ Available' if TESTING_AVAILABLE else '‚ùå Not available'}\")\n",
    "        \n",
    "        if self.history:\n",
    "            print(f\"   Recent tests:\")\n",
    "            for test in self.history[-3:]:  # Last 3 tests\n",
    "                status = \"‚úÖ\" if test['success'] else \"‚ùå\"\n",
    "                print(f\"      {test['time']} {status} {test['type']}({test['config']}) - {test['accuracy']:.1%}\")\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"Show test history as DataFrame\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No test history yet\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.history)\n",
    "        return df\n",
    "    \n",
    "    def save(self, filename=None):\n",
    "        \"\"\"Save last result to file\"\"\"\n",
    "        if not self.last_result:\n",
    "            print(\"‚ùå No result to save\")\n",
    "            return\n",
    "        \n",
    "        if filename is None:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"gaia_result_{timestamp}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(self.last_result, f, indent=2)\n",
    "            print(f\"üíæ Saved: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Save failed: {e}\")\n",
    "\n",
    "# Create the validator instance\n",
    "print(\"\\nüöÄ Creating GAIA Validator...\")\n",
    "validator = GAIAValidator()\n",
    "\n",
    "print(\"\\nüí° READY TO USE!\")\n",
    "print(\"Quick commands:\")\n",
    "print(\"  validator.quick('groq')     # Quick test\")\n",
    "print(\"  validator.full('groq', 20)  # Full test\") \n",
    "print(\"  validator.compare(['groq', 'google'])  # Compare configs\")\n",
    "print(\"  validator.routing('performance')  # Test routing\")\n",
    "print(\"  validator.insights()        # Analyze results\")\n",
    "print(\"  validator.status()          # Show status\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Create Small Test Batch (Data Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_logic import GAIAAgent, GAIAConfig\n",
    "\n",
    "# Create config object for Openrouter\n",
    "config = GAIAConfig(\n",
    "    model_provider=\"openrouter\",\n",
    "    model_name=\"qwen/qwen3-30b-a3b\"\n",
    ")\n",
    "agent = GAIAAgent(config)\n",
    "result = agent.process_question(\"Is Musk still ceo of Tesla?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache issues nuclear.way:\n",
    "import os\n",
    "os._exit(0)  # Completely restart Python process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Create Small Test Batch using Pure Data Layer\n",
    "print(\"üì¶ Creating Small Test Batch (5 questions)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from gaia_dataset_utils import GAIADatasetManager, quick_dataset_check\n",
    "\n",
    "# Step 1: Validate dataset\n",
    "print(\"üîç Step 1: Dataset Validation\")\n",
    "dataset_ready = quick_dataset_check(\"./tests/gaia_data\")\n",
    "\n",
    "if dataset_ready:\n",
    "    # Step 2: Create dataset manager\n",
    "    print(\"\\nüìä Step 2: Loading Dataset Manager\")\n",
    "    manager = GAIADatasetManager(\"./tests/gaia_data\")\n",
    "    \n",
    "    if manager.metadata:\n",
    "        print(f\"‚úÖ Dataset loaded: {len(manager.metadata)} total questions\")\n",
    "        print(f\"üìÅ Questions with files: {len(manager.file_questions)}\")\n",
    "        \n",
    "        # Step 3: Create small test batch\n",
    "        print(\"\\nüì¶ Step 3: Creating Small Test Batch\")\n",
    "        test_batch = manager.create_test_batch(5, \"small_sample\")\n",
    "        \n",
    "        if test_batch:\n",
    "            print(f\"‚úÖ Created test batch with {len(test_batch)} questions\")\n",
    "            \n",
    "            # Show batch composition\n",
    "            print(f\"\\nüìã Batch Composition:\")\n",
    "            levels = {}\n",
    "            file_count = 0\n",
    "            file_types = set()\n",
    "            \n",
    "            for i, question in enumerate(test_batch, 1):\n",
    "                level = question.get('Level', 'Unknown')\n",
    "                levels[level] = levels.get(level, 0) + 1\n",
    "                \n",
    "                has_file = question['task_id'] in manager.file_questions\n",
    "                if has_file:\n",
    "                    file_count += 1\n",
    "                    file_name = manager.file_questions[question['task_id']].get('file_name', '')\n",
    "                    if file_name:\n",
    "                        ext = file_name.split('.')[-1].lower()\n",
    "                        file_types.add(ext)\n",
    "                \n",
    "                # Show question preview\n",
    "                question_text = question.get('Question', '')\n",
    "                preview = question_text[:60] + \"...\" if len(question_text) > 60 else question_text\n",
    "                file_info = f\" (File: {file_name})\" if has_file else \"\"\n",
    "                \n",
    "                print(f\"  {i}. Level {level}: {preview}{file_info}\")\n",
    "            \n",
    "            print(f\"\\nüìä Summary:\")\n",
    "            print(f\"  Level distribution: {dict(levels)}\")\n",
    "            print(f\"  Questions with files: {file_count}/{len(test_batch)}\")\n",
    "            if file_types:\n",
    "                print(f\"  File types: {', '.join(sorted(file_types))}\")\n",
    "            \n",
    "            # Verify blind testing - show what agent will see vs hidden\n",
    "            print(f\"\\nüîí Blind Testing Verification:\")\n",
    "            sample_question = test_batch[0]\n",
    "            \n",
    "            print(f\"  ü§ñ Agent will see:\")\n",
    "            visible_fields = ['task_id', 'Question', 'Level', 'file_name', 'file_path']\n",
    "            for field in visible_fields:\n",
    "                if field in sample_question:\n",
    "                    value = sample_question[field]\n",
    "                    if isinstance(value, str) and len(value) > 50:\n",
    "                        value = value[:50] + \"...\"\n",
    "                    print(f\"    {field}: {value}\")\n",
    "            \n",
    "            print(f\"  üîí Agent will NOT see:\")\n",
    "            # Check what's hidden by looking at full dataset\n",
    "            full_question = manager.get_question_by_id(sample_question['task_id'])\n",
    "            hidden_info = ['Final answer', 'Annotator Metadata']\n",
    "            for field in hidden_info:\n",
    "                if field in full_question:\n",
    "                    print(f\"    {field}: HIDDEN\")\n",
    "            \n",
    "            print(f\"\\n‚úÖ Test batch ready for execution!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ùå Failed to create test batch\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load dataset\")\n",
    "else:\n",
    "    print(\"‚ùå Dataset not ready - check your setup\")\n",
    "    test_batch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Execute Test Batch with provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Execute Test Batch using Pure Testing Layer\n",
    "print(\"ü§ñ Executing Test Batch with Openrouter\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import testing layer components\n",
    "from agent_testing import GAIATestExecutor, GAIATestEvaluator, get_agent_config\n",
    "import time\n",
    "\n",
    "# Check if we have test batch from previous cell\n",
    "if 'test_batch' not in locals() or not test_batch:\n",
    "    print(\"‚ùå No test batch available. Run Cell 1 first.\")\n",
    "else:\n",
    "    print(f\"üìã Test batch loaded: {len(test_batch)} questions\")\n",
    "    \n",
    "    # Step 1: Configure Groq agent (supports tools)\n",
    "    print(f\"\\nüîß Step 1: Configure Openrouter Agent\")\n",
    "    \n",
    "    try:\n",
    "        # Get Groq configuration (supports tool calling)\n",
    "        ollama_config = get_agent_config(\"openrouter\")\n",
    "        print(f\"‚úÖ Openrouter config loaded\")\n",
    "        print(f\"   Model: google/gemini-2.5-flash\")\n",
    "        print(f\"   Provider: Ollama\")\n",
    "        print(f\"   Tool Support: ‚úÖ YES\")\n",
    "        \n",
    "        # Create test executor\n",
    "        executor = GAIATestExecutor(\"openrouter\")\n",
    "        print(f\"‚úÖ Test executor created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create agent: {e}\")\n",
    "        executor = None\n",
    "\n",
    "    if executor:\n",
    "        # Step 2: Execute test batch (blind)\n",
    "        print(f\"\\nüöÄ Step 2: Execute Test Batch (Blind)\")\n",
    "        print(f\"‚ö†Ô∏è  This will possibly use provider API credits\")\n",
    "        \n",
    "        proceed = input(\"Proceed with execution? (y/n): \")\n",
    "        \n",
    "        if proceed.lower() in ['y', 'yes']:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Execute the batch\n",
    "                execution_results = executor.execute_test_batch(test_batch)\n",
    "                \n",
    "                execution_time = time.time() - start_time\n",
    "                \n",
    "                if execution_results:\n",
    "                    print(f\"\\n‚úÖ Execution completed!\")\n",
    "                    print(f\"‚è±Ô∏è Total time: {execution_time:.1f}s\")\n",
    "                    \n",
    "                    # Analyze execution results\n",
    "                    successful = len([r for r in execution_results if r.get('execution_successful', False)])\n",
    "                    avg_time = sum(r.get('execution_time', 0) for r in execution_results) / len(execution_results)\n",
    "                    \n",
    "                    print(f\"üìä Execution Summary:\")\n",
    "                    print(f\"  Total questions: {len(execution_results)}\")\n",
    "                    print(f\"  Successful question executions: {successful}/{len(execution_results)} ({successful/len(execution_results):.1%})\")\n",
    "                    print(f\"  Average time per question: {avg_time:.2f}s\")\n",
    "                    \n",
    "                    # Show sample results (without revealing correctness yet)\n",
    "                    print(f\"\\nü§ñ Sample Agent Responses:\")\n",
    "                    for i, result in enumerate(execution_results[:3], 1):\n",
    "                        answer = result.get('agent_answer', 'No answer')\n",
    "                        strategy = result.get('strategy_used', 'unknown')\n",
    "                        exec_time = result.get('execution_time', 0)\n",
    "                        \n",
    "                        print(f\"  {i}. Answer: '{answer}' (Strategy: {strategy}, Time: {exec_time:.1f}s)\")\n",
    "                    \n",
    "                    if len(execution_results) > 3:\n",
    "                        print(f\"  ... and {len(execution_results) - 3} more\")\n",
    "                    \n",
    "                    print(f\"\\nüîí Note: Correctness not yet determined - evaluation needed!\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"‚ùå Execution failed - no results returned\")\n",
    "                    execution_results = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Execution error: {e}\")\n",
    "                execution_results = None\n",
    "        else:\n",
    "            print(\"‚è≠Ô∏è Execution skipped\")\n",
    "            execution_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Evaluate Results (Testing Layer + Data Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluate Results against Ground Truth\n",
    "print(\"üéØ Evaluating Results against Ground Truth\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have execution results\n",
    "if 'execution_results' not in locals() or not execution_results:\n",
    "    print(\"‚ùå No execution results available. Run Cell 2 first.\")\n",
    "elif 'manager' not in locals() or not manager:\n",
    "    print(\"‚ùå No dataset manager available. Run Cell 1 first.\")\n",
    "else:\n",
    "    print(f\"üìã Execution results loaded: {len(execution_results)} results\")\n",
    "    \n",
    "    # Step 1: Create evaluator with dataset manager for ground truth access\n",
    "    print(f\"\\nüîç Step 1: Initialize Evaluator\")\n",
    "    \n",
    "    try:\n",
    "        evaluator = GAIATestEvaluator(manager)\n",
    "        print(f\"‚úÖ Evaluator created with ground truth access\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create evaluator: {e}\")\n",
    "        evaluator = None\n",
    "\n",
    "    if evaluator:\n",
    "        # Step 2: Evaluate execution results\n",
    "        print(f\"\\nüéØ Step 2: Evaluate Against Ground Truth\")\n",
    "        \n",
    "        try:\n",
    "            evaluation_results = evaluator.evaluate_execution_results(execution_results)\n",
    "            \n",
    "            if evaluation_results and 'evaluation_metadata' in evaluation_results:\n",
    "                metadata = evaluation_results['evaluation_metadata']\n",
    "                analysis = evaluation_results.get('analysis', {})\n",
    "                \n",
    "                # Overall performance\n",
    "                print(f\"\\nüìä Overall Performance:\")\n",
    "                print(f\"  Total questions: {metadata.get('total_questions', 0)}\")\n",
    "                print(f\"  Correct answers: {metadata.get('correct_answers', 0)}\")\n",
    "                print(f\"  Overall accuracy: {metadata.get('overall_accuracy', 0):.1%}\")\n",
    "                \n",
    "                gaia_target_met = metadata.get('overall_accuracy', 0) >= 0.45\n",
    "                print(f\"  GAIA target (45%): {'‚úÖ MET' if gaia_target_met else '‚ùå NOT MET'}\")\n",
    "                \n",
    "                # Performance by level\n",
    "                level_perf = analysis.get('level_performance', {})\n",
    "                if level_perf:\n",
    "                    print(f\"\\nüìà Performance by Level:\")\n",
    "                    for level_key, stats in level_perf.items():\n",
    "                        level_num = level_key.replace('level_', '')\n",
    "                        accuracy = stats.get('accuracy', 0)\n",
    "                        total = stats.get('total_questions', 0)\n",
    "                        correct = stats.get('correct_answers', 0)\n",
    "                        \n",
    "                        print(f\"  Level {level_num}: {accuracy:.1%} ({correct}/{total} correct)\")\n",
    "                \n",
    "                # Strategy performance\n",
    "                strategy_perf = analysis.get('strategy_performance', {})\n",
    "                if strategy_perf:\n",
    "                    print(f\"\\nüéØ Strategy Performance:\")\n",
    "                    for strategy, stats in strategy_perf.items():\n",
    "                        accuracy = stats.get('accuracy', 0)\n",
    "                        total = stats.get('total_questions', 0)\n",
    "                        avg_time = stats.get('avg_execution_time', 0)\n",
    "                        \n",
    "                        strategy_name = strategy.replace('_', ' ').title()\n",
    "                        print(f\"  {strategy_name}: {accuracy:.1%} ({total} questions, {avg_time:.1f}s avg)\")\n",
    "                \n",
    "                # File attachment performance\n",
    "                file_perf = analysis.get('file_attachment_performance', {})\n",
    "                if file_perf:\n",
    "                    print(f\"\\nüìé File Attachment Performance:\")\n",
    "                    for category, stats in file_perf.items():\n",
    "                        accuracy = stats.get('accuracy', 0)\n",
    "                        total = stats.get('total_questions', 0)\n",
    "                        category_name = category.replace('_', ' ').title()\n",
    "                        print(f\"  {category_name}: {accuracy:.1%} ({total} questions)\")\n",
    "                \n",
    "                # Show detailed results for each question\n",
    "                print(f\"\\nüìù Detailed Question Results:\")\n",
    "                detailed_results = evaluation_results.get('detailed_results', [])\n",
    "                \n",
    "                for i, result in enumerate(detailed_results, 1):\n",
    "                    is_correct = result.get('is_correct', False)\n",
    "                    agent_answer = result.get('agent_answer', '')\n",
    "                    expected_answer = result.get('expected_answer', '')\n",
    "                    level = result.get('level', 'Unknown')\n",
    "                    strategy = result.get('strategy_used', 'unknown')\n",
    "                    exec_time = result.get('execution_time', 0)\n",
    "                    \n",
    "                    status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå INCORRECT\"\n",
    "                    \n",
    "                    print(f\"  {i}. {status} (Level {level}, {strategy}, {exec_time:.1f}s)\")\n",
    "                    print(f\"     Agent: '{agent_answer}'\")\n",
    "                    print(f\"     Expected: '{expected_answer}'\")\n",
    "                    \n",
    "                    if result.get('has_file'):\n",
    "                        file_name = result.get('file_name', 'unknown')\n",
    "                        print(f\"     File: {file_name}\")\n",
    "                    print()\n",
    "                \n",
    "                # Error analysis\n",
    "                error_analysis = analysis.get('error_analysis', {})\n",
    "                if error_analysis:\n",
    "                    exec_errors = error_analysis.get('execution_errors', 0)\n",
    "                    success_rate = error_analysis.get('execution_success_rate', 0)\n",
    "                    \n",
    "                    print(f\"üîß Execution Analysis:\")\n",
    "                    print(f\"  Success rate: {success_rate:.1%}\")\n",
    "                    print(f\"  Execution errors: {exec_errors}\")\n",
    "                    \n",
    "                    if exec_errors > 0:\n",
    "                        sample_errors = error_analysis.get('sample_errors', [])\n",
    "                        if sample_errors:\n",
    "                            print(f\"  Sample errors:\")\n",
    "                            for error in sample_errors[:2]:\n",
    "                                print(f\"    - {error}\")\n",
    "                \n",
    "                print(f\"\\n‚úÖ Evaluation completed!\")\n",
    "                print(f\"üíæ Results saved to logs/gaia_evaluation_*.json\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"‚ùå Evaluation failed - no results returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Evaluation error: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Test completed!\")\n",
    "print(f\"üìä You've tested the clean architecture with:\")\n",
    "print(f\"  ‚úÖ Pure data layer (gaia_dataset_utils)\")\n",
    "print(f\"  ‚úÖ Pure testing layer (agent_testing)\")\n",
    "print(f\"  ‚úÖ OpenRouter free model\")\n",
    "print(f\"  ‚úÖ Small batch strategy\")\n",
    "print(f\"  ‚úÖ Blind testing methodology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Performance Baseline Establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish performance baselines across different question types\n",
    "\n",
    "print(\"üìä Establishing Performance Baselines\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define baseline test configurations\n",
    "baseline_configs = {\n",
    "    'level_1_only': {\n",
    "        'name': 'Level 1 Questions Only',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'level_1_2_mix': {\n",
    "        'name': 'Level 1 & 2 Mixed',\n",
    "        'params': {\n",
    "            'max_questions': 20,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'all_levels': {\n",
    "        'name': 'All Difficulty Levels',\n",
    "        'params': {\n",
    "            'max_questions': 25,\n",
    "            'target_levels': [1, 2, 3],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'text_only': {\n",
    "        'name': 'Text-Only Questions',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': False,\n",
    "            'include_images': False\n",
    "        }\n",
    "    },\n",
    "    'with_files': {\n",
    "        'name': 'File-Based Questions',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run baseline tests with groq configuration\n",
    "baseline_results = {}\n",
    "baseline_agent = \"ollama\"  # Use most reliable configuration\n",
    "\n",
    "for config_key, config_data in baseline_configs.items():\n",
    "    config_name = config_data['name']\n",
    "    params = config_data['params']\n",
    "    \n",
    "    print(f\"\\nüß™ Running: {config_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = run_gaia_test(\n",
    "            agent_config_name=baseline_agent,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if result and 'evaluation_metadata' in result:\n",
    "            metadata = result['evaluation_metadata']\n",
    "            analysis = result.get('analysis', {})\n",
    "            \n",
    "            # Extract key metrics\n",
    "            baseline_results[config_key] = {\n",
    "                'name': config_name,\n",
    "                'total_questions': metadata.get('total_questions', 0),\n",
    "                'correct_answers': metadata.get('correct_answers', 0),\n",
    "                'accuracy': metadata.get('overall_accuracy', 0),\n",
    "                'analysis': analysis,\n",
    "                'full_result': result\n",
    "            }\n",
    "            \n",
    "            # Print immediate summary\n",
    "            accuracy = metadata.get('overall_accuracy', 0)\n",
    "            total = metadata.get('total_questions', 0)\n",
    "            correct = metadata.get('correct_answers', 0)\n",
    "            \n",
    "            print(f\"‚úÖ Completed: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "            \n",
    "            # Check GAIA target\n",
    "            if accuracy >= 0.45:\n",
    "                print(\"üèÜ GAIA target achieved (45%+)\")\n",
    "            elif accuracy >= 0.35:\n",
    "                print(\"‚ö†Ô∏è Approaching GAIA target\")\n",
    "            else:\n",
    "                print(\"‚ùå Below GAIA target\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå Test failed for {config_name}\")\n",
    "            baseline_results[config_key] = {'name': config_name, 'error': 'Test failed'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {config_name}: {e}\")\n",
    "        baseline_results[config_key] = {'name': config_name, 'error': str(e)}\n",
    "\n",
    "# Baseline summary table\n",
    "print(f\"\\nüìä BASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_df_data = []\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'error' not in result:\n",
    "        baseline_df_data.append({\n",
    "            'Test Configuration': result['name'],\n",
    "            'Questions': result['total_questions'],\n",
    "            'Correct': result['correct_answers'],\n",
    "            'Accuracy': f\"{result['accuracy']:.1%}\",\n",
    "            'GAIA Target': \"‚úÖ\" if result['accuracy'] >= 0.45 else \"‚ùå\"\n",
    "        })\n",
    "    else:\n",
    "        baseline_df_data.append({\n",
    "            'Test Configuration': result['name'],\n",
    "            'Questions': 0,\n",
    "            'Correct': 0,\n",
    "            'Accuracy': \"ERROR\",\n",
    "            'GAIA Target': \"‚ùå\"\n",
    "        })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_df_data)\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "# Identify best and worst performing configurations\n",
    "if baseline_df_data:\n",
    "    successful_results = [r for r in baseline_results.values() if 'error' not in r and r.get('accuracy', 0) > 0]\n",
    "    if successful_results:\n",
    "        best_config = max(successful_results, key=lambda x: x['accuracy'])\n",
    "        worst_config = min(successful_results, key=lambda x: x['accuracy'])\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Performance: {best_config['name']} ({best_config['accuracy']:.1%})\")\n",
    "        print(f\"‚ö†Ô∏è Needs Improvement: {worst_config['name']} ({worst_config['accuracy']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Routing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into routing effectiveness and optimization\n",
    "\n",
    "print(\"üîÄ Smart Routing Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test routing with performance-optimized configuration\n",
    "routing_test_configs = [\"groq\", \"performance\"]\n",
    "\n",
    "routing_analysis_results = {}\n",
    "\n",
    "for config in routing_test_configs:\n",
    "    print(f\"\\nüß™ Testing routing with {config} configuration\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        routing_result = run_smart_routing_test(config)\n",
    "        \n",
    "        if routing_result and 'analysis' in routing_result:\n",
    "            analysis = routing_result['analysis']\n",
    "            routing_analysis = analysis.get('routing_analysis', {})\n",
    "            strategy_performance = analysis.get('strategy_performance', {})\n",
    "            level_performance = analysis.get('level_performance', {})\n",
    "            \n",
    "            routing_analysis_results[config] = {\n",
    "                'routing_stats': routing_analysis,\n",
    "                'strategy_stats': strategy_performance,\n",
    "                'level_stats': level_performance,\n",
    "                'overall_accuracy': routing_result['evaluation_metadata']['overall_accuracy']\n",
    "            }\n",
    "            \n",
    "            # Print routing effectiveness\n",
    "            one_shot_count = routing_analysis.get('one_shot_questions', 0)\n",
    "            manager_count = routing_analysis.get('manager_questions', 0)\n",
    "            routing_accuracy = routing_analysis.get('routing_accuracy', 0)\n",
    "            \n",
    "            print(f\"üìä Question Distribution:\")\n",
    "            print(f\"‚îú‚îÄ‚îÄ One-shot LLM: {one_shot_count} questions\")\n",
    "            print(f\"‚îú‚îÄ‚îÄ Manager Coordination: {manager_count} questions\")\n",
    "            print(f\"‚îî‚îÄ‚îÄ Routing Accuracy: {routing_accuracy:.1%}\")\n",
    "            \n",
    "            print(f\"\\nüìà Strategy Performance:\")\n",
    "            for strategy, stats in strategy_performance.items():\n",
    "                accuracy = stats.get('accuracy', 0)\n",
    "                avg_time = stats.get('avg_execution_time', 0)\n",
    "                total = stats.get('total_questions', 0)\n",
    "                print(f\"‚îú‚îÄ‚îÄ {strategy.replace('_', ' ').title()}: {accuracy:.1%} accuracy, {avg_time:.1f}s avg ({total} questions)\")\n",
    "            \n",
    "            # Routing insights\n",
    "            print(f\"\\nüí° Routing Insights:\")\n",
    "            if routing_accuracy >= 0.8:\n",
    "                print(\"‚úÖ Routing decisions are highly accurate\")\n",
    "            elif routing_accuracy >= 0.6:\n",
    "                print(\"‚ö†Ô∏è Routing decisions are moderately accurate\")\n",
    "            else:\n",
    "                print(\"‚ùå Routing decisions need improvement\")\n",
    "            \n",
    "            # Strategy effectiveness analysis\n",
    "            if 'one_shot_llm' in strategy_performance and 'manager_coordination' in strategy_performance:\n",
    "                one_shot_acc = strategy_performance['one_shot_llm'].get('accuracy', 0)\n",
    "                manager_acc = strategy_performance['manager_coordination'].get('accuracy', 0)\n",
    "                one_shot_time = strategy_performance['one_shot_llm'].get('avg_execution_time', 0)\n",
    "                manager_time = strategy_performance['manager_coordination'].get('avg_execution_time', 0)\n",
    "                \n",
    "                print(f\"\\n‚öñÔ∏è Strategy Comparison:\")\n",
    "                print(f\"‚îú‚îÄ‚îÄ One-shot: {one_shot_acc:.1%} accuracy, {one_shot_time:.1f}s avg\")\n",
    "                print(f\"‚îú‚îÄ‚îÄ Manager: {manager_acc:.1%} accuracy, {manager_time:.1f}s avg\")\n",
    "                \n",
    "                if one_shot_acc > manager_acc:\n",
    "                    print(\"‚îî‚îÄ‚îÄ üí° One-shot performing better - consider simpler routing\")\n",
    "                elif manager_acc > one_shot_acc + 0.1:\n",
    "                    print(\"‚îî‚îÄ‚îÄ üí° Manager significantly better - routing working well\")\n",
    "                else:\n",
    "                    print(\"‚îî‚îÄ‚îÄ üí° Similar performance - routing providing good balance\")\n",
    "        else:\n",
    "            print(f\"‚ùå Routing test failed for {config}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Routing test error for {config}: {e}\")\n",
    "\n",
    "# Compare routing across configurations\n",
    "if len(routing_analysis_results) > 1:\n",
    "    print(f\"\\nüìä ROUTING COMPARISON ACROSS CONFIGURATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    routing_comparison_data = []\n",
    "    for config, results in routing_analysis_results.items():\n",
    "        routing_stats = results['routing_stats']\n",
    "        one_shot_acc = results['strategy_stats'].get('one_shot_llm', {}).get('accuracy', 0)\n",
    "        manager_acc = results['strategy_stats'].get('manager_coordination', {}).get('accuracy', 0)\n",
    "        \n",
    "        routing_comparison_data.append({\n",
    "            'Configuration': config,\n",
    "            'Overall Accuracy': f\"{results['overall_accuracy']:.1%}\",\n",
    "            'Routing Accuracy': f\"{routing_stats.get('routing_accuracy', 0):.1%}\",\n",
    "            'One-shot Accuracy': f\"{one_shot_acc:.1%}\",\n",
    "            'Manager Accuracy': f\"{manager_acc:.1%}\",\n",
    "            'One-shot Questions': routing_stats.get('one_shot_questions', 0),\n",
    "            'Manager Questions': routing_stats.get('manager_questions', 0)\n",
    "        })\n",
    "    \n",
    "    routing_comparison_df = pd.DataFrame(routing_comparison_data)\n",
    "    print(routing_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Provider-Specific Testing (Ollama, OpenRouter, Groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specific providers independently as requested\n",
    "\n",
    "print(\"üîå Provider-Specific Performance Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define provider-specific test configurations\n",
    "provider_configs = {\n",
    "    'groq_standard': {\n",
    "        'name': 'Groq (QwQ-32B)',\n",
    "        'config': 'groq',\n",
    "        'description': 'Standard Groq configuration with QwQ-32B model'\n",
    "    },\n",
    "    'groq_fast': {\n",
    "        'name': 'Groq (Llama-3.3-70B)', \n",
    "        'config': 'groq_fast',\n",
    "        'description': 'Faster Groq model for speed comparison'\n",
    "    },\n",
    "    'openrouter_free': {\n",
    "        'name': 'OpenRouter (Free)',\n",
    "        'config': 'openrouter',\n",
    "        'description': 'OpenRouter free tier model'\n",
    "    },\n",
    "    'openrouter_premium': {\n",
    "        'name': 'OpenRouter (Premium)',\n",
    "        'config': 'openrouter_premium', \n",
    "        'description': 'OpenRouter premium model for accuracy'\n",
    "    },\n",
    "    'ollama_local': {\n",
    "        'name': 'Ollama (Local)',\n",
    "        'config': 'ollama',\n",
    "        'description': 'Local Ollama deployment'\n",
    "    }\n",
    "}\n",
    "\n",
    "provider_test_results = {}\n",
    "\n",
    "# Standard test parameters for all providers\n",
    "test_params = {\n",
    "    'max_questions': 15,\n",
    "    'target_levels': [1, 2],\n",
    "    'include_files': True,\n",
    "    'include_images': True\n",
    "}\n",
    "\n",
    "for provider_key, provider_info in provider_configs.items():\n",
    "    config_name = provider_info['config']\n",
    "    provider_name = provider_info['name']\n",
    "    description = provider_info['description']\n",
    "    \n",
    "    print(f\"\\nüß™ Testing {provider_name}\")\n",
    "    print(f\"üìù {description}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Test if provider is available first\n",
    "        from agent_interface import get_agent_config\n",
    "        test_config = get_agent_config(config_name)\n",
    "        \n",
    "        result = run_gaia_test(\n",
    "            agent_config_name=config_name,\n",
    "            **test_params\n",
    "        )\n",
    "        \n",
    "        if result and 'evaluation_metadata' in result:\n",
    "            metadata = result['evaluation_metadata']\n",
    "            analysis = result.get('analysis', {})\n",
    "            \n",
    "            # Extract comprehensive metrics\n",
    "            provider_test_results[provider_key] = {\n",
    "                'name': provider_name,\n",
    "                'config': config_name,\n",
    "                'total_questions': metadata.get('total_questions', 0),\n",
    "                'correct_answers': metadata.get('correct_answers', 0),\n",
    "                'accuracy': metadata.get('overall_accuracy', 0),\n",
    "                'strategy_performance': analysis.get('strategy_performance', {}),\n",
    "                'level_performance': analysis.get('level_performance', {}),\n",
    "                'execution_time': 0,  # Will calculate from strategy data\n",
    "                'error_rate': 1 - analysis.get('error_analysis', {}).get('execution_success_rate', 1),\n",
    "                'full_result': result\n",
    "            }\n",
    "            \n",
    "            # Calculate average execution time\n",
    "            strategy_perf = analysis.get('strategy_performance', {})\n",
    "            if strategy_perf:\n",
    "                total_time = sum(stats.get('avg_execution_time', 0) * stats.get('total_questions', 0) \n",
    "                               for stats in strategy_perf.values())\n",
    "                total_questions = sum(stats.get('total_questions', 0) for stats in strategy_perf.values())\n",
    "                avg_time = total_time / total_questions if total_questions > 0 else 0\n",
    "                provider_test_results[provider_key]['execution_time'] = avg_time\n",
    "            \n",
    "            # Print immediate results\n",
    "            accuracy = metadata.get('overall_accuracy', 0)\n",
    "            total = metadata.get('total_questions', 0)\n",
    "            correct = metadata.get('correct_answers', 0)\n",
    "            \n",
    "            print(f\"‚úÖ {provider_name}: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "            print(f\"‚è±Ô∏è Avg execution time: {provider_test_results[provider_key]['execution_time']:.1f}s\")\n",
    "            \n",
    "            # Provider-specific insights\n",
    "            if 'groq' in provider_key:\n",
    "                print(f\"üöÄ Groq performance: {'Excellent' if accuracy >= 0.5 else 'Good' if accuracy >= 0.4 else 'Needs improvement'}\")\n",
    "            elif 'openrouter' in provider_key:\n",
    "                if 'free' in provider_key:\n",
    "                    print(f\"üí∞ Free tier performance: {'Good value' if accuracy >= 0.4 else 'Consider premium'}\")\n",
    "                else:\n",
    "                    print(f\"üíé Premium performance: {'Worth the cost' if accuracy >= 0.5 else 'Evaluate cost/benefit'}\")\n",
    "            elif 'ollama' in provider_key:\n",
    "                print(f\"üè† Local deployment: {'Viable alternative' if accuracy >= 0.4 else 'Cloud providers recommended'}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"‚ùå Test failed for {provider_name}\")\n",
    "            provider_test_results[provider_key] = {\n",
    "                'name': provider_name,\n",
    "                'config': config_name,\n",
    "                'error': 'Test execution failed'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Provider {provider_name} unavailable: {e}\")\n",
    "        provider_test_results[provider_key] = {\n",
    "            'name': provider_name,\n",
    "            'config': config_name,\n",
    "            'error': f'Provider unavailable: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Provider comparison table\n",
    "print(f\"\\nüìä PROVIDER PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "provider_comparison_data = []\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result:\n",
    "        provider_comparison_data.append({\n",
    "            'Provider': result['name'],\n",
    "            'Questions': result['total_questions'],\n",
    "            'Correct': result['correct_answers'],\n",
    "            'Accuracy': f\"{result['accuracy']:.1%}\",\n",
    "            'Avg Time (s)': f\"{result['execution_time']:.1f}\",\n",
    "            'Error Rate': f\"{result['error_rate']:.1%}\",\n",
    "            'GAIA Target': \"‚úÖ\" if result['accuracy'] >= 0.45 else \"‚ùå\"\n",
    "        })\n",
    "    else:\n",
    "        provider_comparison_data.append({\n",
    "            'Provider': result['name'],\n",
    "            'Questions': 'N/A',\n",
    "            'Correct': 'N/A', \n",
    "            'Accuracy': 'ERROR',\n",
    "            'Avg Time (s)': 'N/A',\n",
    "            'Error Rate': 'N/A',\n",
    "            'GAIA Target': \"‚ùå\"\n",
    "        })\n",
    "\n",
    "provider_comparison_df = pd.DataFrame(provider_comparison_data)\n",
    "print(provider_comparison_df.to_string(index=False))\n",
    "\n",
    "# Provider recommendations\n",
    "successful_providers = [r for r in provider_test_results.values() if 'error' not in r and r.get('accuracy', 0) > 0]\n",
    "\n",
    "if successful_providers:\n",
    "    # Best accuracy\n",
    "    best_accuracy_provider = max(successful_providers, key=lambda x: x['accuracy'])\n",
    "    # Fastest provider\n",
    "    fastest_provider = min(successful_providers, key=lambda x: x['execution_time'])\n",
    "    # Most reliable (lowest error rate)\n",
    "    most_reliable_provider = min(successful_providers, key=lambda x: x['error_rate'])\n",
    "    \n",
    "    print(f\"\\nüèÜ PROVIDER RECOMMENDATIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"üéØ Best Accuracy: {best_accuracy_provider['name']} ({best_accuracy_provider['accuracy']:.1%})\")\n",
    "    print(f\"‚ö° Fastest: {fastest_provider['name']} ({fastest_provider['execution_time']:.1f}s avg)\")\n",
    "    print(f\"üõ°Ô∏è Most Reliable: {most_reliable_provider['name']} ({most_reliable_provider['error_rate']:.1%} error rate)\")\n",
    "    \n",
    "    # Cost considerations\n",
    "    print(f\"\\nüí∞ Cost Considerations:\")\n",
    "    groq_providers = [p for p in successful_providers if 'groq' in p['config'].lower()]\n",
    "    openrouter_providers = [p for p in successful_providers if 'openrouter' in p['config'].lower()]\n",
    "    ollama_providers = [p for p in successful_providers if 'ollama' in p['config'].lower()]\n",
    "    \n",
    "    if groq_providers:\n",
    "        avg_groq_acc = sum(p['accuracy'] for p in groq_providers) / len(groq_providers)\n",
    "        print(f\"‚îú‚îÄ‚îÄ Groq: High performance, reasonable cost ({avg_groq_acc:.1%} avg accuracy)\")\n",
    "    \n",
    "    if openrouter_providers:\n",
    "        free_or = [p for p in openrouter_providers if 'free' in p['name'].lower()]\n",
    "        premium_or = [p for p in openrouter_providers if 'premium' in p['name'].lower()]\n",
    "        \n",
    "        if free_or:\n",
    "            print(f\"‚îú‚îÄ‚îÄ OpenRouter Free: Budget option ({free_or[0]['accuracy']:.1%} accuracy)\")\n",
    "        if premium_or:\n",
    "            print(f\"‚îú‚îÄ‚îÄ OpenRouter Premium: Premium option ({premium_or[0]['accuracy']:.1%} accuracy)\")\n",
    "    \n",
    "    if ollama_providers:\n",
    "        print(f\"‚îî‚îÄ‚îÄ Ollama: Zero cost, local control ({ollama_providers[0]['accuracy']:.1%} accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Interactive Visualization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One example of interactive visualization, then direct to source data\n",
    "\n",
    "print(\"üìä Interactive Performance Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create visualization if we have provider comparison data\n",
    "if provider_comparison_data and len(provider_comparison_data) > 1:\n",
    "    # Extract accuracy data for visualization\n",
    "    viz_data = []\n",
    "    for item in provider_comparison_data:\n",
    "        if item['Accuracy'] != 'ERROR':\n",
    "            accuracy_val = float(item['Accuracy'].strip('%')) / 100\n",
    "            time_val = float(item['Avg Time (s)']) if item['Avg Time (s)'] != 'N/A' else 0\n",
    "            \n",
    "            viz_data.append({\n",
    "                'Provider': item['Provider'],\n",
    "                'Accuracy': accuracy_val,\n",
    "                'Avg_Time': time_val,\n",
    "                'Questions': int(item['Questions']) if item['Questions'] != 'N/A' else 0\n",
    "            })\n",
    "    \n",
    "    if viz_data:\n",
    "        viz_df = pd.DataFrame(viz_data)\n",
    "        \n",
    "        # Create a simple performance visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        providers = viz_df['Provider']\n",
    "        accuracies = viz_df['Accuracy']\n",
    "        colors = ['green' if acc >= 0.45 else 'orange' if acc >= 0.35 else 'red' for acc in accuracies]\n",
    "        \n",
    "        bars1 = ax1.bar(providers, accuracies, color=colors, alpha=0.7)\n",
    "        ax1.axhline(y=0.45, color='red', linestyle='--', label='GAIA Target (45%)')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Provider Accuracy Comparison')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.1%}', ha='center', va='bottom')\n",
    "        \n",
    "        # Rotate x-axis labels for readability\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Speed vs Accuracy scatter plot\n",
    "        ax2.scatter(viz_df['Avg_Time'], viz_df['Accuracy'], \n",
    "                   s=viz_df['Questions']*10, alpha=0.6, c=colors)\n",
    "        \n",
    "        # Add provider labels\n",
    "        for idx, row in viz_df.iterrows():\n",
    "            ax2.annotate(row['Provider'], \n",
    "                        (row['Avg_Time'], row['Accuracy']),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8)\n",
    "        \n",
    "        ax2.axhline(y=0.45, color='red', linestyle='--', label='GAIA Target')\n",
    "        ax2.set_xlabel('Average Execution Time (seconds)')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Speed vs Accuracy Trade-off')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"üìà Visualization shows accuracy and speed trade-offs\")\n",
    "        print(\"üí° Larger dots = more questions tested\")\n",
    "        print(\"üéØ Red line = GAIA performance target (45%)\")\n",
    "        \n",
    "        # Analysis of the visualization\n",
    "        best_overall = viz_df.loc[viz_df['Accuracy'].idxmax()]\n",
    "        fastest = viz_df.loc[viz_df['Avg_Time'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nüìä Visual Analysis:\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Highest accuracy: {best_overall['Provider']} ({best_overall['Accuracy']:.1%})\")\n",
    "        print(f\"‚îú‚îÄ‚îÄ Fastest execution: {fastest['Provider']} ({fastest['Avg_Time']:.1f}s)\")\n",
    "        \n",
    "        # Efficiency score (accuracy / time)\n",
    "        viz_df['Efficiency'] = viz_df['Accuracy'] / (viz_df['Avg_Time'] + 1)  # +1 to avoid division by zero\n",
    "        most_efficient = viz_df.loc[viz_df['Efficiency'].idxmax()]\n",
    "        print(f\"‚îî‚îÄ‚îÄ Most efficient: {most_efficient['Provider']} (best accuracy/time ratio)\")\n",
    "\n",
    "print(f\"\\nüíæ Direct Data Access:\")\n",
    "print(\"For detailed analysis, access the source data:\")\n",
    "print(\"‚îú‚îÄ‚îÄ baseline_results: Performance across question types\")\n",
    "print(\"‚îú‚îÄ‚îÄ routing_analysis_results: Routing effectiveness data\") \n",
    "print(\"‚îú‚îÄ‚îÄ provider_test_results: Provider-specific performance\")\n",
    "print(\"‚îî‚îÄ‚îÄ All results include full evaluation metadata and analysis\")\n",
    "\n",
    "# Show how to access specific data\n",
    "print(f\"\\nüîç Example Data Access:\")\n",
    "print(\"# Access baseline results\")\n",
    "print(\"baseline_results['level_1_only']['accuracy']\")\n",
    "print(\"\\n# Access provider comparison\")\n",
    "print(\"provider_test_results['groq_standard']['strategy_performance']\")\n",
    "print(\"\\n# Access detailed analysis\")\n",
    "print(\"provider_test_results['groq_standard']['full_result']['analysis']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Failure Pattern Analysis & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep analysis of failure patterns for improvement insights\n",
    "\n",
    "print(\"üîç Comprehensive Failure Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze failures across all test results\n",
    "all_test_results = []\n",
    "\n",
    "# Collect results from baseline tests\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'full_result' in result:\n",
    "        all_test_results.append({\n",
    "            'source': f'baseline_{config_key}',\n",
    "            'result': result['full_result']\n",
    "        })\n",
    "\n",
    "# Collect results from provider tests  \n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'full_result' in result:\n",
    "        all_test_results.append({\n",
    "            'source': f'provider_{provider_key}',\n",
    "            'result': result['full_result']\n",
    "        })\n",
    "\n",
    "failure_analyses = {}\n",
    "\n",
    "for test_data in all_test_results:\n",
    "    source = test_data['source']\n",
    "    result = test_data['result']\n",
    "    \n",
    "    print(f\"\\nüîç Analyzing failures in {source}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        failure_analysis = analyze_failure_patterns(result)\n",
    "        \n",
    "        if failure_analysis and 'failure_patterns' in failure_analysis:\n",
    "            failure_analyses[source] = failure_analysis\n",
    "            \n",
    "            patterns = failure_analysis['failure_patterns']\n",
    "            recommendations = failure_analysis.get('recommendations', [])\n",
    "            \n",
    "            # Print key failure insights\n",
    "            print(f\"üìä Failure Distribution:\")\n",
    "            \n",
    "            # By level\n",
    "            level_failures = patterns.get('by_level', {})\n",
    "            total_failures = sum(level_failures.values())\n",
    "            if total_failures > 0:\n",
    "                print(f\"‚îú‚îÄ‚îÄ By Level:\")\n",
    "                for level, count in level_failures.items():\n",
    "                    percentage = count / total_failures * 100\n",
    "                    print(f\"‚îÇ   ‚îú‚îÄ‚îÄ Level {level}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # By strategy\n",
    "            strategy_failures = patterns.get('by_strategy', {})\n",
    "            if strategy_failures:\n",
    "                print(f\"‚îú‚îÄ‚îÄ By Strategy:\")\n",
    "                for strategy, count in strategy_failures.items():\n",
    "                    percentage = count / total_failures * 100\n",
    "                    print(f\"‚îÇ   ‚îú‚îÄ‚îÄ {strategy}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Execution issues\n",
    "            exec_failures = patterns.get('execution_failures', 0)\n",
    "            if exec_failures > 0:\n",
    "                print(f\"‚îî‚îÄ‚îÄ Execution Failures: {exec_failures}\")\n",
    "            \n",
    "            # Top recommendations\n",
    "            if recommendations:\n",
    "                print(f\"\\nüí° Top Recommendations for {source}:\")\n",
    "                for i, rec in enumerate(recommendations[:3], 1):\n",
    "                    print(f\"  {i}. {rec}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ No significant failure patterns found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failure analysis error: {e}\")\n",
    "\n",
    "# Cross-test pattern analysis\n",
    "if len(failure_analyses) > 1:\n",
    "    print(f\"\\nüîÑ Cross-Test Pattern Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find common failure patterns across tests\n",
    "    all_level_failures = {}\n",
    "    all_strategy_failures = {}\n",
    "    all_execution_failures = 0\n",
    "    \n",
    "    for source, analysis in failure_analyses.items():\n",
    "        patterns = analysis['failure_patterns']\n",
    "        \n",
    "        # Aggregate level failures\n",
    "        for level, count in patterns.get('by_level', {}).items():\n",
    "            all_level_failures[level] = all_level_failures.get(level, 0) + count\n",
    "        \n",
    "        # Aggregate strategy failures\n",
    "        for strategy, count in patterns.get('by_strategy', {}).items():\n",
    "            all_strategy_failures[strategy] = all_strategy_failures.get(strategy, 0) + count\n",
    "        \n",
    "        # Aggregate execution failures\n",
    "        all_execution_failures += patterns.get('execution_failures', 0)\n",
    "    \n",
    "    print(f\"üìä Aggregated Failure Patterns:\")\n",
    "    \n",
    "    if all_level_failures:\n",
    "        total_level_failures = sum(all_level_failures.values())\n",
    "        print(f\"‚îú‚îÄ‚îÄ Most problematic levels:\")\n",
    "        sorted_levels = sorted(all_level_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for level, count in sorted_levels:\n",
    "            percentage = count / total_level_failures * 100\n",
    "            print(f\"‚îÇ   ‚îú‚îÄ‚îÄ Level {level}: {count} failures ({percentage:.1f}%)\")\n",
    "    \n",
    "    if all_strategy_failures:\n",
    "        total_strategy_failures = sum(all_strategy_failures.values())\n",
    "        print(f\"‚îú‚îÄ‚îÄ Most problematic strategies:\")\n",
    "        sorted_strategies = sorted(all_strategy_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for strategy, count in sorted_strategies:\n",
    "            percentage = count / total_strategy_failures * 100\n",
    "            print(f\"‚îÇ   ‚îú‚îÄ‚îÄ {strategy}: {count} failures ({percentage:.1f}%)\")\n",
    "    \n",
    "    if all_execution_failures > 0:\n",
    "        print(f\"‚îî‚îÄ‚îÄ Total execution failures: {all_execution_failures}\")\n",
    "    \n",
    "    # Global recommendations\n",
    "    print(f\"\\nüéØ Global Optimization Priorities:\")\n",
    "    \n",
    "    if all_level_failures:\n",
    "        worst_level = max(all_level_failures, key=all_level_failures.get)\n",
    "        print(f\"1. Focus on Level {worst_level} performance improvement\")\n",
    "    \n",
    "    if all_strategy_failures:\n",
    "        worst_strategy = max(all_strategy_failures, key=all_strategy_failures.get)\n",
    "        print(f\"2. Optimize {worst_strategy.replace('_', ' ')} strategy\")\n",
    "    \n",
    "    if all_execution_failures > 5:\n",
    "        print(f\"3. Improve system reliability (reduce execution failures)\")\n",
    "    \n",
    "    print(f\"4. Consider routing adjustments based on failure patterns\")\n",
    "    print(f\"5. Enhance answer formatting and validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Production Readiness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive assessment for production deployment\n",
    "\n",
    "print(\"üè≠ Production Readiness Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all performance data for assessment\n",
    "assessment_data = {\n",
    "    'baseline_performance': {},\n",
    "    'provider_performance': {},\n",
    "    'routing_effectiveness': {},\n",
    "    'system_reliability': {},\n",
    "    'file_processing': {},\n",
    "    'overall_metrics': {}\n",
    "}\n",
    "\n",
    "# Extract baseline performance metrics\n",
    "successful_baselines = [r for r in baseline_results.values() if 'error' not in r]\n",
    "if successful_baselines:\n",
    "    baseline_accuracies = [r['accuracy'] for r in successful_baselines]\n",
    "    assessment_data['baseline_performance'] = {\n",
    "        'avg_accuracy': np.mean(baseline_accuracies),\n",
    "        'min_accuracy': np.min(baseline_accuracies),\n",
    "        'max_accuracy': np.max(baseline_accuracies),\n",
    "        'std_accuracy': np.std(baseline_accuracies),\n",
    "        'gaia_target_met': any(acc >= 0.45 for acc in baseline_accuracies)\n",
    "    }\n",
    "\n",
    "# Extract provider performance metrics  \n",
    "successful_providers = [r for r in provider_test_results.values() if 'error' not in r]\n",
    "if successful_providers:\n",
    "    provider_accuracies = [r['accuracy'] for r in successful_providers]\n",
    "    provider_times = [r['execution_time'] for r in successful_providers]\n",
    "    provider_errors = [r['error_rate'] for r in successful_providers]\n",
    "    \n",
    "    assessment_data['provider_performance'] = {\n",
    "        'available_providers': len(successful_providers),\n",
    "        'avg_accuracy': np.mean(provider_accuracies),\n",
    "        'best_accuracy': np.max(provider_accuracies),\n",
    "        'avg_execution_time': np.mean(provider_times),\n",
    "        'fastest_time': np.min(provider_times),\n",
    "        'avg_error_rate': np.mean(provider_errors),\n",
    "        'best_reliability': np.min(provider_errors)\n",
    "    }\n",
    "\n",
    "# Extract routing effectiveness metrics\n",
    "if routing_analysis_results:\n",
    "    routing_accuracies = [r['routing_stats'].get('routing_accuracy', 0) for r in routing_analysis_results.values()]\n",
    "    assessment_data['routing_effectiveness'] = {\n",
    "        'avg_routing_accuracy': np.mean(routing_accuracies),\n",
    "        'min_routing_accuracy': np.min(routing_accuracies),\n",
    "        'routing_working': np.mean(routing_accuracies) >= 0.7\n",
    "    }\n",
    "\n",
    "# System reliability assessment\n",
    "total_execution_failures = 0\n",
    "total_questions = 0\n",
    "\n",
    "for result_set in [baseline_results.values(), provider_test_results.values()]:\n",
    "    for result in result_set:\n",
    "        if 'full_result' in result:\n",
    "            error_analysis = result['full_result'].get('analysis', {}).get('error_analysis', {})\n",
    "            exec_errors = error_analysis.get('execution_errors', 0)\n",
    "            total_exec_questions = result.get('total_questions', 0)\n",
    "            total_execution_failures += exec_errors\n",
    "            total_questions += total_exec_questions\n",
    "\n",
    "system_reliability = 1 - (total_execution_failures / total_questions) if total_questions > 0 else 0\n",
    "assessment_data['system_reliability'] = {\n",
    "    'execution_success_rate': system_reliability,\n",
    "    'total_questions_tested': total_questions,\n",
    "    'total_failures': total_execution_failures,\n",
    "    'reliable': system_reliability >= 0.95\n",
    "}\n",
    "\n",
    "# Overall metrics calculation\n",
    "all_accuracies = []\n",
    "if successful_baselines:\n",
    "    all_accuracies.extend([r['accuracy'] for r in successful_baselines])\n",
    "if successful_providers:\n",
    "    all_accuracies.extend([r['accuracy'] for r in successful_providers])\n",
    "\n",
    "if all_accuracies:\n",
    "    assessment_data['overall_metrics'] = {\n",
    "        'best_accuracy': np.max(all_accuracies),\n",
    "        'avg_accuracy': np.mean(all_accuracies),\n",
    "        'consistency': 1 - np.std(all_accuracies),  # Higher consistency = lower std\n",
    "        'gaia_target_achievement': np.max(all_accuracies) >= 0.45,\n",
    "        'production_ready_accuracy': np.max(all_accuracies) >= 0.50\n",
    "    }\n",
    "\n",
    "# Production readiness scoring\n",
    "print(f\"üìä PRODUCTION READINESS SCORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "readiness_score = 0\n",
    "max_score = 100\n",
    "\n",
    "# Accuracy score (40 points)\n",
    "if assessment_data['overall_metrics']:\n",
    "    best_acc = assessment_data['overall_metrics']['best_accuracy']\n",
    "    if best_acc >= 0.60:\n",
    "        accuracy_score = 40\n",
    "    elif best_acc >= 0.50:\n",
    "        accuracy_score = 35\n",
    "    elif best_acc >= 0.45:\n",
    "        accuracy_score = 30\n",
    "    elif best_acc >= 0.35:\n",
    "        accuracy_score = 20\n",
    "    else:\n",
    "        accuracy_score = 10\n",
    "    \n",
    "    readiness_score += accuracy_score\n",
    "    print(f\"‚úÖ Accuracy Score: {accuracy_score}/40 (Best: {best_acc:.1%})\")\n",
    "\n",
    "# Reliability score (25 points)\n",
    "if assessment_data['system_reliability']:\n",
    "    reliability = assessment_data['system_reliability']['execution_success_rate']\n",
    "    if reliability >= 0.98:\n",
    "        reliability_score = 25\n",
    "    elif reliability >= 0.95:\n",
    "        reliability_score = 20\n",
    "    elif reliability >= 0.90:\n",
    "        reliability_score = 15\n",
    "    else:\n",
    "        reliability_score = 10\n",
    "    \n",
    "    readiness_score += reliability_score\n",
    "    print(f\"‚úÖ Reliability Score: {reliability_score}/25 (Success Rate: {reliability:.1%})\")\n",
    "\n",
    "# Provider availability score (15 points)\n",
    "if assessment_data['provider_performance']:\n",
    "    available_providers = assessment_data['provider_performance']['available_providers']\n",
    "    if available_providers >= 4:\n",
    "        provider_score = 15\n",
    "    elif available_providers >= 3:\n",
    "        provider_score = 12\n",
    "    elif available_providers >= 2:\n",
    "        provider_score = 8\n",
    "    else:\n",
    "        provider_score = 5\n",
    "    \n",
    "    readiness_score += provider_score\n",
    "    print(f\"‚úÖ Provider Score: {provider_score}/15 ({available_providers} providers available)\")\n",
    "\n",
    "# Routing effectiveness score (10 points)\n",
    "if assessment_data['routing_effectiveness']:\n",
    "    routing_acc = assessment_data['routing_effectiveness']['avg_routing_accuracy']\n",
    "    if routing_acc >= 0.80:\n",
    "        routing_score = 10\n",
    "    elif routing_acc >= 0.70:\n",
    "        routing_score = 8\n",
    "    elif routing_acc >= 0.60:\n",
    "        routing_score = 6\n",
    "    else:\n",
    "        routing_score = 3\n",
    "    \n",
    "    readiness_score += routing_score\n",
    "    print(f\"‚úÖ Routing Score: {routing_score}/10 (Accuracy: {routing_acc:.1%})\")\n",
    "\n",
    "# Consistency score (10 points)\n",
    "if assessment_data['overall_metrics']:\n",
    "    consistency = assessment_data['overall_metrics']['consistency']\n",
    "    if consistency >= 0.90:\n",
    "        consistency_score = 10\n",
    "    elif consistency >= 0.80:\n",
    "        consistency_score = 8\n",
    "    elif consistency >= 0.70:\n",
    "        consistency_score = 6\n",
    "    else:\n",
    "        consistency_score = 3\n",
    "    \n",
    "    readiness_score += consistency_score\n",
    "    print(f\"‚úÖ Consistency Score: {consistency_score}/10 (Consistency: {consistency:.1%})\")\n",
    "\n",
    "print(f\"\\nüèÜ OVERALL READINESS SCORE: {readiness_score}/{max_score} ({readiness_score/max_score*100:.1f}%)\")\n",
    "\n",
    "# Production deployment recommendation\n",
    "print(f\"\\nüöÄ PRODUCTION DEPLOYMENT RECOMMENDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if readiness_score >= 85:\n",
    "    recommendation = \"üü¢ READY FOR PRODUCTION\"\n",
    "    details = \"System demonstrates excellent performance, reliability, and consistency.\"\n",
    "    next_steps = [\n",
    "        \"Deploy to production environment\",\n",
    "        \"Set up monitoring and alerting\",\n",
    "        \"Implement gradual rollout strategy\",\n",
    "        \"Document operational procedures\"\n",
    "    ]\n",
    "elif readiness_score >= 70:\n",
    "    recommendation = \"üü° READY WITH MINOR OPTIMIZATIONS\"\n",
    "    details = \"System shows good performance but could benefit from targeted improvements.\"\n",
    "    next_steps = [\n",
    "        \"Address identified failure patterns\",\n",
    "        \"Optimize underperforming configurations\",\n",
    "        \"Enhance error handling\",\n",
    "        \"Conduct limited production trial\"\n",
    "    ]\n",
    "elif readiness_score >= 55:\n",
    "    recommendation = \"üü† NEEDS IMPROVEMENT BEFORE PRODUCTION\"\n",
    "    details = \"System shows promise but requires significant improvements.\"\n",
    "    next_steps = [\n",
    "        \"Focus on accuracy improvements\",\n",
    "        \"Enhance system reliability\",\n",
    "        \"Optimize routing decisions\",\n",
    "        \"Conduct additional testing cycles\"\n",
    "    ]\n",
    "else:\n",
    "    recommendation = \"üî¥ NOT READY FOR PRODUCTION\"\n",
    "    details = \"System requires substantial development before production deployment.\"\n",
    "    next_steps = [\n",
    "        \"Review core architecture\",\n",
    "        \"Improve model configurations\",\n",
    "        \"Enhance error handling and reliability\",\n",
    "        \"Return to development phase\"\n",
    "    ]\n",
    "\n",
    "print(f\"{recommendation}\")\n",
    "print(f\"üìù {details}\")\n",
    "print(f\"\\nüìã Recommended Next Steps:\")\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "# Specific recommendations based on assessment data\n",
    "print(f\"\\nüéØ SPECIFIC OPTIMIZATION RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Accuracy-based recommendations\n",
    "if assessment_data['overall_metrics']:\n",
    "    best_acc = assessment_data['overall_metrics']['best_accuracy']\n",
    "    if best_acc < 0.45:\n",
    "        recommendations.append(\"üéØ Priority: Achieve GAIA benchmark target (45% accuracy)\")\n",
    "    elif best_acc < 0.55:\n",
    "        recommendations.append(\"üìà Focus: Improve accuracy to competitive levels (55%+)\")\n",
    "\n",
    "# Provider-based recommendations\n",
    "if assessment_data['provider_performance']:\n",
    "    best_provider_acc = assessment_data['provider_performance']['best_accuracy']\n",
    "    avg_provider_acc = assessment_data['provider_performance']['avg_accuracy']\n",
    "    \n",
    "    if best_provider_acc - avg_provider_acc > 0.1:\n",
    "        recommendations.append(\"‚öñÔ∏è Standardize: Large performance gap between providers - optimize configurations\")\n",
    "    \n",
    "    fastest_time = assessment_data['provider_performance']['fastest_time']\n",
    "    avg_time = assessment_data['provider_performance']['avg_execution_time']\n",
    "    \n",
    "    if avg_time > 30:\n",
    "        recommendations.append(\"‚ö° Speed: Reduce average execution time below 30 seconds\")\n",
    "\n",
    "# Reliability-based recommendations\n",
    "if assessment_data['system_reliability']:\n",
    "    reliability = assessment_data['system_reliability']['execution_success_rate']\n",
    "    if reliability < 0.95:\n",
    "        recommendations.append(\"üõ°Ô∏è Reliability: Improve execution success rate above 95%\")\n",
    "\n",
    "# Routing-based recommendations\n",
    "if assessment_data['routing_effectiveness']:\n",
    "    routing_acc = assessment_data['routing_effectiveness']['avg_routing_accuracy']\n",
    "    if routing_acc < 0.75:\n",
    "        recommendations.append(\"üîÄ Routing: Improve complexity detection and routing accuracy\")\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(f\"  ‚Ä¢ {rec}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No critical optimizations needed - system performing well\")\n",
    "\n",
    "# Generate final comprehensive report\n",
    "print(f\"\\nüìÑ Generating Comprehensive Production Report...\")\n",
    "\n",
    "# Use the best performing configuration for final report\n",
    "best_config = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result and result.get('accuracy', 0) > best_accuracy:\n",
    "        best_accuracy = result['accuracy']\n",
    "        best_config = result['config']\n",
    "\n",
    "if best_config:\n",
    "    try:\n",
    "        # Run one final comprehensive test with best configuration\n",
    "        final_test = run_gaia_test(\n",
    "            agent_config_name=best_config,\n",
    "            max_questions=30,  # Larger sample for final assessment\n",
    "            target_levels=[1, 2, 3],\n",
    "            include_files=True,\n",
    "            include_images=True\n",
    "        )\n",
    "        \n",
    "        if final_test:\n",
    "            report = generate_test_report(final_test, best_config, save_to_file=True)\n",
    "            print(f\"‚úÖ Final report generated and saved\")\n",
    "            print(f\"üìä Final test: {final_test['evaluation_metadata']['overall_accuracy']:.1%} accuracy\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Final test failed - using existing data for assessment\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Final test error: {e} - using existing data for assessment\")\n",
    "\n",
    "print(f\"\\nüéâ PRODUCTION VALIDATION COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Total Questions Tested: {assessment_data['system_reliability']['total_questions_tested']}\")\n",
    "print(f\"üèÜ Best Accuracy Achieved: {assessment_data['overall_metrics']['best_accuracy']:.1%}\")\n",
    "print(f\"üõ°Ô∏è System Reliability: {assessment_data['system_reliability']['execution_success_rate']:.1%}\")\n",
    "print(f\"‚ö° Available Providers: {assessment_data['provider_performance']['available_providers']}\")\n",
    "print(f\"üîÄ Routing Effectiveness: {assessment_data['routing_effectiveness']['avg_routing_accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\nüíæ All test data available in variables:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ baseline_results: Baseline performance data\")\n",
    "print(f\"‚îú‚îÄ‚îÄ provider_test_results: Provider-specific results\")\n",
    "print(f\"‚îú‚îÄ‚îÄ routing_analysis_results: Routing effectiveness analysis\")\n",
    "print(f\"‚îú‚îÄ‚îÄ failure_analyses: Comprehensive failure pattern analysis\")\n",
    "print(f\"‚îî‚îÄ‚îÄ assessment_data: Production readiness metrics\")\n",
    "\n",
    "print(f\"\\nüöÄ System is now ready for production decision based on comprehensive testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Data Export & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Export all results for further analysis and reporting\n",
    "\n",
    "print(\"üíæ Data Export & Final Reporting\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create comprehensive data export\n",
    "export_data = {\n",
    "    'test_session': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_questions_tested': assessment_data['system_reliability']['total_questions_tested'],\n",
    "        'testing_duration': 'Session duration tracked',\n",
    "        'framework_version': '2.0'\n",
    "    },\n",
    "    'baseline_results': baseline_results,\n",
    "    'provider_results': provider_test_results,\n",
    "    'routing_analysis': routing_analysis_results,\n",
    "    'failure_analysis': failure_analyses,\n",
    "    'production_assessment': assessment_data,\n",
    "    'readiness_score': readiness_score,\n",
    "    'recommendation': recommendation\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "export_file = Path(\"logs\") / f\"production_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "try:\n",
    "    with open(export_file, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Comprehensive results exported: {export_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Export error: {e}\")\n",
    "\n",
    "# Create summary CSV for quick analysis\n",
    "summary_data = []\n",
    "\n",
    "# Add baseline results\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'error' not in result:\n",
    "        summary_data.append({\n",
    "            'Test_Type': 'Baseline',\n",
    "            'Configuration': config_key,\n",
    "            'Total_Questions': result['total_questions'],\n",
    "            'Correct_Answers': result['correct_answers'],\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'GAIA_Target_Met': result['accuracy'] >= 0.45\n",
    "        })\n",
    "\n",
    "# Add provider results\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result:\n",
    "        summary_data.append({\n",
    "            'Test_Type': 'Provider',\n",
    "            'Configuration': result['config'],\n",
    "            'Total_Questions': result['total_questions'],\n",
    "            'Correct_Answers': result['correct_answers'],\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'GAIA_Target_Met': result['accuracy'] >= 0.45\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv = Path(\"logs\") / f\"validation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    try:\n",
    "        summary_df.to_csv(summary_csv, index=False)\n",
    "        print(f\"‚úÖ Summary CSV exported: {summary_csv}\")\n",
    "        \n",
    "        # Display final summary table\n",
    "        print(f\"\\nüìä FINAL SUMMARY TABLE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CSV export error: {e}\")\n",
    "\n",
    "# Performance insights summary\n",
    "print(f\"\\nüìà KEY PERFORMANCE INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if assessment_data['overall_metrics']:\n",
    "    metrics = assessment_data['overall_metrics']\n",
    "    print(f\"üéØ Best Performance: {metrics['best_accuracy']:.1%} accuracy\")\n",
    "    print(f\"üìä Average Performance: {metrics['avg_accuracy']:.1%} accuracy\")\n",
    "    print(f\"üèÜ GAIA Target: {'‚úÖ Achieved' if metrics['gaia_target_achievement'] else '‚ùå Not achieved'}\")\n",
    "    print(f\"üöÄ Production Ready: {'‚úÖ Yes' if metrics['production_ready_accuracy'] else '‚ùå Needs improvement'}\")\n",
    "\n",
    "if assessment_data['provider_performance']:\n",
    "    provider_metrics = assessment_data['provider_performance']\n",
    "    print(f\"‚ö° Fastest Provider: {provider_metrics['fastest_time']:.1f}s average\")\n",
    "    print(f\"üõ°Ô∏è Best Reliability: {(1-provider_metrics['best_reliability']):.1%} success rate\")\n",
    "\n",
    "print(f\"\\nüéì Production Validation Complete!\")\n",
    "print(f\"Use the exported data and reports for production deployment decisions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-assignment-agent-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
