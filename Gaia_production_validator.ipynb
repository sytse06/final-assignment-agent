{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Validator\n",
    "## Implementation plan, testing framework, and deployment\n",
    "\n",
    "**Objective:** Complete implementation roadmap and evaluation system  \n",
    "**Target:** 45-55% GAIA accuracy within $10 budget\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Testing Framework Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the production testing framework\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import our testing framework\n",
    "from agent_testing import (\n",
    "    run_quick_gaia_test, run_gaia_test, run_smart_routing_test,\n",
    "    compare_agent_configs, analyze_failure_patterns, \n",
    "    test_file_vs_text_performance, generate_test_report\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ Production Validator - GAIA Testing Framework\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick system validation\n",
    "print(\"\\nğŸ” Quick System Validation\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    demo_result = run_quick_gaia_test(\"groq\")\n",
    "    \n",
    "    if demo_result and 'evaluation_metadata' in demo_result:\n",
    "        metadata = demo_result['evaluation_metadata']\n",
    "        accuracy = metadata.get('overall_accuracy', 0)\n",
    "        total = metadata.get('total_questions', 0)\n",
    "        correct = metadata.get('correct_answers', 0)\n",
    "        \n",
    "        print(f\"âœ… Framework operational\")\n",
    "        print(f\"ğŸ“Š Demo test: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "        \n",
    "        if accuracy >= 0.5:\n",
    "            print(\"ğŸš€ System showing good performance\")\n",
    "        elif accuracy >= 0.3:\n",
    "            print(\"âš ï¸ System showing moderate performance\")\n",
    "        else:\n",
    "            print(\"âŒ System needs significant improvement\")\n",
    "    else:\n",
    "        print(\"âŒ Demo test failed - check system setup\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Framework error: {e}\")\n",
    "    print(\"Check Agent Builder notebook results first\")\n",
    "\n",
    "# Display available test functions\n",
    "test_functions = {\n",
    "    'run_quick_gaia_test': 'Quick 5-question validation (Level 1)',\n",
    "    'run_gaia_test': 'Comprehensive testing with custom parameters',\n",
    "    'run_smart_routing_test': 'Analyze routing effectiveness',\n",
    "    'compare_agent_configs': 'Compare multiple configurations',\n",
    "    'test_file_vs_text_performance': 'File processing vs text-only',\n",
    "    'analyze_failure_patterns': 'Deep dive into failure analysis',\n",
    "    'generate_test_report': 'Professional reporting'\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“‹ Available Testing Functions:\")\n",
    "for func, description in test_functions.items():\n",
    "    print(f\"â”œâ”€â”€ {func}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Performance Baseline Establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish performance baselines across different question types\n",
    "\n",
    "print(\"ğŸ“Š Establishing Performance Baselines\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define baseline test configurations\n",
    "baseline_configs = {\n",
    "    'level_1_only': {\n",
    "        'name': 'Level 1 Questions Only',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'level_1_2_mix': {\n",
    "        'name': 'Level 1 & 2 Mixed',\n",
    "        'params': {\n",
    "            'max_questions': 20,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'all_levels': {\n",
    "        'name': 'All Difficulty Levels',\n",
    "        'params': {\n",
    "            'max_questions': 25,\n",
    "            'target_levels': [1, 2, 3],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'text_only': {\n",
    "        'name': 'Text-Only Questions',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': False,\n",
    "            'include_images': False\n",
    "        }\n",
    "    },\n",
    "    'with_files': {\n",
    "        'name': 'File-Based Questions',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run baseline tests with groq configuration\n",
    "baseline_results = {}\n",
    "baseline_agent = \"groq\"  # Use most reliable configuration\n",
    "\n",
    "for config_key, config_data in baseline_configs.items():\n",
    "    config_name = config_data['name']\n",
    "    params = config_data['params']\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Running: {config_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = run_gaia_test(\n",
    "            agent_config_name=baseline_agent,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if result and 'evaluation_metadata' in result:\n",
    "            metadata = result['evaluation_metadata']\n",
    "            analysis = result.get('analysis', {})\n",
    "            \n",
    "            # Extract key metrics\n",
    "            baseline_results[config_key] = {\n",
    "                'name': config_name,\n",
    "                'total_questions': metadata.get('total_questions', 0),\n",
    "                'correct_answers': metadata.get('correct_answers', 0),\n",
    "                'accuracy': metadata.get('overall_accuracy', 0),\n",
    "                'analysis': analysis,\n",
    "                'full_result': result\n",
    "            }\n",
    "            \n",
    "            # Print immediate summary\n",
    "            accuracy = metadata.get('overall_accuracy', 0)\n",
    "            total = metadata.get('total_questions', 0)\n",
    "            correct = metadata.get('correct_answers', 0)\n",
    "            \n",
    "            print(f\"âœ… Completed: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "            \n",
    "            # Check GAIA target\n",
    "            if accuracy >= 0.45:\n",
    "                print(\"ğŸ† GAIA target achieved (45%+)\")\n",
    "            elif accuracy >= 0.35:\n",
    "                print(\"âš ï¸ Approaching GAIA target\")\n",
    "            else:\n",
    "                print(\"âŒ Below GAIA target\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"âŒ Test failed for {config_name}\")\n",
    "            baseline_results[config_key] = {'name': config_name, 'error': 'Test failed'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in {config_name}: {e}\")\n",
    "        baseline_results[config_key] = {'name': config_name, 'error': str(e)}\n",
    "\n",
    "# Baseline summary table\n",
    "print(f\"\\nğŸ“Š BASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_df_data = []\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'error' not in result:\n",
    "        baseline_df_data.append({\n",
    "            'Test Configuration': result['name'],\n",
    "            'Questions': result['total_questions'],\n",
    "            'Correct': result['correct_answers'],\n",
    "            'Accuracy': f\"{result['accuracy']:.1%}\",\n",
    "            'GAIA Target': \"âœ…\" if result['accuracy'] >= 0.45 else \"âŒ\"\n",
    "        })\n",
    "    else:\n",
    "        baseline_df_data.append({\n",
    "            'Test Configuration': result['name'],\n",
    "            'Questions': 0,\n",
    "            'Correct': 0,\n",
    "            'Accuracy': \"ERROR\",\n",
    "            'GAIA Target': \"âŒ\"\n",
    "        })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_df_data)\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "# Identify best and worst performing configurations\n",
    "if baseline_df_data:\n",
    "    successful_results = [r for r in baseline_results.values() if 'error' not in r and r.get('accuracy', 0) > 0]\n",
    "    if successful_results:\n",
    "        best_config = max(successful_results, key=lambda x: x['accuracy'])\n",
    "        worst_config = min(successful_results, key=lambda x: x['accuracy'])\n",
    "        \n",
    "        print(f\"\\nğŸ† Best Performance: {best_config['name']} ({best_config['accuracy']:.1%})\")\n",
    "        print(f\"âš ï¸ Needs Improvement: {worst_config['name']} ({worst_config['accuracy']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Routing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into routing effectiveness and optimization\n",
    "\n",
    "print(\"ğŸ”€ Smart Routing Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test routing with performance-optimized configuration\n",
    "routing_test_configs = [\"groq\", \"performance\"]\n",
    "\n",
    "routing_analysis_results = {}\n",
    "\n",
    "for config in routing_test_configs:\n",
    "    print(f\"\\nğŸ§ª Testing routing with {config} configuration\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        routing_result = run_smart_routing_test(config)\n",
    "        \n",
    "        if routing_result and 'analysis' in routing_result:\n",
    "            analysis = routing_result['analysis']\n",
    "            routing_analysis = analysis.get('routing_analysis', {})\n",
    "            strategy_performance = analysis.get('strategy_performance', {})\n",
    "            level_performance = analysis.get('level_performance', {})\n",
    "            \n",
    "            routing_analysis_results[config] = {\n",
    "                'routing_stats': routing_analysis,\n",
    "                'strategy_stats': strategy_performance,\n",
    "                'level_stats': level_performance,\n",
    "                'overall_accuracy': routing_result['evaluation_metadata']['overall_accuracy']\n",
    "            }\n",
    "            \n",
    "            # Print routing effectiveness\n",
    "            one_shot_count = routing_analysis.get('one_shot_questions', 0)\n",
    "            manager_count = routing_analysis.get('manager_questions', 0)\n",
    "            routing_accuracy = routing_analysis.get('routing_accuracy', 0)\n",
    "            \n",
    "            print(f\"ğŸ“Š Question Distribution:\")\n",
    "            print(f\"â”œâ”€â”€ One-shot LLM: {one_shot_count} questions\")\n",
    "            print(f\"â”œâ”€â”€ Manager Coordination: {manager_count} questions\")\n",
    "            print(f\"â””â”€â”€ Routing Accuracy: {routing_accuracy:.1%}\")\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ Strategy Performance:\")\n",
    "            for strategy, stats in strategy_performance.items():\n",
    "                accuracy = stats.get('accuracy', 0)\n",
    "                avg_time = stats.get('avg_execution_time', 0)\n",
    "                total = stats.get('total_questions', 0)\n",
    "                print(f\"â”œâ”€â”€ {strategy.replace('_', ' ').title()}: {accuracy:.1%} accuracy, {avg_time:.1f}s avg ({total} questions)\")\n",
    "            \n",
    "            # Routing insights\n",
    "            print(f\"\\nğŸ’¡ Routing Insights:\")\n",
    "            if routing_accuracy >= 0.8:\n",
    "                print(\"âœ… Routing decisions are highly accurate\")\n",
    "            elif routing_accuracy >= 0.6:\n",
    "                print(\"âš ï¸ Routing decisions are moderately accurate\")\n",
    "            else:\n",
    "                print(\"âŒ Routing decisions need improvement\")\n",
    "            \n",
    "            # Strategy effectiveness analysis\n",
    "            if 'one_shot_llm' in strategy_performance and 'manager_coordination' in strategy_performance:\n",
    "                one_shot_acc = strategy_performance['one_shot_llm'].get('accuracy', 0)\n",
    "                manager_acc = strategy_performance['manager_coordination'].get('accuracy', 0)\n",
    "                one_shot_time = strategy_performance['one_shot_llm'].get('avg_execution_time', 0)\n",
    "                manager_time = strategy_performance['manager_coordination'].get('avg_execution_time', 0)\n",
    "                \n",
    "                print(f\"\\nâš–ï¸ Strategy Comparison:\")\n",
    "                print(f\"â”œâ”€â”€ One-shot: {one_shot_acc:.1%} accuracy, {one_shot_time:.1f}s avg\")\n",
    "                print(f\"â”œâ”€â”€ Manager: {manager_acc:.1%} accuracy, {manager_time:.1f}s avg\")\n",
    "                \n",
    "                if one_shot_acc > manager_acc:\n",
    "                    print(\"â””â”€â”€ ğŸ’¡ One-shot performing better - consider simpler routing\")\n",
    "                elif manager_acc > one_shot_acc + 0.1:\n",
    "                    print(\"â””â”€â”€ ğŸ’¡ Manager significantly better - routing working well\")\n",
    "                else:\n",
    "                    print(\"â””â”€â”€ ğŸ’¡ Similar performance - routing providing good balance\")\n",
    "        else:\n",
    "            print(f\"âŒ Routing test failed for {config}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Routing test error for {config}: {e}\")\n",
    "\n",
    "# Compare routing across configurations\n",
    "if len(routing_analysis_results) > 1:\n",
    "    print(f\"\\nğŸ“Š ROUTING COMPARISON ACROSS CONFIGURATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    routing_comparison_data = []\n",
    "    for config, results in routing_analysis_results.items():\n",
    "        routing_stats = results['routing_stats']\n",
    "        one_shot_acc = results['strategy_stats'].get('one_shot_llm', {}).get('accuracy', 0)\n",
    "        manager_acc = results['strategy_stats'].get('manager_coordination', {}).get('accuracy', 0)\n",
    "        \n",
    "        routing_comparison_data.append({\n",
    "            'Configuration': config,\n",
    "            'Overall Accuracy': f\"{results['overall_accuracy']:.1%}\",\n",
    "            'Routing Accuracy': f\"{routing_stats.get('routing_accuracy', 0):.1%}\",\n",
    "            'One-shot Accuracy': f\"{one_shot_acc:.1%}\",\n",
    "            'Manager Accuracy': f\"{manager_acc:.1%}\",\n",
    "            'One-shot Questions': routing_stats.get('one_shot_questions', 0),\n",
    "            'Manager Questions': routing_stats.get('manager_questions', 0)\n",
    "        })\n",
    "    \n",
    "    routing_comparison_df = pd.DataFrame(routing_comparison_data)\n",
    "    print(routing_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Provider-Specific Testing (Ollama, OpenRouter, Groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specific providers independently as requested\n",
    "\n",
    "print(\"ğŸ”Œ Provider-Specific Performance Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define provider-specific test configurations\n",
    "provider_configs = {\n",
    "    'groq_standard': {\n",
    "        'name': 'Groq (QwQ-32B)',\n",
    "        'config': 'groq',\n",
    "        'description': 'Standard Groq configuration with QwQ-32B model'\n",
    "    },\n",
    "    'groq_fast': {\n",
    "        'name': 'Groq (Llama-3.3-70B)', \n",
    "        'config': 'groq_fast',\n",
    "        'description': 'Faster Groq model for speed comparison'\n",
    "    },\n",
    "    'openrouter_free': {\n",
    "        'name': 'OpenRouter (Free)',\n",
    "        'config': 'openrouter',\n",
    "        'description': 'OpenRouter free tier model'\n",
    "    },\n",
    "    'openrouter_premium': {\n",
    "        'name': 'OpenRouter (Premium)',\n",
    "        'config': 'openrouter_premium', \n",
    "        'description': 'OpenRouter premium model for accuracy'\n",
    "    },\n",
    "    'ollama_local': {\n",
    "        'name': 'Ollama (Local)',\n",
    "        'config': 'ollama',\n",
    "        'description': 'Local Ollama deployment'\n",
    "    }\n",
    "}\n",
    "\n",
    "provider_test_results = {}\n",
    "\n",
    "# Standard test parameters for all providers\n",
    "test_params = {\n",
    "    'max_questions': 15,\n",
    "    'target_levels': [1, 2],\n",
    "    'include_files': True,\n",
    "    'include_images': True\n",
    "}\n",
    "\n",
    "for provider_key, provider_info in provider_configs.items():\n",
    "    config_name = provider_info['config']\n",
    "    provider_name = provider_info['name']\n",
    "    description = provider_info['description']\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Testing {provider_name}\")\n",
    "    print(f\"ğŸ“ {description}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Test if provider is available first\n",
    "        from agent_interface import get_agent_config_by_name\n",
    "        test_config = get_agent_config_by_name(config_name)\n",
    "        \n",
    "        result = run_gaia_test(\n",
    "            agent_config_name=config_name,\n",
    "            **test_params\n",
    "        )\n",
    "        \n",
    "        if result and 'evaluation_metadata' in result:\n",
    "            metadata = result['evaluation_metadata']\n",
    "            analysis = result.get('analysis', {})\n",
    "            \n",
    "            # Extract comprehensive metrics\n",
    "            provider_test_results[provider_key] = {\n",
    "                'name': provider_name,\n",
    "                'config': config_name,\n",
    "                'total_questions': metadata.get('total_questions', 0),\n",
    "                'correct_answers': metadata.get('correct_answers', 0),\n",
    "                'accuracy': metadata.get('overall_accuracy', 0),\n",
    "                'strategy_performance': analysis.get('strategy_performance', {}),\n",
    "                'level_performance': analysis.get('level_performance', {}),\n",
    "                'execution_time': 0,  # Will calculate from strategy data\n",
    "                'error_rate': 1 - analysis.get('error_analysis', {}).get('execution_success_rate', 1),\n",
    "                'full_result': result\n",
    "            }\n",
    "            \n",
    "            # Calculate average execution time\n",
    "            strategy_perf = analysis.get('strategy_performance', {})\n",
    "            if strategy_perf:\n",
    "                total_time = sum(stats.get('avg_execution_time', 0) * stats.get('total_questions', 0) \n",
    "                               for stats in strategy_perf.values())\n",
    "                total_questions = sum(stats.get('total_questions', 0) for stats in strategy_perf.values())\n",
    "                avg_time = total_time / total_questions if total_questions > 0 else 0\n",
    "                provider_test_results[provider_key]['execution_time'] = avg_time\n",
    "            \n",
    "            # Print immediate results\n",
    "            accuracy = metadata.get('overall_accuracy', 0)\n",
    "            total = metadata.get('total_questions', 0)\n",
    "            correct = metadata.get('correct_answers', 0)\n",
    "            \n",
    "            print(f\"âœ… {provider_name}: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "            print(f\"â±ï¸ Avg execution time: {provider_test_results[provider_key]['execution_time']:.1f}s\")\n",
    "            \n",
    "            # Provider-specific insights\n",
    "            if 'groq' in provider_key:\n",
    "                print(f\"ğŸš€ Groq performance: {'Excellent' if accuracy >= 0.5 else 'Good' if accuracy >= 0.4 else 'Needs improvement'}\")\n",
    "            elif 'openrouter' in provider_key:\n",
    "                if 'free' in provider_key:\n",
    "                    print(f\"ğŸ’° Free tier performance: {'Good value' if accuracy >= 0.4 else 'Consider premium'}\")\n",
    "                else:\n",
    "                    print(f\"ğŸ’ Premium performance: {'Worth the cost' if accuracy >= 0.5 else 'Evaluate cost/benefit'}\")\n",
    "            elif 'ollama' in provider_key:\n",
    "                print(f\"ğŸ  Local deployment: {'Viable alternative' if accuracy >= 0.4 else 'Cloud providers recommended'}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"âŒ Test failed for {provider_name}\")\n",
    "            provider_test_results[provider_key] = {\n",
    "                'name': provider_name,\n",
    "                'config': config_name,\n",
    "                'error': 'Test execution failed'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Provider {provider_name} unavailable: {e}\")\n",
    "        provider_test_results[provider_key] = {\n",
    "            'name': provider_name,\n",
    "            'config': config_name,\n",
    "            'error': f'Provider unavailable: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Provider comparison table\n",
    "print(f\"\\nğŸ“Š PROVIDER PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "provider_comparison_data = []\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result:\n",
    "        provider_comparison_data.append({\n",
    "            'Provider': result['name'],\n",
    "            'Questions': result['total_questions'],\n",
    "            'Correct': result['correct_answers'],\n",
    "            'Accuracy': f\"{result['accuracy']:.1%}\",\n",
    "            'Avg Time (s)': f\"{result['execution_time']:.1f}\",\n",
    "            'Error Rate': f\"{result['error_rate']:.1%}\",\n",
    "            'GAIA Target': \"âœ…\" if result['accuracy'] >= 0.45 else \"âŒ\"\n",
    "        })\n",
    "    else:\n",
    "        provider_comparison_data.append({\n",
    "            'Provider': result['name'],\n",
    "            'Questions': 'N/A',\n",
    "            'Correct': 'N/A', \n",
    "            'Accuracy': 'ERROR',\n",
    "            'Avg Time (s)': 'N/A',\n",
    "            'Error Rate': 'N/A',\n",
    "            'GAIA Target': \"âŒ\"\n",
    "        })\n",
    "\n",
    "provider_comparison_df = pd.DataFrame(provider_comparison_data)\n",
    "print(provider_comparison_df.to_string(index=False))\n",
    "\n",
    "# Provider recommendations\n",
    "successful_providers = [r for r in provider_test_results.values() if 'error' not in r and r.get('accuracy', 0) > 0]\n",
    "\n",
    "if successful_providers:\n",
    "    # Best accuracy\n",
    "    best_accuracy_provider = max(successful_providers, key=lambda x: x['accuracy'])\n",
    "    # Fastest provider\n",
    "    fastest_provider = min(successful_providers, key=lambda x: x['execution_time'])\n",
    "    # Most reliable (lowest error rate)\n",
    "    most_reliable_provider = min(successful_providers, key=lambda x: x['error_rate'])\n",
    "    \n",
    "    print(f\"\\nğŸ† PROVIDER RECOMMENDATIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"ğŸ¯ Best Accuracy: {best_accuracy_provider['name']} ({best_accuracy_provider['accuracy']:.1%})\")\n",
    "    print(f\"âš¡ Fastest: {fastest_provider['name']} ({fastest_provider['execution_time']:.1f}s avg)\")\n",
    "    print(f\"ğŸ›¡ï¸ Most Reliable: {most_reliable_provider['name']} ({most_reliable_provider['error_rate']:.1%} error rate)\")\n",
    "    \n",
    "    # Cost considerations\n",
    "    print(f\"\\nğŸ’° Cost Considerations:\")\n",
    "    groq_providers = [p for p in successful_providers if 'groq' in p['config'].lower()]\n",
    "    openrouter_providers = [p for p in successful_providers if 'openrouter' in p['config'].lower()]\n",
    "    ollama_providers = [p for p in successful_providers if 'ollama' in p['config'].lower()]\n",
    "    \n",
    "    if groq_providers:\n",
    "        avg_groq_acc = sum(p['accuracy'] for p in groq_providers) / len(groq_providers)\n",
    "        print(f\"â”œâ”€â”€ Groq: High performance, reasonable cost ({avg_groq_acc:.1%} avg accuracy)\")\n",
    "    \n",
    "    if openrouter_providers:\n",
    "        free_or = [p for p in openrouter_providers if 'free' in p['name'].lower()]\n",
    "        premium_or = [p for p in openrouter_providers if 'premium' in p['name'].lower()]\n",
    "        \n",
    "        if free_or:\n",
    "            print(f\"â”œâ”€â”€ OpenRouter Free: Budget option ({free_or[0]['accuracy']:.1%} accuracy)\")\n",
    "        if premium_or:\n",
    "            print(f\"â”œâ”€â”€ OpenRouter Premium: Premium option ({premium_or[0]['accuracy']:.1%} accuracy)\")\n",
    "    \n",
    "    if ollama_providers:\n",
    "        print(f\"â””â”€â”€ Ollama: Zero cost, local control ({ollama_providers[0]['accuracy']:.1%} accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Interactive Visualization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One example of interactive visualization, then direct to source data\n",
    "\n",
    "print(\"ğŸ“Š Interactive Performance Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create visualization if we have provider comparison data\n",
    "if provider_comparison_data and len(provider_comparison_data) > 1:\n",
    "    # Extract accuracy data for visualization\n",
    "    viz_data = []\n",
    "    for item in provider_comparison_data:\n",
    "        if item['Accuracy'] != 'ERROR':\n",
    "            accuracy_val = float(item['Accuracy'].strip('%')) / 100\n",
    "            time_val = float(item['Avg Time (s)']) if item['Avg Time (s)'] != 'N/A' else 0\n",
    "            \n",
    "            viz_data.append({\n",
    "                'Provider': item['Provider'],\n",
    "                'Accuracy': accuracy_val,\n",
    "                'Avg_Time': time_val,\n",
    "                'Questions': int(item['Questions']) if item['Questions'] != 'N/A' else 0\n",
    "            })\n",
    "    \n",
    "    if viz_data:\n",
    "        viz_df = pd.DataFrame(viz_data)\n",
    "        \n",
    "        # Create a simple performance visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        providers = viz_df['Provider']\n",
    "        accuracies = viz_df['Accuracy']\n",
    "        colors = ['green' if acc >= 0.45 else 'orange' if acc >= 0.35 else 'red' for acc in accuracies]\n",
    "        \n",
    "        bars1 = ax1.bar(providers, accuracies, color=colors, alpha=0.7)\n",
    "        ax1.axhline(y=0.45, color='red', linestyle='--', label='GAIA Target (45%)')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Provider Accuracy Comparison')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.1%}', ha='center', va='bottom')\n",
    "        \n",
    "        # Rotate x-axis labels for readability\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Speed vs Accuracy scatter plot\n",
    "        ax2.scatter(viz_df['Avg_Time'], viz_df['Accuracy'], \n",
    "                   s=viz_df['Questions']*10, alpha=0.6, c=colors)\n",
    "        \n",
    "        # Add provider labels\n",
    "        for idx, row in viz_df.iterrows():\n",
    "            ax2.annotate(row['Provider'], \n",
    "                        (row['Avg_Time'], row['Accuracy']),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8)\n",
    "        \n",
    "        ax2.axhline(y=0.45, color='red', linestyle='--', label='GAIA Target')\n",
    "        ax2.set_xlabel('Average Execution Time (seconds)')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Speed vs Accuracy Trade-off')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"ğŸ“ˆ Visualization shows accuracy and speed trade-offs\")\n",
    "        print(\"ğŸ’¡ Larger dots = more questions tested\")\n",
    "        print(\"ğŸ¯ Red line = GAIA performance target (45%)\")\n",
    "        \n",
    "        # Analysis of the visualization\n",
    "        best_overall = viz_df.loc[viz_df['Accuracy'].idxmax()]\n",
    "        fastest = viz_df.loc[viz_df['Avg_Time'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Visual Analysis:\")\n",
    "        print(f\"â”œâ”€â”€ Highest accuracy: {best_overall['Provider']} ({best_overall['Accuracy']:.1%})\")\n",
    "        print(f\"â”œâ”€â”€ Fastest execution: {fastest['Provider']} ({fastest['Avg_Time']:.1f}s)\")\n",
    "        \n",
    "        # Efficiency score (accuracy / time)\n",
    "        viz_df['Efficiency'] = viz_df['Accuracy'] / (viz_df['Avg_Time'] + 1)  # +1 to avoid division by zero\n",
    "        most_efficient = viz_df.loc[viz_df['Efficiency'].idxmax()]\n",
    "        print(f\"â””â”€â”€ Most efficient: {most_efficient['Provider']} (best accuracy/time ratio)\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Direct Data Access:\")\n",
    "print(\"For detailed analysis, access the source data:\")\n",
    "print(\"â”œâ”€â”€ baseline_results: Performance across question types\")\n",
    "print(\"â”œâ”€â”€ routing_analysis_results: Routing effectiveness data\") \n",
    "print(\"â”œâ”€â”€ provider_test_results: Provider-specific performance\")\n",
    "print(\"â””â”€â”€ All results include full evaluation metadata and analysis\")\n",
    "\n",
    "# Show how to access specific data\n",
    "print(f\"\\nğŸ” Example Data Access:\")\n",
    "print(\"# Access baseline results\")\n",
    "print(\"baseline_results['level_1_only']['accuracy']\")\n",
    "print(\"\\n# Access provider comparison\")\n",
    "print(\"provider_test_results['groq_standard']['strategy_performance']\")\n",
    "print(\"\\n# Access detailed analysis\")\n",
    "print(\"provider_test_results['groq_standard']['full_result']['analysis']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Failure Pattern Analysis & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep analysis of failure patterns for improvement insights\n",
    "\n",
    "print(\"ğŸ” Comprehensive Failure Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze failures across all test results\n",
    "all_test_results = []\n",
    "\n",
    "# Collect results from baseline tests\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'full_result' in result:\n",
    "        all_test_results.append({\n",
    "            'source': f'baseline_{config_key}',\n",
    "            'result': result['full_result']\n",
    "        })\n",
    "\n",
    "# Collect results from provider tests  \n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'full_result' in result:\n",
    "        all_test_results.append({\n",
    "            'source': f'provider_{provider_key}',\n",
    "            'result': result['full_result']\n",
    "        })\n",
    "\n",
    "failure_analyses = {}\n",
    "\n",
    "for test_data in all_test_results:\n",
    "    source = test_data['source']\n",
    "    result = test_data['result']\n",
    "    \n",
    "    print(f\"\\nğŸ” Analyzing failures in {source}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        failure_analysis = analyze_failure_patterns(result)\n",
    "        \n",
    "        if failure_analysis and 'failure_patterns' in failure_analysis:\n",
    "            failure_analyses[source] = failure_analysis\n",
    "            \n",
    "            patterns = failure_analysis['failure_patterns']\n",
    "            recommendations = failure_analysis.get('recommendations', [])\n",
    "            \n",
    "            # Print key failure insights\n",
    "            print(f\"ğŸ“Š Failure Distribution:\")\n",
    "            \n",
    "            # By level\n",
    "            level_failures = patterns.get('by_level', {})\n",
    "            total_failures = sum(level_failures.values())\n",
    "            if total_failures > 0:\n",
    "                print(f\"â”œâ”€â”€ By Level:\")\n",
    "                for level, count in level_failures.items():\n",
    "                    percentage = count / total_failures * 100\n",
    "                    print(f\"â”‚   â”œâ”€â”€ Level {level}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # By strategy\n",
    "            strategy_failures = patterns.get('by_strategy', {})\n",
    "            if strategy_failures:\n",
    "                print(f\"â”œâ”€â”€ By Strategy:\")\n",
    "                for strategy, count in strategy_failures.items():\n",
    "                    percentage = count / total_failures * 100\n",
    "                    print(f\"â”‚   â”œâ”€â”€ {strategy}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Execution issues\n",
    "            exec_failures = patterns.get('execution_failures', 0)\n",
    "            if exec_failures > 0:\n",
    "                print(f\"â””â”€â”€ Execution Failures: {exec_failures}\")\n",
    "            \n",
    "            # Top recommendations\n",
    "            if recommendations:\n",
    "                print(f\"\\nğŸ’¡ Top Recommendations for {source}:\")\n",
    "                for i, rec in enumerate(recommendations[:3], 1):\n",
    "                    print(f\"  {i}. {rec}\")\n",
    "        else:\n",
    "            print(f\"âœ… No significant failure patterns found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failure analysis error: {e}\")\n",
    "\n",
    "# Cross-test pattern analysis\n",
    "if len(failure_analyses) > 1:\n",
    "    print(f\"\\nğŸ”„ Cross-Test Pattern Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find common failure patterns across tests\n",
    "    all_level_failures = {}\n",
    "    all_strategy_failures = {}\n",
    "    all_execution_failures = 0\n",
    "    \n",
    "    for source, analysis in failure_analyses.items():\n",
    "        patterns = analysis['failure_patterns']\n",
    "        \n",
    "        # Aggregate level failures\n",
    "        for level, count in patterns.get('by_level', {}).items():\n",
    "            all_level_failures[level] = all_level_failures.get(level, 0) + count\n",
    "        \n",
    "        # Aggregate strategy failures\n",
    "        for strategy, count in patterns.get('by_strategy', {}).items():\n",
    "            all_strategy_failures[strategy] = all_strategy_failures.get(strategy, 0) + count\n",
    "        \n",
    "        # Aggregate execution failures\n",
    "        all_execution_failures += patterns.get('execution_failures', 0)\n",
    "    \n",
    "    print(f\"ğŸ“Š Aggregated Failure Patterns:\")\n",
    "    \n",
    "    if all_level_failures:\n",
    "        total_level_failures = sum(all_level_failures.values())\n",
    "        print(f\"â”œâ”€â”€ Most problematic levels:\")\n",
    "        sorted_levels = sorted(all_level_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for level, count in sorted_levels:\n",
    "            percentage = count / total_level_failures * 100\n",
    "            print(f\"â”‚   â”œâ”€â”€ Level {level}: {count} failures ({percentage:.1f}%)\")\n",
    "    \n",
    "    if all_strategy_failures:\n",
    "        total_strategy_failures = sum(all_strategy_failures.values())\n",
    "        print(f\"â”œâ”€â”€ Most problematic strategies:\")\n",
    "        sorted_strategies = sorted(all_strategy_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for strategy, count in sorted_strategies:\n",
    "            percentage = count / total_strategy_failures * 100\n",
    "            print(f\"â”‚   â”œâ”€â”€ {strategy}: {count} failures ({percentage:.1f}%)\")\n",
    "    \n",
    "    if all_execution_failures > 0:\n",
    "        print(f\"â””â”€â”€ Total execution failures: {all_execution_failures}\")\n",
    "    \n",
    "    # Global recommendations\n",
    "    print(f\"\\nğŸ¯ Global Optimization Priorities:\")\n",
    "    \n",
    "    if all_level_failures:\n",
    "        worst_level = max(all_level_failures, key=all_level_failures.get)\n",
    "        print(f\"1. Focus on Level {worst_level} performance improvement\")\n",
    "    \n",
    "    if all_strategy_failures:\n",
    "        worst_strategy = max(all_strategy_failures, key=all_strategy_failures.get)\n",
    "        print(f\"2. Optimize {worst_strategy.replace('_', ' ')} strategy\")\n",
    "    \n",
    "    if all_execution_failures > 5:\n",
    "        print(f\"3. Improve system reliability (reduce execution failures)\")\n",
    "    \n",
    "    print(f\"4. Consider routing adjustments based on failure patterns\")\n",
    "    print(f\"5. Enhance answer formatting and validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Production Readiness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive assessment for production deployment\n",
    "\n",
    "print(\"ğŸ­ Production Readiness Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all performance data for assessment\n",
    "assessment_data = {\n",
    "    'baseline_performance': {},\n",
    "    'provider_performance': {},\n",
    "    'routing_effectiveness': {},\n",
    "    'system_reliability': {},\n",
    "    'file_processing': {},\n",
    "    'overall_metrics': {}\n",
    "}\n",
    "\n",
    "# Extract baseline performance metrics\n",
    "successful_baselines = [r for r in baseline_results.values() if 'error' not in r]\n",
    "if successful_baselines:\n",
    "    baseline_accuracies = [r['accuracy'] for r in successful_baselines]\n",
    "    assessment_data['baseline_performance'] = {\n",
    "        'avg_accuracy': np.mean(baseline_accuracies),\n",
    "        'min_accuracy': np.min(baseline_accuracies),\n",
    "        'max_accuracy': np.max(baseline_accuracies),\n",
    "        'std_accuracy': np.std(baseline_accuracies),\n",
    "        'gaia_target_met': any(acc >= 0.45 for acc in baseline_accuracies)\n",
    "    }\n",
    "\n",
    "# Extract provider performance metrics  \n",
    "successful_providers = [r for r in provider_test_results.values() if 'error' not in r]\n",
    "if successful_providers:\n",
    "    provider_accuracies = [r['accuracy'] for r in successful_providers]\n",
    "    provider_times = [r['execution_time'] for r in successful_providers]\n",
    "    provider_errors = [r['error_rate'] for r in successful_providers]\n",
    "    \n",
    "    assessment_data['provider_performance'] = {\n",
    "        'available_providers': len(successful_providers),\n",
    "        'avg_accuracy': np.mean(provider_accuracies),\n",
    "        'best_accuracy': np.max(provider_accuracies),\n",
    "        'avg_execution_time': np.mean(provider_times),\n",
    "        'fastest_time': np.min(provider_times),\n",
    "        'avg_error_rate': np.mean(provider_errors),\n",
    "        'best_reliability': np.min(provider_errors)\n",
    "    }\n",
    "\n",
    "# Extract routing effectiveness metrics\n",
    "if routing_analysis_results:\n",
    "    routing_accuracies = [r['routing_stats'].get('routing_accuracy', 0) for r in routing_analysis_results.values()]\n",
    "    assessment_data['routing_effectiveness'] = {\n",
    "        'avg_routing_accuracy': np.mean(routing_accuracies),\n",
    "        'min_routing_accuracy': np.min(routing_accuracies),\n",
    "        'routing_working': np.mean(routing_accuracies) >= 0.7\n",
    "    }\n",
    "\n",
    "# System reliability assessment\n",
    "total_execution_failures = 0\n",
    "total_questions = 0\n",
    "\n",
    "for result_set in [baseline_results.values(), provider_test_results.values()]:\n",
    "    for result in result_set:\n",
    "        if 'full_result' in result:\n",
    "            error_analysis = result['full_result'].get('analysis', {}).get('error_analysis', {})\n",
    "            exec_errors = error_analysis.get('execution_errors', 0)\n",
    "            total_exec_questions = result.get('total_questions', 0)\n",
    "            total_execution_failures += exec_errors\n",
    "            total_questions += total_exec_questions\n",
    "\n",
    "system_reliability = 1 - (total_execution_failures / total_questions) if total_questions > 0 else 0\n",
    "assessment_data['system_reliability'] = {\n",
    "    'execution_success_rate': system_reliability,\n",
    "    'total_questions_tested': total_questions,\n",
    "    'total_failures': total_execution_failures,\n",
    "    'reliable': system_reliability >= 0.95\n",
    "}\n",
    "\n",
    "# Overall metrics calculation\n",
    "all_accuracies = []\n",
    "if successful_baselines:\n",
    "    all_accuracies.extend([r['accuracy'] for r in successful_baselines])\n",
    "if successful_providers:\n",
    "    all_accuracies.extend([r['accuracy'] for r in successful_providers])\n",
    "\n",
    "if all_accuracies:\n",
    "    assessment_data['overall_metrics'] = {\n",
    "        'best_accuracy': np.max(all_accuracies),\n",
    "        'avg_accuracy': np.mean(all_accuracies),\n",
    "        'consistency': 1 - np.std(all_accuracies),  # Higher consistency = lower std\n",
    "        'gaia_target_achievement': np.max(all_accuracies) >= 0.45,\n",
    "        'production_ready_accuracy': np.max(all_accuracies) >= 0.50\n",
    "    }\n",
    "\n",
    "# Production readiness scoring\n",
    "print(f\"ğŸ“Š PRODUCTION READINESS SCORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "readiness_score = 0\n",
    "max_score = 100\n",
    "\n",
    "# Accuracy score (40 points)\n",
    "if assessment_data['overall_metrics']:\n",
    "    best_acc = assessment_data['overall_metrics']['best_accuracy']\n",
    "    if best_acc >= 0.60:\n",
    "        accuracy_score = 40\n",
    "    elif best_acc >= 0.50:\n",
    "        accuracy_score = 35\n",
    "    elif best_acc >= 0.45:\n",
    "        accuracy_score = 30\n",
    "    elif best_acc >= 0.35:\n",
    "        accuracy_score = 20\n",
    "    else:\n",
    "        accuracy_score = 10\n",
    "    \n",
    "    readiness_score += accuracy_score\n",
    "    print(f\"âœ… Accuracy Score: {accuracy_score}/40 (Best: {best_acc:.1%})\")\n",
    "\n",
    "# Reliability score (25 points)\n",
    "if assessment_data['system_reliability']:\n",
    "    reliability = assessment_data['system_reliability']['execution_success_rate']\n",
    "    if reliability >= 0.98:\n",
    "        reliability_score = 25\n",
    "    elif reliability >= 0.95:\n",
    "        reliability_score = 20\n",
    "    elif reliability >= 0.90:\n",
    "        reliability_score = 15\n",
    "    else:\n",
    "        reliability_score = 10\n",
    "    \n",
    "    readiness_score += reliability_score\n",
    "    print(f\"âœ… Reliability Score: {reliability_score}/25 (Success Rate: {reliability:.1%})\")\n",
    "\n",
    "# Provider availability score (15 points)\n",
    "if assessment_data['provider_performance']:\n",
    "    available_providers = assessment_data['provider_performance']['available_providers']\n",
    "    if available_providers >= 4:\n",
    "        provider_score = 15\n",
    "    elif available_providers >= 3:\n",
    "        provider_score = 12\n",
    "    elif available_providers >= 2:\n",
    "        provider_score = 8\n",
    "    else:\n",
    "        provider_score = 5\n",
    "    \n",
    "    readiness_score += provider_score\n",
    "    print(f\"âœ… Provider Score: {provider_score}/15 ({available_providers} providers available)\")\n",
    "\n",
    "# Routing effectiveness score (10 points)\n",
    "if assessment_data['routing_effectiveness']:\n",
    "    routing_acc = assessment_data['routing_effectiveness']['avg_routing_accuracy']\n",
    "    if routing_acc >= 0.80:\n",
    "        routing_score = 10\n",
    "    elif routing_acc >= 0.70:\n",
    "        routing_score = 8\n",
    "    elif routing_acc >= 0.60:\n",
    "        routing_score = 6\n",
    "    else:\n",
    "        routing_score = 3\n",
    "    \n",
    "    readiness_score += routing_score\n",
    "    print(f\"âœ… Routing Score: {routing_score}/10 (Accuracy: {routing_acc:.1%})\")\n",
    "\n",
    "# Consistency score (10 points)\n",
    "if assessment_data['overall_metrics']:\n",
    "    consistency = assessment_data['overall_metrics']['consistency']\n",
    "    if consistency >= 0.90:\n",
    "        consistency_score = 10\n",
    "    elif consistency >= 0.80:\n",
    "        consistency_score = 8\n",
    "    elif consistency >= 0.70:\n",
    "        consistency_score = 6\n",
    "    else:\n",
    "        consistency_score = 3\n",
    "    \n",
    "    readiness_score += consistency_score\n",
    "    print(f\"âœ… Consistency Score: {consistency_score}/10 (Consistency: {consistency:.1%})\")\n",
    "\n",
    "print(f\"\\nğŸ† OVERALL READINESS SCORE: {readiness_score}/{max_score} ({readiness_score/max_score*100:.1f}%)\")\n",
    "\n",
    "# Production deployment recommendation\n",
    "print(f\"\\nğŸš€ PRODUCTION DEPLOYMENT RECOMMENDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if readiness_score >= 85:\n",
    "    recommendation = \"ğŸŸ¢ READY FOR PRODUCTION\"\n",
    "    details = \"System demonstrates excellent performance, reliability, and consistency.\"\n",
    "    next_steps = [\n",
    "        \"Deploy to production environment\",\n",
    "        \"Set up monitoring and alerting\",\n",
    "        \"Implement gradual rollout strategy\",\n",
    "        \"Document operational procedures\"\n",
    "    ]\n",
    "elif readiness_score >= 70:\n",
    "    recommendation = \"ğŸŸ¡ READY WITH MINOR OPTIMIZATIONS\"\n",
    "    details = \"System shows good performance but could benefit from targeted improvements.\"\n",
    "    next_steps = [\n",
    "        \"Address identified failure patterns\",\n",
    "        \"Optimize underperforming configurations\",\n",
    "        \"Enhance error handling\",\n",
    "        \"Conduct limited production trial\"\n",
    "    ]\n",
    "elif readiness_score >= 55:\n",
    "    recommendation = \"ğŸŸ  NEEDS IMPROVEMENT BEFORE PRODUCTION\"\n",
    "    details = \"System shows promise but requires significant improvements.\"\n",
    "    next_steps = [\n",
    "        \"Focus on accuracy improvements\",\n",
    "        \"Enhance system reliability\",\n",
    "        \"Optimize routing decisions\",\n",
    "        \"Conduct additional testing cycles\"\n",
    "    ]\n",
    "else:\n",
    "    recommendation = \"ğŸ”´ NOT READY FOR PRODUCTION\"\n",
    "    details = \"System requires substantial development before production deployment.\"\n",
    "    next_steps = [\n",
    "        \"Review core architecture\",\n",
    "        \"Improve model configurations\",\n",
    "        \"Enhance error handling and reliability\",\n",
    "        \"Return to development phase\"\n",
    "    ]\n",
    "\n",
    "print(f\"{recommendation}\")\n",
    "print(f\"ğŸ“ {details}\")\n",
    "print(f\"\\nğŸ“‹ Recommended Next Steps:\")\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "# Specific recommendations based on assessment data\n",
    "print(f\"\\nğŸ¯ SPECIFIC OPTIMIZATION RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Accuracy-based recommendations\n",
    "if assessment_data['overall_metrics']:\n",
    "    best_acc = assessment_data['overall_metrics']['best_accuracy']\n",
    "    if best_acc < 0.45:\n",
    "        recommendations.append(\"ğŸ¯ Priority: Achieve GAIA benchmark target (45% accuracy)\")\n",
    "    elif best_acc < 0.55:\n",
    "        recommendations.append(\"ğŸ“ˆ Focus: Improve accuracy to competitive levels (55%+)\")\n",
    "\n",
    "# Provider-based recommendations\n",
    "if assessment_data['provider_performance']:\n",
    "    best_provider_acc = assessment_data['provider_performance']['best_accuracy']\n",
    "    avg_provider_acc = assessment_data['provider_performance']['avg_accuracy']\n",
    "    \n",
    "    if best_provider_acc - avg_provider_acc > 0.1:\n",
    "        recommendations.append(\"âš–ï¸ Standardize: Large performance gap between providers - optimize configurations\")\n",
    "    \n",
    "    fastest_time = assessment_data['provider_performance']['fastest_time']\n",
    "    avg_time = assessment_data['provider_performance']['avg_execution_time']\n",
    "    \n",
    "    if avg_time > 30:\n",
    "        recommendations.append(\"âš¡ Speed: Reduce average execution time below 30 seconds\")\n",
    "\n",
    "# Reliability-based recommendations\n",
    "if assessment_data['system_reliability']:\n",
    "    reliability = assessment_data['system_reliability']['execution_success_rate']\n",
    "    if reliability < 0.95:\n",
    "        recommendations.append(\"ğŸ›¡ï¸ Reliability: Improve execution success rate above 95%\")\n",
    "\n",
    "# Routing-based recommendations\n",
    "if assessment_data['routing_effectiveness']:\n",
    "    routing_acc = assessment_data['routing_effectiveness']['avg_routing_accuracy']\n",
    "    if routing_acc < 0.75:\n",
    "        recommendations.append(\"ğŸ”€ Routing: Improve complexity detection and routing accuracy\")\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(f\"  â€¢ {rec}\")\n",
    "else:\n",
    "    print(\"  âœ… No critical optimizations needed - system performing well\")\n",
    "\n",
    "# Generate final comprehensive report\n",
    "print(f\"\\nğŸ“„ Generating Comprehensive Production Report...\")\n",
    "\n",
    "# Use the best performing configuration for final report\n",
    "best_config = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result and result.get('accuracy', 0) > best_accuracy:\n",
    "        best_accuracy = result['accuracy']\n",
    "        best_config = result['config']\n",
    "\n",
    "if best_config:\n",
    "    try:\n",
    "        # Run one final comprehensive test with best configuration\n",
    "        final_test = run_gaia_test(\n",
    "            agent_config_name=best_config,\n",
    "            max_questions=30,  # Larger sample for final assessment\n",
    "            target_levels=[1, 2, 3],\n",
    "            include_files=True,\n",
    "            include_images=True\n",
    "        )\n",
    "        \n",
    "        if final_test:\n",
    "            report = generate_test_report(final_test, best_config, save_to_file=True)\n",
    "            print(f\"âœ… Final report generated and saved\")\n",
    "            print(f\"ğŸ“Š Final test: {final_test['evaluation_metadata']['overall_accuracy']:.1%} accuracy\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Final test failed - using existing data for assessment\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Final test error: {e} - using existing data for assessment\")\n",
    "\n",
    "print(f\"\\nğŸ‰ PRODUCTION VALIDATION COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Total Questions Tested: {assessment_data['system_reliability']['total_questions_tested']}\")\n",
    "print(f\"ğŸ† Best Accuracy Achieved: {assessment_data['overall_metrics']['best_accuracy']:.1%}\")\n",
    "print(f\"ğŸ›¡ï¸ System Reliability: {assessment_data['system_reliability']['execution_success_rate']:.1%}\")\n",
    "print(f\"âš¡ Available Providers: {assessment_data['provider_performance']['available_providers']}\")\n",
    "print(f\"ğŸ”€ Routing Effectiveness: {assessment_data['routing_effectiveness']['avg_routing_accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ All test data available in variables:\")\n",
    "print(f\"â”œâ”€â”€ baseline_results: Baseline performance data\")\n",
    "print(f\"â”œâ”€â”€ provider_test_results: Provider-specific results\")\n",
    "print(f\"â”œâ”€â”€ routing_analysis_results: Routing effectiveness analysis\")\n",
    "print(f\"â”œâ”€â”€ failure_analyses: Comprehensive failure pattern analysis\")\n",
    "print(f\"â””â”€â”€ assessment_data: Production readiness metrics\")\n",
    "\n",
    "print(f\"\\nğŸš€ System is now ready for production decision based on comprehensive testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Data Export & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Export all results for further analysis and reporting\n",
    "\n",
    "print(\"ğŸ’¾ Data Export & Final Reporting\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create comprehensive data export\n",
    "export_data = {\n",
    "    'test_session': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_questions_tested': assessment_data['system_reliability']['total_questions_tested'],\n",
    "        'testing_duration': 'Session duration tracked',\n",
    "        'framework_version': '2.0'\n",
    "    },\n",
    "    'baseline_results': baseline_results,\n",
    "    'provider_results': provider_test_results,\n",
    "    'routing_analysis': routing_analysis_results,\n",
    "    'failure_analysis': failure_analyses,\n",
    "    'production_assessment': assessment_data,\n",
    "    'readiness_score': readiness_score,\n",
    "    'recommendation': recommendation\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "export_file = Path(\"logs\") / f\"production_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "try:\n",
    "    with open(export_file, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"âœ… Comprehensive results exported: {export_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Export error: {e}\")\n",
    "\n",
    "# Create summary CSV for quick analysis\n",
    "summary_data = []\n",
    "\n",
    "# Add baseline results\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'error' not in result:\n",
    "        summary_data.append({\n",
    "            'Test_Type': 'Baseline',\n",
    "            'Configuration': config_key,\n",
    "            'Total_Questions': result['total_questions'],\n",
    "            'Correct_Answers': result['correct_answers'],\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'GAIA_Target_Met': result['accuracy'] >= 0.45\n",
    "        })\n",
    "\n",
    "# Add provider results\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result:\n",
    "        summary_data.append({\n",
    "            'Test_Type': 'Provider',\n",
    "            'Configuration': result['config'],\n",
    "            'Total_Questions': result['total_questions'],\n",
    "            'Correct_Answers': result['correct_answers'],\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'GAIA_Target_Met': result['accuracy'] >= 0.45\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv = Path(\"logs\") / f\"validation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    try:\n",
    "        summary_df.to_csv(summary_csv, index=False)\n",
    "        print(f\"âœ… Summary CSV exported: {summary_csv}\")\n",
    "        \n",
    "        # Display final summary table\n",
    "        print(f\"\\nğŸ“Š FINAL SUMMARY TABLE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ CSV export error: {e}\")\n",
    "\n",
    "# Performance insights summary\n",
    "print(f\"\\nğŸ“ˆ KEY PERFORMANCE INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if assessment_data['overall_metrics']:\n",
    "    metrics = assessment_data['overall_metrics']\n",
    "    print(f\"ğŸ¯ Best Performance: {metrics['best_accuracy']:.1%} accuracy\")\n",
    "    print(f\"ğŸ“Š Average Performance: {metrics['avg_accuracy']:.1%} accuracy\")\n",
    "    print(f\"ğŸ† GAIA Target: {'âœ… Achieved' if metrics['gaia_target_achievement'] else 'âŒ Not achieved'}\")\n",
    "    print(f\"ğŸš€ Production Ready: {'âœ… Yes' if metrics['production_ready_accuracy'] else 'âŒ Needs improvement'}\")\n",
    "\n",
    "if assessment_data['provider_performance']:\n",
    "    provider_metrics = assessment_data['provider_performance']\n",
    "    print(f\"âš¡ Fastest Provider: {provider_metrics['fastest_time']:.1f}s average\")\n",
    "    print(f\"ğŸ›¡ï¸ Best Reliability: {(1-provider_metrics['best_reliability']):.1%} success rate\")\n",
    "\n",
    "print(f\"\\nğŸ“ Production Validation Complete!\")\n",
    "print(f\"Use the exported data and reports for production deployment decisions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
