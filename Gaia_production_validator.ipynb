{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIA Validator\n",
    "## Implementation plan, testing framework, and deployment\n",
    "\n",
    "**Objective:** Complete implementation roadmap and evaluation system  \n",
    "**Target:** 45-55% GAIA accuracy within $10 budget\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Status Quo Gaia-benchmark Agent Testing Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Create Small Test Batch (Data Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_logic import GAIAAgent, GAIAConfig\n",
    "\n",
    "# Create config object for Openrouter\n",
    "config = GAIAConfig(\n",
    "    model_provider=\"openrouter\",\n",
    "    model_name=\"qwen/qwen3-30b-a3b\"\n",
    ")\n",
    "agent = GAIAAgent(config)\n",
    "result = agent.process_question(\"What is Mark Rutte doing right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Create Small Test Batch using Pure Data Layer\n",
    "print(\"ðŸ“¦ Creating Small Test Batch (5 questions)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from gaia_dataset_utils import GAIADatasetManager, quick_dataset_check\n",
    "\n",
    "# Step 1: Validate dataset\n",
    "print(\"ðŸ” Step 1: Dataset Validation\")\n",
    "dataset_ready = quick_dataset_check(\"./tests/gaia_data\")\n",
    "\n",
    "if dataset_ready:\n",
    "    # Step 2: Create dataset manager\n",
    "    print(\"\\nðŸ“Š Step 2: Loading Dataset Manager\")\n",
    "    manager = GAIADatasetManager(\"./tests/gaia_data\")\n",
    "    \n",
    "    if manager.metadata:\n",
    "        print(f\"âœ… Dataset loaded: {len(manager.metadata)} total questions\")\n",
    "        print(f\"ðŸ“ Questions with files: {len(manager.file_questions)}\")\n",
    "        \n",
    "        # Step 3: Create small test batch\n",
    "        print(\"\\nðŸ“¦ Step 3: Creating Small Test Batch\")\n",
    "        test_batch = manager.create_test_batch(5, \"small_sample\")\n",
    "        \n",
    "        if test_batch:\n",
    "            print(f\"âœ… Created test batch with {len(test_batch)} questions\")\n",
    "            \n",
    "            # Show batch composition\n",
    "            print(f\"\\nðŸ“‹ Batch Composition:\")\n",
    "            levels = {}\n",
    "            file_count = 0\n",
    "            file_types = set()\n",
    "            \n",
    "            for i, question in enumerate(test_batch, 1):\n",
    "                level = question.get('Level', 'Unknown')\n",
    "                levels[level] = levels.get(level, 0) + 1\n",
    "                \n",
    "                has_file = question['task_id'] in manager.file_questions\n",
    "                if has_file:\n",
    "                    file_count += 1\n",
    "                    file_name = manager.file_questions[question['task_id']].get('file_name', '')\n",
    "                    if file_name:\n",
    "                        ext = file_name.split('.')[-1].lower()\n",
    "                        file_types.add(ext)\n",
    "                \n",
    "                # Show question preview\n",
    "                question_text = question.get('Question', '')\n",
    "                preview = question_text[:60] + \"...\" if len(question_text) > 60 else question_text\n",
    "                file_info = f\" (File: {file_name})\" if has_file else \"\"\n",
    "                \n",
    "                print(f\"  {i}. Level {level}: {preview}{file_info}\")\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Summary:\")\n",
    "            print(f\"  Level distribution: {dict(levels)}\")\n",
    "            print(f\"  Questions with files: {file_count}/{len(test_batch)}\")\n",
    "            if file_types:\n",
    "                print(f\"  File types: {', '.join(sorted(file_types))}\")\n",
    "            \n",
    "            # Verify blind testing - show what agent will see vs hidden\n",
    "            print(f\"\\nðŸ”’ Blind Testing Verification:\")\n",
    "            sample_question = test_batch[0]\n",
    "            \n",
    "            print(f\"  ðŸ¤– Agent will see:\")\n",
    "            visible_fields = ['task_id', 'Question', 'Level', 'file_name', 'file_path']\n",
    "            for field in visible_fields:\n",
    "                if field in sample_question:\n",
    "                    value = sample_question[field]\n",
    "                    if isinstance(value, str) and len(value) > 50:\n",
    "                        value = value[:50] + \"...\"\n",
    "                    print(f\"    {field}: {value}\")\n",
    "            \n",
    "            print(f\"  ðŸ”’ Agent will NOT see:\")\n",
    "            # Check what's hidden by looking at full dataset\n",
    "            full_question = manager.get_question_by_id(sample_question['task_id'])\n",
    "            hidden_info = ['Final answer', 'Annotator Metadata']\n",
    "            for field in hidden_info:\n",
    "                if field in full_question:\n",
    "                    print(f\"    {field}: HIDDEN\")\n",
    "            \n",
    "            print(f\"\\nâœ… Test batch ready for execution!\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Failed to create test batch\")\n",
    "    else:\n",
    "        print(\"âŒ Failed to load dataset\")\n",
    "else:\n",
    "    print(\"âŒ Dataset not ready - check your setup\")\n",
    "    test_batch = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Execute Test Batch with Groc free tier (Testing Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Execute Test Batch using Pure Testing Layer\n",
    "print(\"ðŸ¤– Executing Test Batch with Ollama\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import testing layer components\n",
    "from agent_testing import GAIATestExecutor, GAIATestEvaluator, get_agent_config_by_name\n",
    "import time\n",
    "\n",
    "# Check if we have test batch from previous cell\n",
    "if 'test_batch' not in locals() or not test_batch:\n",
    "    print(\"âŒ No test batch available. Run Cell 1 first.\")\n",
    "else:\n",
    "    print(f\"ðŸ“‹ Test batch loaded: {len(test_batch)} questions\")\n",
    "    \n",
    "    # Step 1: Configure Groq agent (supports tools)\n",
    "    print(f\"\\nðŸ”§ Step 1: Configure Groq Agent\")\n",
    "    \n",
    "    try:\n",
    "        # Get Groq configuration (supports tool calling)\n",
    "        ollama_config = get_agent_config_by_name(\"ollama\")\n",
    "        print(f\"âœ… Ollama config loaded\")\n",
    "        print(f\"   Model: qwen-agent-custom\")\n",
    "        print(f\"   Provider: Ollama\")\n",
    "        print(f\"   Tool Support: âœ… YES\")\n",
    "        \n",
    "        # Create test executor\n",
    "        executor = GAIATestExecutor(\"ollama\")\n",
    "        print(f\"âœ… Test executor created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create agent: {e}\")\n",
    "        executor = None\n",
    "\n",
    "    if executor:\n",
    "        # Step 2: Execute test batch (blind)\n",
    "        print(f\"\\nðŸš€ Step 2: Execute Test Batch (Blind)\")\n",
    "        print(f\"âš ï¸  This will possibly use provider API credits\")\n",
    "        \n",
    "        proceed = input(\"Proceed with execution? (y/n): \")\n",
    "        \n",
    "        if proceed.lower() in ['y', 'yes']:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Execute the batch\n",
    "                execution_results = executor.execute_test_batch(test_batch)\n",
    "                \n",
    "                execution_time = time.time() - start_time\n",
    "                \n",
    "                if execution_results:\n",
    "                    print(f\"\\nâœ… Execution completed!\")\n",
    "                    print(f\"â±ï¸ Total time: {execution_time:.1f}s\")\n",
    "                    \n",
    "                    # Analyze execution results\n",
    "                    successful = len([r for r in execution_results if r.get('execution_successful', False)])\n",
    "                    avg_time = sum(r.get('execution_time', 0) for r in execution_results) / len(execution_results)\n",
    "                    \n",
    "                    print(f\"ðŸ“Š Execution Summary:\")\n",
    "                    print(f\"  Total questions: {len(execution_results)}\")\n",
    "                    print(f\"  Successful question executions: {successful}/{len(execution_results)} ({successful/len(execution_results):.1%})\")\n",
    "                    print(f\"  Average time per question: {avg_time:.2f}s\")\n",
    "                    \n",
    "                    # Show sample results (without revealing correctness yet)\n",
    "                    print(f\"\\nðŸ¤– Sample Agent Responses:\")\n",
    "                    for i, result in enumerate(execution_results[:3], 1):\n",
    "                        answer = result.get('agent_answer', 'No answer')\n",
    "                        strategy = result.get('strategy_used', 'unknown')\n",
    "                        exec_time = result.get('execution_time', 0)\n",
    "                        \n",
    "                        print(f\"  {i}. Answer: '{answer}' (Strategy: {strategy}, Time: {exec_time:.1f}s)\")\n",
    "                    \n",
    "                    if len(execution_results) > 3:\n",
    "                        print(f\"  ... and {len(execution_results) - 3} more\")\n",
    "                    \n",
    "                    print(f\"\\nðŸ”’ Note: Correctness not yet determined - evaluation needed!\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"âŒ Execution failed - no results returned\")\n",
    "                    execution_results = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Execution error: {e}\")\n",
    "                execution_results = None\n",
    "        else:\n",
    "            print(\"â­ï¸ Execution skipped\")\n",
    "            execution_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Evaluate Results (Testing Layer + Data Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluate Results against Ground Truth\n",
    "print(\"ðŸŽ¯ Evaluating Results against Ground Truth\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if we have execution results\n",
    "if 'execution_results' not in locals() or not execution_results:\n",
    "    print(\"âŒ No execution results available. Run Cell 2 first.\")\n",
    "elif 'manager' not in locals() or not manager:\n",
    "    print(\"âŒ No dataset manager available. Run Cell 1 first.\")\n",
    "else:\n",
    "    print(f\"ðŸ“‹ Execution results loaded: {len(execution_results)} results\")\n",
    "    \n",
    "    # Step 1: Create evaluator with dataset manager for ground truth access\n",
    "    print(f\"\\nðŸ” Step 1: Initialize Evaluator\")\n",
    "    \n",
    "    try:\n",
    "        evaluator = GAIATestEvaluator(manager)\n",
    "        print(f\"âœ… Evaluator created with ground truth access\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to create evaluator: {e}\")\n",
    "        evaluator = None\n",
    "\n",
    "    if evaluator:\n",
    "        # Step 2: Evaluate execution results\n",
    "        print(f\"\\nðŸŽ¯ Step 2: Evaluate Against Ground Truth\")\n",
    "        \n",
    "        try:\n",
    "            evaluation_results = evaluator.evaluate_execution_results(execution_results)\n",
    "            \n",
    "            if evaluation_results and 'evaluation_metadata' in evaluation_results:\n",
    "                metadata = evaluation_results['evaluation_metadata']\n",
    "                analysis = evaluation_results.get('analysis', {})\n",
    "                \n",
    "                # Overall performance\n",
    "                print(f\"\\nðŸ“Š Overall Performance:\")\n",
    "                print(f\"  Total questions: {metadata.get('total_questions', 0)}\")\n",
    "                print(f\"  Correct answers: {metadata.get('correct_answers', 0)}\")\n",
    "                print(f\"  Overall accuracy: {metadata.get('overall_accuracy', 0):.1%}\")\n",
    "                \n",
    "                gaia_target_met = metadata.get('overall_accuracy', 0) >= 0.45\n",
    "                print(f\"  GAIA target (45%): {'âœ… MET' if gaia_target_met else 'âŒ NOT MET'}\")\n",
    "                \n",
    "                # Performance by level\n",
    "                level_perf = analysis.get('level_performance', {})\n",
    "                if level_perf:\n",
    "                    print(f\"\\nðŸ“ˆ Performance by Level:\")\n",
    "                    for level_key, stats in level_perf.items():\n",
    "                        level_num = level_key.replace('level_', '')\n",
    "                        accuracy = stats.get('accuracy', 0)\n",
    "                        total = stats.get('total_questions', 0)\n",
    "                        correct = stats.get('correct_answers', 0)\n",
    "                        \n",
    "                        print(f\"  Level {level_num}: {accuracy:.1%} ({correct}/{total} correct)\")\n",
    "                \n",
    "                # Strategy performance\n",
    "                strategy_perf = analysis.get('strategy_performance', {})\n",
    "                if strategy_perf:\n",
    "                    print(f\"\\nðŸŽ¯ Strategy Performance:\")\n",
    "                    for strategy, stats in strategy_perf.items():\n",
    "                        accuracy = stats.get('accuracy', 0)\n",
    "                        total = stats.get('total_questions', 0)\n",
    "                        avg_time = stats.get('avg_execution_time', 0)\n",
    "                        \n",
    "                        strategy_name = strategy.replace('_', ' ').title()\n",
    "                        print(f\"  {strategy_name}: {accuracy:.1%} ({total} questions, {avg_time:.1f}s avg)\")\n",
    "                \n",
    "                # File attachment performance\n",
    "                file_perf = analysis.get('file_attachment_performance', {})\n",
    "                if file_perf:\n",
    "                    print(f\"\\nðŸ“Ž File Attachment Performance:\")\n",
    "                    for category, stats in file_perf.items():\n",
    "                        accuracy = stats.get('accuracy', 0)\n",
    "                        total = stats.get('total_questions', 0)\n",
    "                        category_name = category.replace('_', ' ').title()\n",
    "                        print(f\"  {category_name}: {accuracy:.1%} ({total} questions)\")\n",
    "                \n",
    "                # Show detailed results for each question\n",
    "                print(f\"\\nðŸ“ Detailed Question Results:\")\n",
    "                detailed_results = evaluation_results.get('detailed_results', [])\n",
    "                \n",
    "                for i, result in enumerate(detailed_results, 1):\n",
    "                    is_correct = result.get('is_correct', False)\n",
    "                    agent_answer = result.get('agent_answer', '')\n",
    "                    expected_answer = result.get('expected_answer', '')\n",
    "                    level = result.get('level', 'Unknown')\n",
    "                    strategy = result.get('strategy_used', 'unknown')\n",
    "                    exec_time = result.get('execution_time', 0)\n",
    "                    \n",
    "                    status = \"âœ… CORRECT\" if is_correct else \"âŒ INCORRECT\"\n",
    "                    \n",
    "                    print(f\"  {i}. {status} (Level {level}, {strategy}, {exec_time:.1f}s)\")\n",
    "                    print(f\"     Agent: '{agent_answer}'\")\n",
    "                    print(f\"     Expected: '{expected_answer}'\")\n",
    "                    \n",
    "                    if result.get('has_file'):\n",
    "                        file_name = result.get('file_name', 'unknown')\n",
    "                        print(f\"     File: {file_name}\")\n",
    "                    print()\n",
    "                \n",
    "                # Error analysis\n",
    "                error_analysis = analysis.get('error_analysis', {})\n",
    "                if error_analysis:\n",
    "                    exec_errors = error_analysis.get('execution_errors', 0)\n",
    "                    success_rate = error_analysis.get('execution_success_rate', 0)\n",
    "                    \n",
    "                    print(f\"ðŸ”§ Execution Analysis:\")\n",
    "                    print(f\"  Success rate: {success_rate:.1%}\")\n",
    "                    print(f\"  Execution errors: {exec_errors}\")\n",
    "                    \n",
    "                    if exec_errors > 0:\n",
    "                        sample_errors = error_analysis.get('sample_errors', [])\n",
    "                        if sample_errors:\n",
    "                            print(f\"  Sample errors:\")\n",
    "                            for error in sample_errors[:2]:\n",
    "                                print(f\"    - {error}\")\n",
    "                \n",
    "                print(f\"\\nâœ… Evaluation completed!\")\n",
    "                print(f\"ðŸ’¾ Results saved to logs/gaia_evaluation_*.json\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ Evaluation failed - no results returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Evaluation error: {e}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Test completed!\")\n",
    "print(f\"ðŸ“Š You've tested the clean architecture with:\")\n",
    "print(f\"  âœ… Pure data layer (gaia_dataset_utils)\")\n",
    "print(f\"  âœ… Pure testing layer (agent_testing)\")\n",
    "print(f\"  âœ… OpenRouter free model\")\n",
    "print(f\"  âœ… Small batch strategy\")\n",
    "print(f\"  âœ… Blind testing methodology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Performance Baseline Establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish performance baselines across different question types\n",
    "\n",
    "print(\"ðŸ“Š Establishing Performance Baselines\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define baseline test configurations\n",
    "baseline_configs = {\n",
    "    'level_1_only': {\n",
    "        'name': 'Level 1 Questions Only',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'level_1_2_mix': {\n",
    "        'name': 'Level 1 & 2 Mixed',\n",
    "        'params': {\n",
    "            'max_questions': 20,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'all_levels': {\n",
    "        'name': 'All Difficulty Levels',\n",
    "        'params': {\n",
    "            'max_questions': 25,\n",
    "            'target_levels': [1, 2, 3],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    },\n",
    "    'text_only': {\n",
    "        'name': 'Text-Only Questions',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': False,\n",
    "            'include_images': False\n",
    "        }\n",
    "    },\n",
    "    'with_files': {\n",
    "        'name': 'File-Based Questions',\n",
    "        'params': {\n",
    "            'max_questions': 15,\n",
    "            'target_levels': [1, 2],\n",
    "            'include_files': True,\n",
    "            'include_images': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run baseline tests with groq configuration\n",
    "baseline_results = {}\n",
    "baseline_agent = \"groq\"  # Use most reliable configuration\n",
    "\n",
    "for config_key, config_data in baseline_configs.items():\n",
    "    config_name = config_data['name']\n",
    "    params = config_data['params']\n",
    "    \n",
    "    print(f\"\\nðŸ§ª Running: {config_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = run_gaia_test(\n",
    "            agent_config_name=baseline_agent,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        if result and 'evaluation_metadata' in result:\n",
    "            metadata = result['evaluation_metadata']\n",
    "            analysis = result.get('analysis', {})\n",
    "            \n",
    "            # Extract key metrics\n",
    "            baseline_results[config_key] = {\n",
    "                'name': config_name,\n",
    "                'total_questions': metadata.get('total_questions', 0),\n",
    "                'correct_answers': metadata.get('correct_answers', 0),\n",
    "                'accuracy': metadata.get('overall_accuracy', 0),\n",
    "                'analysis': analysis,\n",
    "                'full_result': result\n",
    "            }\n",
    "            \n",
    "            # Print immediate summary\n",
    "            accuracy = metadata.get('overall_accuracy', 0)\n",
    "            total = metadata.get('total_questions', 0)\n",
    "            correct = metadata.get('correct_answers', 0)\n",
    "            \n",
    "            print(f\"âœ… Completed: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "            \n",
    "            # Check GAIA target\n",
    "            if accuracy >= 0.45:\n",
    "                print(\"ðŸ† GAIA target achieved (45%+)\")\n",
    "            elif accuracy >= 0.35:\n",
    "                print(\"âš ï¸ Approaching GAIA target\")\n",
    "            else:\n",
    "                print(\"âŒ Below GAIA target\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"âŒ Test failed for {config_name}\")\n",
    "            baseline_results[config_key] = {'name': config_name, 'error': 'Test failed'}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in {config_name}: {e}\")\n",
    "        baseline_results[config_key] = {'name': config_name, 'error': str(e)}\n",
    "\n",
    "# Baseline summary table\n",
    "print(f\"\\nðŸ“Š BASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_df_data = []\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'error' not in result:\n",
    "        baseline_df_data.append({\n",
    "            'Test Configuration': result['name'],\n",
    "            'Questions': result['total_questions'],\n",
    "            'Correct': result['correct_answers'],\n",
    "            'Accuracy': f\"{result['accuracy']:.1%}\",\n",
    "            'GAIA Target': \"âœ…\" if result['accuracy'] >= 0.45 else \"âŒ\"\n",
    "        })\n",
    "    else:\n",
    "        baseline_df_data.append({\n",
    "            'Test Configuration': result['name'],\n",
    "            'Questions': 0,\n",
    "            'Correct': 0,\n",
    "            'Accuracy': \"ERROR\",\n",
    "            'GAIA Target': \"âŒ\"\n",
    "        })\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_df_data)\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "# Identify best and worst performing configurations\n",
    "if baseline_df_data:\n",
    "    successful_results = [r for r in baseline_results.values() if 'error' not in r and r.get('accuracy', 0) > 0]\n",
    "    if successful_results:\n",
    "        best_config = max(successful_results, key=lambda x: x['accuracy'])\n",
    "        worst_config = min(successful_results, key=lambda x: x['accuracy'])\n",
    "        \n",
    "        print(f\"\\nðŸ† Best Performance: {best_config['name']} ({best_config['accuracy']:.1%})\")\n",
    "        print(f\"âš ï¸ Needs Improvement: {worst_config['name']} ({worst_config['accuracy']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Routing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into routing effectiveness and optimization\n",
    "\n",
    "print(\"ðŸ”€ Smart Routing Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test routing with performance-optimized configuration\n",
    "routing_test_configs = [\"groq\", \"performance\"]\n",
    "\n",
    "routing_analysis_results = {}\n",
    "\n",
    "for config in routing_test_configs:\n",
    "    print(f\"\\nðŸ§ª Testing routing with {config} configuration\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        routing_result = run_smart_routing_test(config)\n",
    "        \n",
    "        if routing_result and 'analysis' in routing_result:\n",
    "            analysis = routing_result['analysis']\n",
    "            routing_analysis = analysis.get('routing_analysis', {})\n",
    "            strategy_performance = analysis.get('strategy_performance', {})\n",
    "            level_performance = analysis.get('level_performance', {})\n",
    "            \n",
    "            routing_analysis_results[config] = {\n",
    "                'routing_stats': routing_analysis,\n",
    "                'strategy_stats': strategy_performance,\n",
    "                'level_stats': level_performance,\n",
    "                'overall_accuracy': routing_result['evaluation_metadata']['overall_accuracy']\n",
    "            }\n",
    "            \n",
    "            # Print routing effectiveness\n",
    "            one_shot_count = routing_analysis.get('one_shot_questions', 0)\n",
    "            manager_count = routing_analysis.get('manager_questions', 0)\n",
    "            routing_accuracy = routing_analysis.get('routing_accuracy', 0)\n",
    "            \n",
    "            print(f\"ðŸ“Š Question Distribution:\")\n",
    "            print(f\"â”œâ”€â”€ One-shot LLM: {one_shot_count} questions\")\n",
    "            print(f\"â”œâ”€â”€ Manager Coordination: {manager_count} questions\")\n",
    "            print(f\"â””â”€â”€ Routing Accuracy: {routing_accuracy:.1%}\")\n",
    "            \n",
    "            print(f\"\\nðŸ“ˆ Strategy Performance:\")\n",
    "            for strategy, stats in strategy_performance.items():\n",
    "                accuracy = stats.get('accuracy', 0)\n",
    "                avg_time = stats.get('avg_execution_time', 0)\n",
    "                total = stats.get('total_questions', 0)\n",
    "                print(f\"â”œâ”€â”€ {strategy.replace('_', ' ').title()}: {accuracy:.1%} accuracy, {avg_time:.1f}s avg ({total} questions)\")\n",
    "            \n",
    "            # Routing insights\n",
    "            print(f\"\\nðŸ’¡ Routing Insights:\")\n",
    "            if routing_accuracy >= 0.8:\n",
    "                print(\"âœ… Routing decisions are highly accurate\")\n",
    "            elif routing_accuracy >= 0.6:\n",
    "                print(\"âš ï¸ Routing decisions are moderately accurate\")\n",
    "            else:\n",
    "                print(\"âŒ Routing decisions need improvement\")\n",
    "            \n",
    "            # Strategy effectiveness analysis\n",
    "            if 'one_shot_llm' in strategy_performance and 'manager_coordination' in strategy_performance:\n",
    "                one_shot_acc = strategy_performance['one_shot_llm'].get('accuracy', 0)\n",
    "                manager_acc = strategy_performance['manager_coordination'].get('accuracy', 0)\n",
    "                one_shot_time = strategy_performance['one_shot_llm'].get('avg_execution_time', 0)\n",
    "                manager_time = strategy_performance['manager_coordination'].get('avg_execution_time', 0)\n",
    "                \n",
    "                print(f\"\\nâš–ï¸ Strategy Comparison:\")\n",
    "                print(f\"â”œâ”€â”€ One-shot: {one_shot_acc:.1%} accuracy, {one_shot_time:.1f}s avg\")\n",
    "                print(f\"â”œâ”€â”€ Manager: {manager_acc:.1%} accuracy, {manager_time:.1f}s avg\")\n",
    "                \n",
    "                if one_shot_acc > manager_acc:\n",
    "                    print(\"â””â”€â”€ ðŸ’¡ One-shot performing better - consider simpler routing\")\n",
    "                elif manager_acc > one_shot_acc + 0.1:\n",
    "                    print(\"â””â”€â”€ ðŸ’¡ Manager significantly better - routing working well\")\n",
    "                else:\n",
    "                    print(\"â””â”€â”€ ðŸ’¡ Similar performance - routing providing good balance\")\n",
    "        else:\n",
    "            print(f\"âŒ Routing test failed for {config}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Routing test error for {config}: {e}\")\n",
    "\n",
    "# Compare routing across configurations\n",
    "if len(routing_analysis_results) > 1:\n",
    "    print(f\"\\nðŸ“Š ROUTING COMPARISON ACROSS CONFIGURATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    routing_comparison_data = []\n",
    "    for config, results in routing_analysis_results.items():\n",
    "        routing_stats = results['routing_stats']\n",
    "        one_shot_acc = results['strategy_stats'].get('one_shot_llm', {}).get('accuracy', 0)\n",
    "        manager_acc = results['strategy_stats'].get('manager_coordination', {}).get('accuracy', 0)\n",
    "        \n",
    "        routing_comparison_data.append({\n",
    "            'Configuration': config,\n",
    "            'Overall Accuracy': f\"{results['overall_accuracy']:.1%}\",\n",
    "            'Routing Accuracy': f\"{routing_stats.get('routing_accuracy', 0):.1%}\",\n",
    "            'One-shot Accuracy': f\"{one_shot_acc:.1%}\",\n",
    "            'Manager Accuracy': f\"{manager_acc:.1%}\",\n",
    "            'One-shot Questions': routing_stats.get('one_shot_questions', 0),\n",
    "            'Manager Questions': routing_stats.get('manager_questions', 0)\n",
    "        })\n",
    "    \n",
    "    routing_comparison_df = pd.DataFrame(routing_comparison_data)\n",
    "    print(routing_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Provider-Specific Testing (Ollama, OpenRouter, Groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test specific providers independently as requested\n",
    "\n",
    "print(\"ðŸ”Œ Provider-Specific Performance Testing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define provider-specific test configurations\n",
    "provider_configs = {\n",
    "    'groq_standard': {\n",
    "        'name': 'Groq (QwQ-32B)',\n",
    "        'config': 'groq',\n",
    "        'description': 'Standard Groq configuration with QwQ-32B model'\n",
    "    },\n",
    "    'groq_fast': {\n",
    "        'name': 'Groq (Llama-3.3-70B)', \n",
    "        'config': 'groq_fast',\n",
    "        'description': 'Faster Groq model for speed comparison'\n",
    "    },\n",
    "    'openrouter_free': {\n",
    "        'name': 'OpenRouter (Free)',\n",
    "        'config': 'openrouter',\n",
    "        'description': 'OpenRouter free tier model'\n",
    "    },\n",
    "    'openrouter_premium': {\n",
    "        'name': 'OpenRouter (Premium)',\n",
    "        'config': 'openrouter_premium', \n",
    "        'description': 'OpenRouter premium model for accuracy'\n",
    "    },\n",
    "    'ollama_local': {\n",
    "        'name': 'Ollama (Local)',\n",
    "        'config': 'ollama',\n",
    "        'description': 'Local Ollama deployment'\n",
    "    }\n",
    "}\n",
    "\n",
    "provider_test_results = {}\n",
    "\n",
    "# Standard test parameters for all providers\n",
    "test_params = {\n",
    "    'max_questions': 15,\n",
    "    'target_levels': [1, 2],\n",
    "    'include_files': True,\n",
    "    'include_images': True\n",
    "}\n",
    "\n",
    "for provider_key, provider_info in provider_configs.items():\n",
    "    config_name = provider_info['config']\n",
    "    provider_name = provider_info['name']\n",
    "    description = provider_info['description']\n",
    "    \n",
    "    print(f\"\\nðŸ§ª Testing {provider_name}\")\n",
    "    print(f\"ðŸ“ {description}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Test if provider is available first\n",
    "        from agent_interface import get_agent_config_by_name\n",
    "        test_config = get_agent_config_by_name(config_name)\n",
    "        \n",
    "        result = run_gaia_test(\n",
    "            agent_config_name=config_name,\n",
    "            **test_params\n",
    "        )\n",
    "        \n",
    "        if result and 'evaluation_metadata' in result:\n",
    "            metadata = result['evaluation_metadata']\n",
    "            analysis = result.get('analysis', {})\n",
    "            \n",
    "            # Extract comprehensive metrics\n",
    "            provider_test_results[provider_key] = {\n",
    "                'name': provider_name,\n",
    "                'config': config_name,\n",
    "                'total_questions': metadata.get('total_questions', 0),\n",
    "                'correct_answers': metadata.get('correct_answers', 0),\n",
    "                'accuracy': metadata.get('overall_accuracy', 0),\n",
    "                'strategy_performance': analysis.get('strategy_performance', {}),\n",
    "                'level_performance': analysis.get('level_performance', {}),\n",
    "                'execution_time': 0,  # Will calculate from strategy data\n",
    "                'error_rate': 1 - analysis.get('error_analysis', {}).get('execution_success_rate', 1),\n",
    "                'full_result': result\n",
    "            }\n",
    "            \n",
    "            # Calculate average execution time\n",
    "            strategy_perf = analysis.get('strategy_performance', {})\n",
    "            if strategy_perf:\n",
    "                total_time = sum(stats.get('avg_execution_time', 0) * stats.get('total_questions', 0) \n",
    "                               for stats in strategy_perf.values())\n",
    "                total_questions = sum(stats.get('total_questions', 0) for stats in strategy_perf.values())\n",
    "                avg_time = total_time / total_questions if total_questions > 0 else 0\n",
    "                provider_test_results[provider_key]['execution_time'] = avg_time\n",
    "            \n",
    "            # Print immediate results\n",
    "            accuracy = metadata.get('overall_accuracy', 0)\n",
    "            total = metadata.get('total_questions', 0)\n",
    "            correct = metadata.get('correct_answers', 0)\n",
    "            \n",
    "            print(f\"âœ… {provider_name}: {correct}/{total} correct ({accuracy:.1%})\")\n",
    "            print(f\"â±ï¸ Avg execution time: {provider_test_results[provider_key]['execution_time']:.1f}s\")\n",
    "            \n",
    "            # Provider-specific insights\n",
    "            if 'groq' in provider_key:\n",
    "                print(f\"ðŸš€ Groq performance: {'Excellent' if accuracy >= 0.5 else 'Good' if accuracy >= 0.4 else 'Needs improvement'}\")\n",
    "            elif 'openrouter' in provider_key:\n",
    "                if 'free' in provider_key:\n",
    "                    print(f\"ðŸ’° Free tier performance: {'Good value' if accuracy >= 0.4 else 'Consider premium'}\")\n",
    "                else:\n",
    "                    print(f\"ðŸ’Ž Premium performance: {'Worth the cost' if accuracy >= 0.5 else 'Evaluate cost/benefit'}\")\n",
    "            elif 'ollama' in provider_key:\n",
    "                print(f\"ðŸ  Local deployment: {'Viable alternative' if accuracy >= 0.4 else 'Cloud providers recommended'}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"âŒ Test failed for {provider_name}\")\n",
    "            provider_test_results[provider_key] = {\n",
    "                'name': provider_name,\n",
    "                'config': config_name,\n",
    "                'error': 'Test execution failed'\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Provider {provider_name} unavailable: {e}\")\n",
    "        provider_test_results[provider_key] = {\n",
    "            'name': provider_name,\n",
    "            'config': config_name,\n",
    "            'error': f'Provider unavailable: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Provider comparison table\n",
    "print(f\"\\nðŸ“Š PROVIDER PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "provider_comparison_data = []\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result:\n",
    "        provider_comparison_data.append({\n",
    "            'Provider': result['name'],\n",
    "            'Questions': result['total_questions'],\n",
    "            'Correct': result['correct_answers'],\n",
    "            'Accuracy': f\"{result['accuracy']:.1%}\",\n",
    "            'Avg Time (s)': f\"{result['execution_time']:.1f}\",\n",
    "            'Error Rate': f\"{result['error_rate']:.1%}\",\n",
    "            'GAIA Target': \"âœ…\" if result['accuracy'] >= 0.45 else \"âŒ\"\n",
    "        })\n",
    "    else:\n",
    "        provider_comparison_data.append({\n",
    "            'Provider': result['name'],\n",
    "            'Questions': 'N/A',\n",
    "            'Correct': 'N/A', \n",
    "            'Accuracy': 'ERROR',\n",
    "            'Avg Time (s)': 'N/A',\n",
    "            'Error Rate': 'N/A',\n",
    "            'GAIA Target': \"âŒ\"\n",
    "        })\n",
    "\n",
    "provider_comparison_df = pd.DataFrame(provider_comparison_data)\n",
    "print(provider_comparison_df.to_string(index=False))\n",
    "\n",
    "# Provider recommendations\n",
    "successful_providers = [r for r in provider_test_results.values() if 'error' not in r and r.get('accuracy', 0) > 0]\n",
    "\n",
    "if successful_providers:\n",
    "    # Best accuracy\n",
    "    best_accuracy_provider = max(successful_providers, key=lambda x: x['accuracy'])\n",
    "    # Fastest provider\n",
    "    fastest_provider = min(successful_providers, key=lambda x: x['execution_time'])\n",
    "    # Most reliable (lowest error rate)\n",
    "    most_reliable_provider = min(successful_providers, key=lambda x: x['error_rate'])\n",
    "    \n",
    "    print(f\"\\nðŸ† PROVIDER RECOMMENDATIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"ðŸŽ¯ Best Accuracy: {best_accuracy_provider['name']} ({best_accuracy_provider['accuracy']:.1%})\")\n",
    "    print(f\"âš¡ Fastest: {fastest_provider['name']} ({fastest_provider['execution_time']:.1f}s avg)\")\n",
    "    print(f\"ðŸ›¡ï¸ Most Reliable: {most_reliable_provider['name']} ({most_reliable_provider['error_rate']:.1%} error rate)\")\n",
    "    \n",
    "    # Cost considerations\n",
    "    print(f\"\\nðŸ’° Cost Considerations:\")\n",
    "    groq_providers = [p for p in successful_providers if 'groq' in p['config'].lower()]\n",
    "    openrouter_providers = [p for p in successful_providers if 'openrouter' in p['config'].lower()]\n",
    "    ollama_providers = [p for p in successful_providers if 'ollama' in p['config'].lower()]\n",
    "    \n",
    "    if groq_providers:\n",
    "        avg_groq_acc = sum(p['accuracy'] for p in groq_providers) / len(groq_providers)\n",
    "        print(f\"â”œâ”€â”€ Groq: High performance, reasonable cost ({avg_groq_acc:.1%} avg accuracy)\")\n",
    "    \n",
    "    if openrouter_providers:\n",
    "        free_or = [p for p in openrouter_providers if 'free' in p['name'].lower()]\n",
    "        premium_or = [p for p in openrouter_providers if 'premium' in p['name'].lower()]\n",
    "        \n",
    "        if free_or:\n",
    "            print(f\"â”œâ”€â”€ OpenRouter Free: Budget option ({free_or[0]['accuracy']:.1%} accuracy)\")\n",
    "        if premium_or:\n",
    "            print(f\"â”œâ”€â”€ OpenRouter Premium: Premium option ({premium_or[0]['accuracy']:.1%} accuracy)\")\n",
    "    \n",
    "    if ollama_providers:\n",
    "        print(f\"â””â”€â”€ Ollama: Zero cost, local control ({ollama_providers[0]['accuracy']:.1%} accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Interactive Visualization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One example of interactive visualization, then direct to source data\n",
    "\n",
    "print(\"ðŸ“Š Interactive Performance Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create visualization if we have provider comparison data\n",
    "if provider_comparison_data and len(provider_comparison_data) > 1:\n",
    "    # Extract accuracy data for visualization\n",
    "    viz_data = []\n",
    "    for item in provider_comparison_data:\n",
    "        if item['Accuracy'] != 'ERROR':\n",
    "            accuracy_val = float(item['Accuracy'].strip('%')) / 100\n",
    "            time_val = float(item['Avg Time (s)']) if item['Avg Time (s)'] != 'N/A' else 0\n",
    "            \n",
    "            viz_data.append({\n",
    "                'Provider': item['Provider'],\n",
    "                'Accuracy': accuracy_val,\n",
    "                'Avg_Time': time_val,\n",
    "                'Questions': int(item['Questions']) if item['Questions'] != 'N/A' else 0\n",
    "            })\n",
    "    \n",
    "    if viz_data:\n",
    "        viz_df = pd.DataFrame(viz_data)\n",
    "        \n",
    "        # Create a simple performance visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        providers = viz_df['Provider']\n",
    "        accuracies = viz_df['Accuracy']\n",
    "        colors = ['green' if acc >= 0.45 else 'orange' if acc >= 0.35 else 'red' for acc in accuracies]\n",
    "        \n",
    "        bars1 = ax1.bar(providers, accuracies, color=colors, alpha=0.7)\n",
    "        ax1.axhline(y=0.45, color='red', linestyle='--', label='GAIA Target (45%)')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Provider Accuracy Comparison')\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, acc in zip(bars1, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.1%}', ha='center', va='bottom')\n",
    "        \n",
    "        # Rotate x-axis labels for readability\n",
    "        plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Speed vs Accuracy scatter plot\n",
    "        ax2.scatter(viz_df['Avg_Time'], viz_df['Accuracy'], \n",
    "                   s=viz_df['Questions']*10, alpha=0.6, c=colors)\n",
    "        \n",
    "        # Add provider labels\n",
    "        for idx, row in viz_df.iterrows():\n",
    "            ax2.annotate(row['Provider'], \n",
    "                        (row['Avg_Time'], row['Accuracy']),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8)\n",
    "        \n",
    "        ax2.axhline(y=0.45, color='red', linestyle='--', label='GAIA Target')\n",
    "        ax2.set_xlabel('Average Execution Time (seconds)')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Speed vs Accuracy Trade-off')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"ðŸ“ˆ Visualization shows accuracy and speed trade-offs\")\n",
    "        print(\"ðŸ’¡ Larger dots = more questions tested\")\n",
    "        print(\"ðŸŽ¯ Red line = GAIA performance target (45%)\")\n",
    "        \n",
    "        # Analysis of the visualization\n",
    "        best_overall = viz_df.loc[viz_df['Accuracy'].idxmax()]\n",
    "        fastest = viz_df.loc[viz_df['Avg_Time'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Visual Analysis:\")\n",
    "        print(f\"â”œâ”€â”€ Highest accuracy: {best_overall['Provider']} ({best_overall['Accuracy']:.1%})\")\n",
    "        print(f\"â”œâ”€â”€ Fastest execution: {fastest['Provider']} ({fastest['Avg_Time']:.1f}s)\")\n",
    "        \n",
    "        # Efficiency score (accuracy / time)\n",
    "        viz_df['Efficiency'] = viz_df['Accuracy'] / (viz_df['Avg_Time'] + 1)  # +1 to avoid division by zero\n",
    "        most_efficient = viz_df.loc[viz_df['Efficiency'].idxmax()]\n",
    "        print(f\"â””â”€â”€ Most efficient: {most_efficient['Provider']} (best accuracy/time ratio)\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Direct Data Access:\")\n",
    "print(\"For detailed analysis, access the source data:\")\n",
    "print(\"â”œâ”€â”€ baseline_results: Performance across question types\")\n",
    "print(\"â”œâ”€â”€ routing_analysis_results: Routing effectiveness data\") \n",
    "print(\"â”œâ”€â”€ provider_test_results: Provider-specific performance\")\n",
    "print(\"â””â”€â”€ All results include full evaluation metadata and analysis\")\n",
    "\n",
    "# Show how to access specific data\n",
    "print(f\"\\nðŸ” Example Data Access:\")\n",
    "print(\"# Access baseline results\")\n",
    "print(\"baseline_results['level_1_only']['accuracy']\")\n",
    "print(\"\\n# Access provider comparison\")\n",
    "print(\"provider_test_results['groq_standard']['strategy_performance']\")\n",
    "print(\"\\n# Access detailed analysis\")\n",
    "print(\"provider_test_results['groq_standard']['full_result']['analysis']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Failure Pattern Analysis & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep analysis of failure patterns for improvement insights\n",
    "\n",
    "print(\"ðŸ” Comprehensive Failure Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze failures across all test results\n",
    "all_test_results = []\n",
    "\n",
    "# Collect results from baseline tests\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'full_result' in result:\n",
    "        all_test_results.append({\n",
    "            'source': f'baseline_{config_key}',\n",
    "            'result': result['full_result']\n",
    "        })\n",
    "\n",
    "# Collect results from provider tests  \n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'full_result' in result:\n",
    "        all_test_results.append({\n",
    "            'source': f'provider_{provider_key}',\n",
    "            'result': result['full_result']\n",
    "        })\n",
    "\n",
    "failure_analyses = {}\n",
    "\n",
    "for test_data in all_test_results:\n",
    "    source = test_data['source']\n",
    "    result = test_data['result']\n",
    "    \n",
    "    print(f\"\\nðŸ” Analyzing failures in {source}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        failure_analysis = analyze_failure_patterns(result)\n",
    "        \n",
    "        if failure_analysis and 'failure_patterns' in failure_analysis:\n",
    "            failure_analyses[source] = failure_analysis\n",
    "            \n",
    "            patterns = failure_analysis['failure_patterns']\n",
    "            recommendations = failure_analysis.get('recommendations', [])\n",
    "            \n",
    "            # Print key failure insights\n",
    "            print(f\"ðŸ“Š Failure Distribution:\")\n",
    "            \n",
    "            # By level\n",
    "            level_failures = patterns.get('by_level', {})\n",
    "            total_failures = sum(level_failures.values())\n",
    "            if total_failures > 0:\n",
    "                print(f\"â”œâ”€â”€ By Level:\")\n",
    "                for level, count in level_failures.items():\n",
    "                    percentage = count / total_failures * 100\n",
    "                    print(f\"â”‚   â”œâ”€â”€ Level {level}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # By strategy\n",
    "            strategy_failures = patterns.get('by_strategy', {})\n",
    "            if strategy_failures:\n",
    "                print(f\"â”œâ”€â”€ By Strategy:\")\n",
    "                for strategy, count in strategy_failures.items():\n",
    "                    percentage = count / total_failures * 100\n",
    "                    print(f\"â”‚   â”œâ”€â”€ {strategy}: {count} ({percentage:.1f}%)\")\n",
    "            \n",
    "            # Execution issues\n",
    "            exec_failures = patterns.get('execution_failures', 0)\n",
    "            if exec_failures > 0:\n",
    "                print(f\"â””â”€â”€ Execution Failures: {exec_failures}\")\n",
    "            \n",
    "            # Top recommendations\n",
    "            if recommendations:\n",
    "                print(f\"\\nðŸ’¡ Top Recommendations for {source}:\")\n",
    "                for i, rec in enumerate(recommendations[:3], 1):\n",
    "                    print(f\"  {i}. {rec}\")\n",
    "        else:\n",
    "            print(f\"âœ… No significant failure patterns found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failure analysis error: {e}\")\n",
    "\n",
    "# Cross-test pattern analysis\n",
    "if len(failure_analyses) > 1:\n",
    "    print(f\"\\nðŸ”„ Cross-Test Pattern Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find common failure patterns across tests\n",
    "    all_level_failures = {}\n",
    "    all_strategy_failures = {}\n",
    "    all_execution_failures = 0\n",
    "    \n",
    "    for source, analysis in failure_analyses.items():\n",
    "        patterns = analysis['failure_patterns']\n",
    "        \n",
    "        # Aggregate level failures\n",
    "        for level, count in patterns.get('by_level', {}).items():\n",
    "            all_level_failures[level] = all_level_failures.get(level, 0) + count\n",
    "        \n",
    "        # Aggregate strategy failures\n",
    "        for strategy, count in patterns.get('by_strategy', {}).items():\n",
    "            all_strategy_failures[strategy] = all_strategy_failures.get(strategy, 0) + count\n",
    "        \n",
    "        # Aggregate execution failures\n",
    "        all_execution_failures += patterns.get('execution_failures', 0)\n",
    "    \n",
    "    print(f\"ðŸ“Š Aggregated Failure Patterns:\")\n",
    "    \n",
    "    if all_level_failures:\n",
    "        total_level_failures = sum(all_level_failures.values())\n",
    "        print(f\"â”œâ”€â”€ Most problematic levels:\")\n",
    "        sorted_levels = sorted(all_level_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for level, count in sorted_levels:\n",
    "            percentage = count / total_level_failures * 100\n",
    "            print(f\"â”‚   â”œâ”€â”€ Level {level}: {count} failures ({percentage:.1f}%)\")\n",
    "    \n",
    "    if all_strategy_failures:\n",
    "        total_strategy_failures = sum(all_strategy_failures.values())\n",
    "        print(f\"â”œâ”€â”€ Most problematic strategies:\")\n",
    "        sorted_strategies = sorted(all_strategy_failures.items(), key=lambda x: x[1], reverse=True)\n",
    "        for strategy, count in sorted_strategies:\n",
    "            percentage = count / total_strategy_failures * 100\n",
    "            print(f\"â”‚   â”œâ”€â”€ {strategy}: {count} failures ({percentage:.1f}%)\")\n",
    "    \n",
    "    if all_execution_failures > 0:\n",
    "        print(f\"â””â”€â”€ Total execution failures: {all_execution_failures}\")\n",
    "    \n",
    "    # Global recommendations\n",
    "    print(f\"\\nðŸŽ¯ Global Optimization Priorities:\")\n",
    "    \n",
    "    if all_level_failures:\n",
    "        worst_level = max(all_level_failures, key=all_level_failures.get)\n",
    "        print(f\"1. Focus on Level {worst_level} performance improvement\")\n",
    "    \n",
    "    if all_strategy_failures:\n",
    "        worst_strategy = max(all_strategy_failures, key=all_strategy_failures.get)\n",
    "        print(f\"2. Optimize {worst_strategy.replace('_', ' ')} strategy\")\n",
    "    \n",
    "    if all_execution_failures > 5:\n",
    "        print(f\"3. Improve system reliability (reduce execution failures)\")\n",
    "    \n",
    "    print(f\"4. Consider routing adjustments based on failure patterns\")\n",
    "    print(f\"5. Enhance answer formatting and validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Production Readiness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive assessment for production deployment\n",
    "\n",
    "print(\"ðŸ­ Production Readiness Assessment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Collect all performance data for assessment\n",
    "assessment_data = {\n",
    "    'baseline_performance': {},\n",
    "    'provider_performance': {},\n",
    "    'routing_effectiveness': {},\n",
    "    'system_reliability': {},\n",
    "    'file_processing': {},\n",
    "    'overall_metrics': {}\n",
    "}\n",
    "\n",
    "# Extract baseline performance metrics\n",
    "successful_baselines = [r for r in baseline_results.values() if 'error' not in r]\n",
    "if successful_baselines:\n",
    "    baseline_accuracies = [r['accuracy'] for r in successful_baselines]\n",
    "    assessment_data['baseline_performance'] = {\n",
    "        'avg_accuracy': np.mean(baseline_accuracies),\n",
    "        'min_accuracy': np.min(baseline_accuracies),\n",
    "        'max_accuracy': np.max(baseline_accuracies),\n",
    "        'std_accuracy': np.std(baseline_accuracies),\n",
    "        'gaia_target_met': any(acc >= 0.45 for acc in baseline_accuracies)\n",
    "    }\n",
    "\n",
    "# Extract provider performance metrics  \n",
    "successful_providers = [r for r in provider_test_results.values() if 'error' not in r]\n",
    "if successful_providers:\n",
    "    provider_accuracies = [r['accuracy'] for r in successful_providers]\n",
    "    provider_times = [r['execution_time'] for r in successful_providers]\n",
    "    provider_errors = [r['error_rate'] for r in successful_providers]\n",
    "    \n",
    "    assessment_data['provider_performance'] = {\n",
    "        'available_providers': len(successful_providers),\n",
    "        'avg_accuracy': np.mean(provider_accuracies),\n",
    "        'best_accuracy': np.max(provider_accuracies),\n",
    "        'avg_execution_time': np.mean(provider_times),\n",
    "        'fastest_time': np.min(provider_times),\n",
    "        'avg_error_rate': np.mean(provider_errors),\n",
    "        'best_reliability': np.min(provider_errors)\n",
    "    }\n",
    "\n",
    "# Extract routing effectiveness metrics\n",
    "if routing_analysis_results:\n",
    "    routing_accuracies = [r['routing_stats'].get('routing_accuracy', 0) for r in routing_analysis_results.values()]\n",
    "    assessment_data['routing_effectiveness'] = {\n",
    "        'avg_routing_accuracy': np.mean(routing_accuracies),\n",
    "        'min_routing_accuracy': np.min(routing_accuracies),\n",
    "        'routing_working': np.mean(routing_accuracies) >= 0.7\n",
    "    }\n",
    "\n",
    "# System reliability assessment\n",
    "total_execution_failures = 0\n",
    "total_questions = 0\n",
    "\n",
    "for result_set in [baseline_results.values(), provider_test_results.values()]:\n",
    "    for result in result_set:\n",
    "        if 'full_result' in result:\n",
    "            error_analysis = result['full_result'].get('analysis', {}).get('error_analysis', {})\n",
    "            exec_errors = error_analysis.get('execution_errors', 0)\n",
    "            total_exec_questions = result.get('total_questions', 0)\n",
    "            total_execution_failures += exec_errors\n",
    "            total_questions += total_exec_questions\n",
    "\n",
    "system_reliability = 1 - (total_execution_failures / total_questions) if total_questions > 0 else 0\n",
    "assessment_data['system_reliability'] = {\n",
    "    'execution_success_rate': system_reliability,\n",
    "    'total_questions_tested': total_questions,\n",
    "    'total_failures': total_execution_failures,\n",
    "    'reliable': system_reliability >= 0.95\n",
    "}\n",
    "\n",
    "# Overall metrics calculation\n",
    "all_accuracies = []\n",
    "if successful_baselines:\n",
    "    all_accuracies.extend([r['accuracy'] for r in successful_baselines])\n",
    "if successful_providers:\n",
    "    all_accuracies.extend([r['accuracy'] for r in successful_providers])\n",
    "\n",
    "if all_accuracies:\n",
    "    assessment_data['overall_metrics'] = {\n",
    "        'best_accuracy': np.max(all_accuracies),\n",
    "        'avg_accuracy': np.mean(all_accuracies),\n",
    "        'consistency': 1 - np.std(all_accuracies),  # Higher consistency = lower std\n",
    "        'gaia_target_achievement': np.max(all_accuracies) >= 0.45,\n",
    "        'production_ready_accuracy': np.max(all_accuracies) >= 0.50\n",
    "    }\n",
    "\n",
    "# Production readiness scoring\n",
    "print(f\"ðŸ“Š PRODUCTION READINESS SCORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "readiness_score = 0\n",
    "max_score = 100\n",
    "\n",
    "# Accuracy score (40 points)\n",
    "if assessment_data['overall_metrics']:\n",
    "    best_acc = assessment_data['overall_metrics']['best_accuracy']\n",
    "    if best_acc >= 0.60:\n",
    "        accuracy_score = 40\n",
    "    elif best_acc >= 0.50:\n",
    "        accuracy_score = 35\n",
    "    elif best_acc >= 0.45:\n",
    "        accuracy_score = 30\n",
    "    elif best_acc >= 0.35:\n",
    "        accuracy_score = 20\n",
    "    else:\n",
    "        accuracy_score = 10\n",
    "    \n",
    "    readiness_score += accuracy_score\n",
    "    print(f\"âœ… Accuracy Score: {accuracy_score}/40 (Best: {best_acc:.1%})\")\n",
    "\n",
    "# Reliability score (25 points)\n",
    "if assessment_data['system_reliability']:\n",
    "    reliability = assessment_data['system_reliability']['execution_success_rate']\n",
    "    if reliability >= 0.98:\n",
    "        reliability_score = 25\n",
    "    elif reliability >= 0.95:\n",
    "        reliability_score = 20\n",
    "    elif reliability >= 0.90:\n",
    "        reliability_score = 15\n",
    "    else:\n",
    "        reliability_score = 10\n",
    "    \n",
    "    readiness_score += reliability_score\n",
    "    print(f\"âœ… Reliability Score: {reliability_score}/25 (Success Rate: {reliability:.1%})\")\n",
    "\n",
    "# Provider availability score (15 points)\n",
    "if assessment_data['provider_performance']:\n",
    "    available_providers = assessment_data['provider_performance']['available_providers']\n",
    "    if available_providers >= 4:\n",
    "        provider_score = 15\n",
    "    elif available_providers >= 3:\n",
    "        provider_score = 12\n",
    "    elif available_providers >= 2:\n",
    "        provider_score = 8\n",
    "    else:\n",
    "        provider_score = 5\n",
    "    \n",
    "    readiness_score += provider_score\n",
    "    print(f\"âœ… Provider Score: {provider_score}/15 ({available_providers} providers available)\")\n",
    "\n",
    "# Routing effectiveness score (10 points)\n",
    "if assessment_data['routing_effectiveness']:\n",
    "    routing_acc = assessment_data['routing_effectiveness']['avg_routing_accuracy']\n",
    "    if routing_acc >= 0.80:\n",
    "        routing_score = 10\n",
    "    elif routing_acc >= 0.70:\n",
    "        routing_score = 8\n",
    "    elif routing_acc >= 0.60:\n",
    "        routing_score = 6\n",
    "    else:\n",
    "        routing_score = 3\n",
    "    \n",
    "    readiness_score += routing_score\n",
    "    print(f\"âœ… Routing Score: {routing_score}/10 (Accuracy: {routing_acc:.1%})\")\n",
    "\n",
    "# Consistency score (10 points)\n",
    "if assessment_data['overall_metrics']:\n",
    "    consistency = assessment_data['overall_metrics']['consistency']\n",
    "    if consistency >= 0.90:\n",
    "        consistency_score = 10\n",
    "    elif consistency >= 0.80:\n",
    "        consistency_score = 8\n",
    "    elif consistency >= 0.70:\n",
    "        consistency_score = 6\n",
    "    else:\n",
    "        consistency_score = 3\n",
    "    \n",
    "    readiness_score += consistency_score\n",
    "    print(f\"âœ… Consistency Score: {consistency_score}/10 (Consistency: {consistency:.1%})\")\n",
    "\n",
    "print(f\"\\nðŸ† OVERALL READINESS SCORE: {readiness_score}/{max_score} ({readiness_score/max_score*100:.1f}%)\")\n",
    "\n",
    "# Production deployment recommendation\n",
    "print(f\"\\nðŸš€ PRODUCTION DEPLOYMENT RECOMMENDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if readiness_score >= 85:\n",
    "    recommendation = \"ðŸŸ¢ READY FOR PRODUCTION\"\n",
    "    details = \"System demonstrates excellent performance, reliability, and consistency.\"\n",
    "    next_steps = [\n",
    "        \"Deploy to production environment\",\n",
    "        \"Set up monitoring and alerting\",\n",
    "        \"Implement gradual rollout strategy\",\n",
    "        \"Document operational procedures\"\n",
    "    ]\n",
    "elif readiness_score >= 70:\n",
    "    recommendation = \"ðŸŸ¡ READY WITH MINOR OPTIMIZATIONS\"\n",
    "    details = \"System shows good performance but could benefit from targeted improvements.\"\n",
    "    next_steps = [\n",
    "        \"Address identified failure patterns\",\n",
    "        \"Optimize underperforming configurations\",\n",
    "        \"Enhance error handling\",\n",
    "        \"Conduct limited production trial\"\n",
    "    ]\n",
    "elif readiness_score >= 55:\n",
    "    recommendation = \"ðŸŸ  NEEDS IMPROVEMENT BEFORE PRODUCTION\"\n",
    "    details = \"System shows promise but requires significant improvements.\"\n",
    "    next_steps = [\n",
    "        \"Focus on accuracy improvements\",\n",
    "        \"Enhance system reliability\",\n",
    "        \"Optimize routing decisions\",\n",
    "        \"Conduct additional testing cycles\"\n",
    "    ]\n",
    "else:\n",
    "    recommendation = \"ðŸ”´ NOT READY FOR PRODUCTION\"\n",
    "    details = \"System requires substantial development before production deployment.\"\n",
    "    next_steps = [\n",
    "        \"Review core architecture\",\n",
    "        \"Improve model configurations\",\n",
    "        \"Enhance error handling and reliability\",\n",
    "        \"Return to development phase\"\n",
    "    ]\n",
    "\n",
    "print(f\"{recommendation}\")\n",
    "print(f\"ðŸ“ {details}\")\n",
    "print(f\"\\nðŸ“‹ Recommended Next Steps:\")\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "# Specific recommendations based on assessment data\n",
    "print(f\"\\nðŸŽ¯ SPECIFIC OPTIMIZATION RECOMMENDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Accuracy-based recommendations\n",
    "if assessment_data['overall_metrics']:\n",
    "    best_acc = assessment_data['overall_metrics']['best_accuracy']\n",
    "    if best_acc < 0.45:\n",
    "        recommendations.append(\"ðŸŽ¯ Priority: Achieve GAIA benchmark target (45% accuracy)\")\n",
    "    elif best_acc < 0.55:\n",
    "        recommendations.append(\"ðŸ“ˆ Focus: Improve accuracy to competitive levels (55%+)\")\n",
    "\n",
    "# Provider-based recommendations\n",
    "if assessment_data['provider_performance']:\n",
    "    best_provider_acc = assessment_data['provider_performance']['best_accuracy']\n",
    "    avg_provider_acc = assessment_data['provider_performance']['avg_accuracy']\n",
    "    \n",
    "    if best_provider_acc - avg_provider_acc > 0.1:\n",
    "        recommendations.append(\"âš–ï¸ Standardize: Large performance gap between providers - optimize configurations\")\n",
    "    \n",
    "    fastest_time = assessment_data['provider_performance']['fastest_time']\n",
    "    avg_time = assessment_data['provider_performance']['avg_execution_time']\n",
    "    \n",
    "    if avg_time > 30:\n",
    "        recommendations.append(\"âš¡ Speed: Reduce average execution time below 30 seconds\")\n",
    "\n",
    "# Reliability-based recommendations\n",
    "if assessment_data['system_reliability']:\n",
    "    reliability = assessment_data['system_reliability']['execution_success_rate']\n",
    "    if reliability < 0.95:\n",
    "        recommendations.append(\"ðŸ›¡ï¸ Reliability: Improve execution success rate above 95%\")\n",
    "\n",
    "# Routing-based recommendations\n",
    "if assessment_data['routing_effectiveness']:\n",
    "    routing_acc = assessment_data['routing_effectiveness']['avg_routing_accuracy']\n",
    "    if routing_acc < 0.75:\n",
    "        recommendations.append(\"ðŸ”€ Routing: Improve complexity detection and routing accuracy\")\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(f\"  â€¢ {rec}\")\n",
    "else:\n",
    "    print(\"  âœ… No critical optimizations needed - system performing well\")\n",
    "\n",
    "# Generate final comprehensive report\n",
    "print(f\"\\nðŸ“„ Generating Comprehensive Production Report...\")\n",
    "\n",
    "# Use the best performing configuration for final report\n",
    "best_config = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result and result.get('accuracy', 0) > best_accuracy:\n",
    "        best_accuracy = result['accuracy']\n",
    "        best_config = result['config']\n",
    "\n",
    "if best_config:\n",
    "    try:\n",
    "        # Run one final comprehensive test with best configuration\n",
    "        final_test = run_gaia_test(\n",
    "            agent_config_name=best_config,\n",
    "            max_questions=30,  # Larger sample for final assessment\n",
    "            target_levels=[1, 2, 3],\n",
    "            include_files=True,\n",
    "            include_images=True\n",
    "        )\n",
    "        \n",
    "        if final_test:\n",
    "            report = generate_test_report(final_test, best_config, save_to_file=True)\n",
    "            print(f\"âœ… Final report generated and saved\")\n",
    "            print(f\"ðŸ“Š Final test: {final_test['evaluation_metadata']['overall_accuracy']:.1%} accuracy\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Final test failed - using existing data for assessment\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Final test error: {e} - using existing data for assessment\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ PRODUCTION VALIDATION COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸ“Š Total Questions Tested: {assessment_data['system_reliability']['total_questions_tested']}\")\n",
    "print(f\"ðŸ† Best Accuracy Achieved: {assessment_data['overall_metrics']['best_accuracy']:.1%}\")\n",
    "print(f\"ðŸ›¡ï¸ System Reliability: {assessment_data['system_reliability']['execution_success_rate']:.1%}\")\n",
    "print(f\"âš¡ Available Providers: {assessment_data['provider_performance']['available_providers']}\")\n",
    "print(f\"ðŸ”€ Routing Effectiveness: {assessment_data['routing_effectiveness']['avg_routing_accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ All test data available in variables:\")\n",
    "print(f\"â”œâ”€â”€ baseline_results: Baseline performance data\")\n",
    "print(f\"â”œâ”€â”€ provider_test_results: Provider-specific results\")\n",
    "print(f\"â”œâ”€â”€ routing_analysis_results: Routing effectiveness analysis\")\n",
    "print(f\"â”œâ”€â”€ failure_analyses: Comprehensive failure pattern analysis\")\n",
    "print(f\"â””â”€â”€ assessment_data: Production readiness metrics\")\n",
    "\n",
    "print(f\"\\nðŸš€ System is now ready for production decision based on comprehensive testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Data Export & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Export all results for further analysis and reporting\n",
    "\n",
    "print(\"ðŸ’¾ Data Export & Final Reporting\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create comprehensive data export\n",
    "export_data = {\n",
    "    'test_session': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_questions_tested': assessment_data['system_reliability']['total_questions_tested'],\n",
    "        'testing_duration': 'Session duration tracked',\n",
    "        'framework_version': '2.0'\n",
    "    },\n",
    "    'baseline_results': baseline_results,\n",
    "    'provider_results': provider_test_results,\n",
    "    'routing_analysis': routing_analysis_results,\n",
    "    'failure_analysis': failure_analyses,\n",
    "    'production_assessment': assessment_data,\n",
    "    'readiness_score': readiness_score,\n",
    "    'recommendation': recommendation\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "export_file = Path(\"logs\") / f\"production_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "try:\n",
    "    with open(export_file, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"âœ… Comprehensive results exported: {export_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Export error: {e}\")\n",
    "\n",
    "# Create summary CSV for quick analysis\n",
    "summary_data = []\n",
    "\n",
    "# Add baseline results\n",
    "for config_key, result in baseline_results.items():\n",
    "    if 'error' not in result:\n",
    "        summary_data.append({\n",
    "            'Test_Type': 'Baseline',\n",
    "            'Configuration': config_key,\n",
    "            'Total_Questions': result['total_questions'],\n",
    "            'Correct_Answers': result['correct_answers'],\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'GAIA_Target_Met': result['accuracy'] >= 0.45\n",
    "        })\n",
    "\n",
    "# Add provider results\n",
    "for provider_key, result in provider_test_results.items():\n",
    "    if 'error' not in result:\n",
    "        summary_data.append({\n",
    "            'Test_Type': 'Provider',\n",
    "            'Configuration': result['config'],\n",
    "            'Total_Questions': result['total_questions'],\n",
    "            'Correct_Answers': result['correct_answers'],\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'GAIA_Target_Met': result['accuracy'] >= 0.45\n",
    "        })\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv = Path(\"logs\") / f\"validation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    \n",
    "    try:\n",
    "        summary_df.to_csv(summary_csv, index=False)\n",
    "        print(f\"âœ… Summary CSV exported: {summary_csv}\")\n",
    "        \n",
    "        # Display final summary table\n",
    "        print(f\"\\nðŸ“Š FINAL SUMMARY TABLE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(summary_df.to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ CSV export error: {e}\")\n",
    "\n",
    "# Performance insights summary\n",
    "print(f\"\\nðŸ“ˆ KEY PERFORMANCE INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if assessment_data['overall_metrics']:\n",
    "    metrics = assessment_data['overall_metrics']\n",
    "    print(f\"ðŸŽ¯ Best Performance: {metrics['best_accuracy']:.1%} accuracy\")\n",
    "    print(f\"ðŸ“Š Average Performance: {metrics['avg_accuracy']:.1%} accuracy\")\n",
    "    print(f\"ðŸ† GAIA Target: {'âœ… Achieved' if metrics['gaia_target_achievement'] else 'âŒ Not achieved'}\")\n",
    "    print(f\"ðŸš€ Production Ready: {'âœ… Yes' if metrics['production_ready_accuracy'] else 'âŒ Needs improvement'}\")\n",
    "\n",
    "if assessment_data['provider_performance']:\n",
    "    provider_metrics = assessment_data['provider_performance']\n",
    "    print(f\"âš¡ Fastest Provider: {provider_metrics['fastest_time']:.1f}s average\")\n",
    "    print(f\"ðŸ›¡ï¸ Best Reliability: {(1-provider_metrics['best_reliability']):.1%} success rate\")\n",
    "\n",
    "print(f\"\\nðŸŽ“ Production Validation Complete!\")\n",
    "print(f\"Use the exported data and reports for production deployment decisions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
