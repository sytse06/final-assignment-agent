{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# GAIA VALIDATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Test correct initialization of GaiaAgent object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ GAIA Agent Initialization Test\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìÖ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìÅ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dependencies\n",
    "try:\n",
    "    print(\"üîÑ Importing GAIA components...\")\n",
    "    from agent_logic import GAIAAgent, GAIAConfig\n",
    "    from agent_interface import get_groq_config, create_gaia_agent\n",
    "    print(\"‚úÖ Agent imports successful\")\n",
    "    \n",
    "    # Check API keys\n",
    "    required_vars = ['ANTHROPIC_API_KEY', 'GOOGLE_API_KEY', 'OPENROUTER_API_KEY']\n",
    "    available_providers = []\n",
    "    \n",
    "    for var in required_vars:\n",
    "        if os.getenv(var):\n",
    "            provider = var.replace('_API_KEY', '').lower()\n",
    "            available_providers.append(provider)\n",
    "            print(f\"‚úÖ {var}: Available\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {var}: Not set\")\n",
    "    \n",
    "    print(f\"üéØ Available providers: {available_providers}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Setting up configuration...\")\n",
    "\n",
    "# Choose your provider\n",
    "chosen_provider = \"anthropuc\"\n",
    "\n",
    "# Handle different return types correctly\n",
    "if chosen_provider == \"google\":\n",
    "    # get_groq_config() returns GAIAConfig object directly\n",
    "    gaia_config = get_google_config()\n",
    "    print(f\"‚úÖ Using Groq config (GAIAConfig object)\")\n",
    "    \n",
    "elif chosen_provider == \"google\":\n",
    "    # Create GAIAConfig for Google\n",
    "    gaia_config = GAIAConfig(\n",
    "        model_provider=\"anthropic\",\n",
    "        model_name=\"claude-sonnet-4-20250514\", \n",
    "        temperature=0.1,\n",
    "        enable_smart_routing=True,\n",
    "        enable_context_bridge=True,\n",
    "        enable_csv_logging=False,\n",
    "        debug_mode=True\n",
    "    )\n",
    "    print(f\"‚úÖ Created Google config\")\n",
    "    \n",
    "elif chosen_provider == \"openrouter\":\n",
    "    # Create GAIAConfig for OpenRouter\n",
    "    gaia_config = GAIAConfig(\n",
    "        model_provider=\"openrouter\",\n",
    "        model_name=\"google/gemini-2.5-flash\",\n",
    "        temperature=0.1,\n",
    "        enable_smart_routing=True,\n",
    "        enable_context_bridge=True,\n",
    "        enable_csv_logging=False,\n",
    "        debug_mode=True\n",
    "    )\n",
    "    print(f\"‚úÖ Created OpenRouter config\")\n",
    "    \n",
    "else:\n",
    "    # Fallback to default\n",
    "    gaia_config = GAIAConfig()\n",
    "    print(f\"‚úÖ Using default config\")\n",
    "\n",
    "# Display the configuration\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"   Provider: {gaia_config.model_provider}\")\n",
    "print(f\"   Model: {gaia_config.model_name}\")\n",
    "print(f\"   Temperature: {gaia_config.temperature}\")\n",
    "print(f\"   Smart Routing: {gaia_config.enable_smart_routing}\")\n",
    "print(f\"   Context Bridge: {gaia_config.enable_context_bridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaia agent initialization\n",
    "print(\"üî• TESTING GAIA AGENT INITIALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    print(\"üöÄ Creating GAIAAgent (with SmolagAgent validation)...\")\n",
    "    gaia_agent = GAIAAgent(gaia_config)\n",
    "    \n",
    "    print(\"\\n‚úÖ SUCCESS! GAIA Agent initialized\")\n",
    "    print(\"üéØ All SmolagAgent validations passed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå INITIALIZATION FAILED!\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(f\"Type: {type(e).__name__}\")\n",
    "    print(\"\\nüîç Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    gaia_agent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Test (if initialization succeeded)\n",
    "\n",
    "if 'gaia_agent' in locals() and gaia_agent is not None:\n",
    "    print(\"üß™ QUICK FUNCTIONALITY TEST\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        test_question = \"What is 15% of 200?\"\n",
    "        result = gaia_agent.process_question(test_question, \"test_001\")\n",
    "        \n",
    "        print(f\"‚úÖ Test successful!\")\n",
    "        print(f\"Question: {test_question}\")\n",
    "        print(f\"Answer: {result.get('final_answer', 'No answer')}\")\n",
    "        print(f\"Strategy: {result.get('complexity', 'unknown')}\")\n",
    "        print(f\"Success: {result.get('execution_successful', False)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status Summary\n",
    "\n",
    "print(\"\\nüìä FINAL STATUS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "if 'gaia_agent' in locals() and gaia_agent is not None:\n",
    "    print(\"üéâ STATUS: SUCCESS\")\n",
    "    print(\"‚úÖ GAIA Agent ready for testing\")\n",
    "    print(f\"‚úÖ Provider: {gaia_agent.config.model_provider}\")\n",
    "    print(f\"‚úÖ Model: {gaia_agent.config.model_name}\")\n",
    "else:\n",
    "    print(\"‚ùå STATUS: FAILED\")\n",
    "    print(\"üîç Check error messages above\")\n",
    "    print(\"üí° Common fixes:\")\n",
    "    print(\"   - Update SmolagAgent version\")\n",
    "    print(\"   - Check API keys\")\n",
    "    print(\"   - Review agent_logic.py\")\n",
    "\n",
    "print(f\"üèÅ Test complete at {datetime.now().strftime('%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Testing Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Declaring validation class by importing methods from testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_testing import run_quick_gaia_test, run_gaia_test, compare_agent_configs, run_smart_routing_test\n",
    "\n",
    "class GAIAValidator:\n",
    "    def __init__(self):\n",
    "        self.last_result = None\n",
    "        print(\"üéØ GAIA Validator ready\")\n",
    "    \n",
    "    def quick(self, config=\"groq\", questions=5):\n",
    "        result = run_quick_gaia_test(config, num_questions=questions)\n",
    "        self.last_result = result\n",
    "        if result and 'overall_performance' in result:\n",
    "            acc = result['overall_performance']['accuracy']\n",
    "            print(f\"‚úÖ {acc:.1%} accuracy\")\n",
    "        return result\n",
    "    \n",
    "    def full(self, config=\"groq\", questions=20):\n",
    "        result = run_gaia_test(config, max_questions=questions)\n",
    "        self.last_result = result\n",
    "        if result and 'overall_performance' in result:\n",
    "            acc = result['overall_performance']['accuracy']\n",
    "            total = result['overall_performance']['total_questions']\n",
    "            correct = result['overall_performance']['correct_answers']\n",
    "            print(f\"‚úÖ {acc:.1%} accuracy ({correct}/{total})\")\n",
    "            print(f\"GAIA Target: {'‚úÖ MET' if acc >= 0.45 else '‚ùå NOT MET'}\")\n",
    "        return result\n",
    "    \n",
    "    def compare(self, configs=[\"groq\", \"google\"], questions=10):\n",
    "        result = compare_agent_configs(configs, questions)\n",
    "        self.last_result = result\n",
    "        if result and 'comparison_results' in result:\n",
    "            for config, data in result['comparison_results'].items():\n",
    "                if 'accuracy' in data:\n",
    "                    print(f\"{config}: {data['accuracy']:.1%}\")\n",
    "        return result\n",
    "    \n",
    "    def insights(self):\n",
    "        \"\"\"Enhanced insights with rich execution analysis and direct log links\"\"\"\n",
    "        if not self.last_result or 'overall_performance' not in self.last_result:\n",
    "            print(\"‚ùå No results to analyze\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n‚ú® COMPREHENSIVE INSIGHTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 0. LOG FILE LINKS (NEW!)\n",
    "        self._show_log_file_links()\n",
    "        \n",
    "        # 1. EXECUTION PERFORMANCE ANALYSIS\n",
    "        self._analyze_execution_performance()\n",
    "        \n",
    "        # 2. ACCURACY BREAKDOWN  \n",
    "        self._analyze_accuracy_breakdown()\n",
    "        \n",
    "        # 3. STRATEGY & ROUTING ANALYSIS\n",
    "        self._analyze_strategy_performance()\n",
    "        \n",
    "        # 4. TECHNICAL PERFORMANCE METRICS\n",
    "        self._analyze_technical_performance()\n",
    "        \n",
    "        # 5. ERROR ANALYSIS (if applicable)\n",
    "        self._analyze_errors_and_failures()\n",
    "        \n",
    "        # 6. ACTIONABLE RECOMMENDATIONS\n",
    "        self._generate_actionable_recommendations()\n",
    "    \n",
    "    def _show_log_file_links(self):\n",
    "        \"\"\"Show direct links to log files for detailed analysis\"\"\"\n",
    "        print(f\"üìÅ LOG FILES & DETAILED DATA\")\n",
    "        \n",
    "        # Try to extract log file paths from the result\n",
    "        execution_file = None\n",
    "        evaluation_file = None\n",
    "        \n",
    "        # Look for execution file in various places\n",
    "        if 'execution_file' in self.last_result:\n",
    "            execution_file = self.last_result['execution_file']\n",
    "        elif 'batch_info' in self.last_result:\n",
    "            # Sometimes it's embedded in batch info\n",
    "            pass\n",
    "        \n",
    "        # Look for evaluation file\n",
    "        if 'evaluation_file' in self.last_result:\n",
    "            evaluation_file = self.last_result['evaluation_file']\n",
    "        \n",
    "        # Try to extract from common result patterns\n",
    "        if not execution_file or not evaluation_file:\n",
    "            # Look in the result structure for file paths\n",
    "            results = self.last_result.get('results', [])\n",
    "            if results:\n",
    "                # Check if we can infer file names from timestamps/structure\n",
    "                timestamp = self.last_result.get('evaluation_timestamp', '')\n",
    "                if timestamp:\n",
    "                    # Try to construct likely file paths\n",
    "                    import os\n",
    "                    base_dir = \"./test_results\"\n",
    "                    if os.path.exists(base_dir):\n",
    "                        # Find most recent files\n",
    "                        try:\n",
    "                            import glob\n",
    "                            execution_files = glob.glob(f\"{base_dir}/**/*execution*.json\", recursive=True)\n",
    "                            evaluation_files = glob.glob(f\"{base_dir}/**/*evaluation*.json\", recursive=True)\n",
    "                            \n",
    "                            if execution_files:\n",
    "                                execution_file = max(execution_files, key=os.path.getctime)\n",
    "                            if evaluation_files:\n",
    "                                evaluation_file = max(evaluation_files, key=os.path.getctime)\n",
    "                        except:\n",
    "                            pass\n",
    "        \n",
    "        # Display file links\n",
    "        if execution_file:\n",
    "            print(f\"   üìä Execution Details: {execution_file}\")\n",
    "            if execution_file.startswith('./'):\n",
    "                print(f\"      üí° Open in notebook: pd.read_json('{execution_file}')\")\n",
    "            else:\n",
    "                print(f\"      üí° Open in notebook: pd.read_json('./{execution_file}')\")\n",
    "        \n",
    "        if evaluation_file:\n",
    "            print(f\"   üìà Evaluation Results: {evaluation_file}\")\n",
    "            if evaluation_file.startswith('./'):\n",
    "                print(f\"      üí° Open in notebook: pd.read_json('{evaluation_file}')\")\n",
    "            else:\n",
    "                print(f\"      üí° Open in notebook: pd.read_json('./{evaluation_file}')\")\n",
    "        \n",
    "        # Look for CSV logs (from agent_logging.py)\n",
    "        try:\n",
    "            import glob\n",
    "            import os\n",
    "            \n",
    "            log_dirs = [\"./logs\", \"./test_results/logs\", \"./logs\"]\n",
    "            csv_files = []\n",
    "            \n",
    "            for log_dir in log_dirs:\n",
    "                if os.path.exists(log_dir):\n",
    "                    csv_files.extend(glob.glob(f\"{log_dir}/*steps*.csv\"))\n",
    "                    csv_files.extend(glob.glob(f\"{log_dir}/*questions*.csv\"))\n",
    "                    csv_files.extend(glob.glob(f\"{log_dir}/*evaluation*.csv\"))\n",
    "            \n",
    "            if csv_files:\n",
    "                # Get most recent CSV files\n",
    "                recent_csvs = sorted(csv_files, key=os.path.getctime, reverse=True)[:3]\n",
    "                print(f\"   üìã Recent CSV Logs:\")\n",
    "                for csv_file in recent_csvs:\n",
    "                    file_type = \"Steps\" if \"steps\" in csv_file else \"Questions\" if \"questions\" in csv_file else \"Evaluation\"\n",
    "                    print(f\"      {file_type}: {csv_file}\")\n",
    "                    print(f\"         üí° Open: pd.read_csv('{csv_file}')\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Quick data access commands\n",
    "        print(f\"\\n   üîß QUICK ACCESS COMMANDS:\")\n",
    "        print(f\"      # View raw results structure\")\n",
    "        print(f\"      validator.last_result.keys()\")\n",
    "        print(f\"      \")\n",
    "        print(f\"      # Access specific data\")\n",
    "        print(f\"      validator.last_result['overall_performance']\")\n",
    "        print(f\"      validator.last_result['level_performance']\")\n",
    "        print(f\"      validator.last_result['strategy_analysis']\")\n",
    "        \n",
    "        if not execution_file and not evaluation_file:\n",
    "            print(f\"   ‚ö†Ô∏è  Log files not automatically detected\")\n",
    "            print(f\"      Check ./test_results/ and ./logs/ directories\")\n",
    "        \n",
    "        print()  # Add spacing before next section\n",
    "    \n",
    "    def _analyze_execution_performance(self):\n",
    "        \"\"\"Analyze execution success and performance\"\"\"\n",
    "        overall = self.last_result['overall_performance']\n",
    "        \n",
    "        total = overall['total_questions']\n",
    "        successful = overall['successful_executions']\n",
    "        correct = overall['correct_answers']\n",
    "        accuracy = overall['accuracy']\n",
    "        \n",
    "        print(f\"üîß EXECUTION PERFORMANCE\")\n",
    "        print(f\"   Total Questions: {total}\")\n",
    "        print(f\"   Successful Executions: {successful}/{total} ({successful/total:.1%})\")\n",
    "        print(f\"   Correct Answers: {correct}/{total} ({accuracy:.1%})\")\n",
    "        \n",
    "        # Execution success analysis\n",
    "        if successful == total:\n",
    "            print(f\"   ‚úÖ Perfect execution success - no technical failures\")\n",
    "        elif successful >= total * 0.9:\n",
    "            print(f\"   ‚úÖ Good execution success - minimal technical issues\")\n",
    "        elif successful >= total * 0.7:\n",
    "            print(f\"   ‚ö†Ô∏è  Moderate execution issues - {total - successful} failures\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Significant execution problems - {total - successful} failures\")\n",
    "        \n",
    "        # Answer quality vs execution success\n",
    "        if successful > 0:\n",
    "            answer_quality = correct / successful\n",
    "            print(f\"   üìä Answer Quality (when executed): {answer_quality:.1%}\")\n",
    "            \n",
    "            if answer_quality >= 0.6:\n",
    "                print(f\"   üéØ High answer quality - agent reasoning is strong\")\n",
    "            elif answer_quality >= 0.3:\n",
    "                print(f\"   üìà Moderate answer quality - room for improvement\")\n",
    "            else:\n",
    "                print(f\"   üìâ Low answer quality - needs significant work\")\n",
    "    \n",
    "    def _analyze_accuracy_breakdown(self):\n",
    "        \"\"\"Detailed accuracy analysis\"\"\"\n",
    "        overall = self.last_result['overall_performance']\n",
    "        accuracy = overall['accuracy']\n",
    "        \n",
    "        print(f\"\\nüéØ ACCURACY ANALYSIS\")\n",
    "        print(f\"   Overall Accuracy: {accuracy:.1%}\")\n",
    "        \n",
    "        # GAIA benchmark context\n",
    "        if accuracy >= 0.70:\n",
    "            print(f\"   üèÜ EXCEPTIONAL - Top-tier performance!\")\n",
    "            benchmark_status = \"exceptional\"\n",
    "        elif accuracy >= 0.60:\n",
    "            print(f\"   üåü EXCELLENT - Competitive performance\")\n",
    "            benchmark_status = \"excellent\"\n",
    "        elif accuracy >= 0.45:\n",
    "            print(f\"   ‚úÖ GOOD - Above GAIA 45% threshold\")\n",
    "            benchmark_status = \"good\"\n",
    "        elif accuracy >= 0.30:\n",
    "            print(f\"   ‚ö†Ô∏è  FAIR - Below GAIA threshold, needs improvement\")\n",
    "            benchmark_status = \"fair\"\n",
    "        else:\n",
    "            print(f\"   ‚ùå POOR - Significant improvements needed\")\n",
    "            benchmark_status = \"poor\"\n",
    "        \n",
    "        # Level breakdown with insights\n",
    "        levels = self.last_result.get('level_performance', {})\n",
    "        if levels:\n",
    "            print(f\"\\n   üìà PERFORMANCE BY LEVEL:\")\n",
    "            for level in sorted(levels.keys()):\n",
    "                perf = levels[level]\n",
    "                acc = perf['accuracy']\n",
    "                correct = perf['correct']\n",
    "                total = perf['total']\n",
    "                \n",
    "                print(f\"      Level {level}: {acc:.1%} ({correct}/{total})\")\n",
    "                \n",
    "                # Level-specific insights\n",
    "                if level == '1':\n",
    "                    if acc >= 0.8:\n",
    "                        print(f\"         ‚úÖ Excellent basic capability\")\n",
    "                    elif acc >= 0.6:\n",
    "                        print(f\"         üìà Good basic capability\")\n",
    "                    else:\n",
    "                        print(f\"         ‚ö†Ô∏è  Basic capabilities need work\")\n",
    "                        \n",
    "                elif level == '2':\n",
    "                    if acc >= 0.5:\n",
    "                        print(f\"         ‚úÖ Strong intermediate reasoning\")\n",
    "                    elif acc >= 0.3:\n",
    "                        print(f\"         üìà Developing intermediate skills\")\n",
    "                    else:\n",
    "                        print(f\"         üìâ Intermediate reasoning struggles\")\n",
    "                        \n",
    "                elif level == '3':\n",
    "                    if acc >= 0.3:\n",
    "                        print(f\"         üéØ Impressive advanced reasoning!\")\n",
    "                    elif acc >= 0.1:\n",
    "                        print(f\"         üìà Some advanced capability\")\n",
    "                    else:\n",
    "                        print(f\"         üî¨ Advanced reasoning very challenging\")\n",
    "    \n",
    "    def _analyze_strategy_performance(self):\n",
    "        \"\"\"Analyze routing and strategy effectiveness\"\"\"\n",
    "        \n",
    "        # Strategy analysis\n",
    "        strategy_analysis = self.last_result.get('strategy_analysis', {})\n",
    "        if strategy_analysis:\n",
    "            print(f\"\\nüõ§Ô∏è  STRATEGY & ROUTING ANALYSIS\")\n",
    "            \n",
    "            total_strategies = sum(s.get('total_questions', 0) for s in strategy_analysis.values())\n",
    "            \n",
    "            for strategy, stats in strategy_analysis.items():\n",
    "                acc = stats.get('accuracy', 0)\n",
    "                count = stats.get('total_questions', 0)\n",
    "                percentage = (count / total_strategies * 100) if total_strategies > 0 else 0\n",
    "                \n",
    "                print(f\"   {strategy}: {acc:.1%} accuracy ({count}q, {percentage:.0f}%)\")\n",
    "                \n",
    "                # Strategy-specific insights\n",
    "                if 'one_shot' in strategy.lower():\n",
    "                    if acc >= 0.7:\n",
    "                        print(f\"      ‚úÖ Excellent simple question handling\")\n",
    "                    elif acc >= 0.5:\n",
    "                        print(f\"      üìà Good simple question performance\") \n",
    "                    else:\n",
    "                        print(f\"      ‚ö†Ô∏è  Simple questions underperforming\")\n",
    "                        \n",
    "                elif 'manager' in strategy.lower() or 'coordination' in strategy.lower():\n",
    "                    if acc >= 0.4:\n",
    "                        print(f\"      ‚úÖ Strong complex reasoning\")\n",
    "                    elif acc >= 0.2:\n",
    "                        print(f\"      üìà Developing complex capabilities\")\n",
    "                    else:\n",
    "                        print(f\"      üîß Complex coordination needs work\")\n",
    "        \n",
    "        # Hybrid state metrics\n",
    "        hybrid_metrics = self.last_result.get('hybrid_state_metrics', {})\n",
    "        if hybrid_metrics:\n",
    "            print(f\"\\nüåâ HYBRID STATE PERFORMANCE\")\n",
    "            \n",
    "            context_usage = hybrid_metrics.get('context_bridge_usage', {})\n",
    "            if context_usage:\n",
    "                usage_pct = context_usage.get('usage_percentage', 0)\n",
    "                total_questions = context_usage.get('total_questions', 0)\n",
    "                bridge_used = context_usage.get('context_bridge_used', 0)\n",
    "                \n",
    "                print(f\"   Context Bridge: {usage_pct:.1%} usage ({bridge_used}/{total_questions})\")\n",
    "                \n",
    "                if usage_pct >= 0.9:\n",
    "                    print(f\"      ‚úÖ Excellent hybrid state integration\")\n",
    "                elif usage_pct >= 0.7:\n",
    "                    print(f\"      üìà Good hybrid state usage\") \n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è  Inconsistent hybrid state integration\")\n",
    "            \n",
    "            avg_time = hybrid_metrics.get('average_execution_time', 0)\n",
    "            if avg_time > 0:\n",
    "                print(f\"   Avg Execution Time: {avg_time:.1f}s per question\")\n",
    "                \n",
    "                if avg_time <= 15:\n",
    "                    print(f\"      ‚ö° Fast execution - excellent efficiency\")\n",
    "                elif avg_time <= 30:\n",
    "                    print(f\"      üìà Reasonable execution speed\")\n",
    "                elif avg_time <= 60:\n",
    "                    print(f\"      ‚è±Ô∏è  Slow execution - consider optimization\")\n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è  Very slow execution - needs optimization\")\n",
    "    \n",
    "    def _analyze_technical_performance(self):\n",
    "        \"\"\"Analyze technical execution metrics\"\"\"\n",
    "        \n",
    "        # Look for execution metadata in results\n",
    "        results = self.last_result.get('results', [])\n",
    "        if not results:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è  TECHNICAL PERFORMANCE\")\n",
    "        \n",
    "        # Execution time analysis\n",
    "        execution_times = [r.get('execution_time', 0) for r in results if r.get('execution_time')]\n",
    "        if execution_times:\n",
    "            avg_time = sum(execution_times) / len(execution_times)\n",
    "            max_time = max(execution_times)\n",
    "            min_time = min(execution_times)\n",
    "            \n",
    "            print(f\"   Execution Times:\")\n",
    "            print(f\"      Average: {avg_time:.1f}s\")\n",
    "            print(f\"      Range: {min_time:.1f}s - {max_time:.1f}s\")\n",
    "            \n",
    "            if max_time > avg_time * 3:\n",
    "                print(f\"      ‚ö†Ô∏è  High variability - some questions much slower\")\n",
    "        \n",
    "        # Step analysis\n",
    "        step_counts = [r.get('total_steps', 0) for r in results if r.get('total_steps')]\n",
    "        if step_counts:\n",
    "            avg_steps = sum(step_counts) / len(step_counts)\n",
    "            max_steps = max(step_counts)\n",
    "            \n",
    "            print(f\"   Step Counts:\")\n",
    "            print(f\"      Average: {avg_steps:.1f} steps\")\n",
    "            print(f\"      Maximum: {max_steps} steps\")\n",
    "            \n",
    "            if avg_steps <= 5:\n",
    "                print(f\"      ‚ö° Efficient processing - low step count\")\n",
    "            elif avg_steps <= 10:\n",
    "                print(f\"      üìà Moderate complexity processing\")\n",
    "            else:\n",
    "                print(f\"      üîß High step count - may need optimization\")\n",
    "        \n",
    "        # File processing analysis\n",
    "        file_questions = [r for r in results if r.get('has_file', False)]\n",
    "        if file_questions:\n",
    "            file_success = sum(1 for r in file_questions if r.get('execution_successful', False))\n",
    "            print(f\"   File Processing:\")\n",
    "            print(f\"      File questions: {len(file_questions)}\")\n",
    "            print(f\"      File success rate: {file_success/len(file_questions):.1%}\")\n",
    "    \n",
    "    def _analyze_errors_and_failures(self):\n",
    "        \"\"\"Analyze errors and execution failures\"\"\"\n",
    "        \n",
    "        results = self.last_result.get('results', [])\n",
    "        failed_results = [r for r in results if not r.get('execution_successful', True)]\n",
    "        error_results = [r for r in results if r.get('error') or 'ERROR' in r.get('final_answer', '')]\n",
    "        \n",
    "        if failed_results or error_results:\n",
    "            print(f\"\\nüêõ ERROR ANALYSIS\")\n",
    "            \n",
    "            if failed_results:\n",
    "                print(f\"   Execution Failures: {len(failed_results)}\")\n",
    "                \n",
    "                # Categorize error types\n",
    "                error_types = {}\n",
    "                for result in failed_results:\n",
    "                    error_type = result.get('error_type', 'unknown')\n",
    "                    error_types[error_type] = error_types.get(error_type, 0) + 1\n",
    "                \n",
    "                print(f\"   Error Types:\")\n",
    "                for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"      {error_type}: {count} occurrences\")\n",
    "            \n",
    "            if error_results:\n",
    "                print(f\"   Answer Errors: {len(error_results)}\")\n",
    "                \n",
    "                # Show sample error messages\n",
    "                sample_errors = [r.get('final_answer', '')[:100] for r in error_results[:3]]\n",
    "                for i, error in enumerate(sample_errors, 1):\n",
    "                    if error:\n",
    "                        print(f\"      Sample {i}: {error}...\")\n",
    "        \n",
    "        # Check for evaluation errors (like the 'dict' object has no attribute 'strip')\n",
    "        eval_errors = [r for r in results if r.get('evaluation_error')]\n",
    "        if eval_errors:\n",
    "            print(f\"\\nüîç EVALUATION ERRORS\")\n",
    "            print(f\"   Evaluation issues: {len(eval_errors)}\")\n",
    "            \n",
    "            # Get unique error types\n",
    "            unique_errors = set(r.get('evaluation_error', '') for r in eval_errors)\n",
    "            for error in unique_errors:\n",
    "                if error:\n",
    "                    print(f\"      {error}\")\n",
    "    \n",
    "    def _generate_actionable_recommendations(self):\n",
    "        \"\"\"Generate specific, actionable recommendations\"\"\"\n",
    "        \n",
    "        overall = self.last_result['overall_performance']\n",
    "        accuracy = overall['accuracy']\n",
    "        successful = overall['successful_executions']\n",
    "        total = overall['total_questions']\n",
    "        \n",
    "        print(f\"\\nüí° ACTIONABLE RECOMMENDATIONS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Accuracy-based recommendations\n",
    "        if accuracy < 0.20:\n",
    "            recommendations.extend([\n",
    "                \"üéØ CRITICAL: Focus on basic functionality - accuracy very low\",\n",
    "                \"üîß Check agent configuration and tool integration\",\n",
    "                \"üìö Verify GAIA answer formatting is working correctly\"\n",
    "            ])\n",
    "        elif accuracy < 0.45:\n",
    "            recommendations.extend([\n",
    "                \"üéØ PRIMARY: Work toward GAIA 45% threshold\",\n",
    "                \"üìà Focus on Level 1 questions first - build foundation\",\n",
    "                \"üîß Optimize basic reasoning and tool usage\"\n",
    "            ])\n",
    "        elif accuracy < 0.60:\n",
    "            recommendations.extend([\n",
    "                \"üéØ OPTIMIZE: Good performance - push toward excellence\",\n",
    "                \"üìà Level 2 questions are the key improvement area\",\n",
    "                \"‚ö° Consider model or prompt optimization\"\n",
    "            ])\n",
    "        else:\n",
    "            recommendations.extend([\n",
    "                \"üèÜ EXCELLENT: Maintain and monitor performance\",\n",
    "                \"üìä Test with larger question sets to validate\",\n",
    "                \"üî¨ Push boundaries with Level 3 questions\"\n",
    "            ])\n",
    "        \n",
    "        # Execution-based recommendations\n",
    "        execution_rate = successful / total if total > 0 else 0\n",
    "        if execution_rate < 0.9:\n",
    "            recommendations.append(\"üîß CRITICAL: Fix execution failures before optimizing accuracy\")\n",
    "        \n",
    "        # Strategy-based recommendations\n",
    "        strategy_analysis = self.last_result.get('strategy_analysis', {})\n",
    "        if strategy_analysis:\n",
    "            one_shot_performance = 0\n",
    "            manager_performance = 0\n",
    "            \n",
    "            for strategy, stats in strategy_analysis.items():\n",
    "                if 'one_shot' in strategy.lower():\n",
    "                    one_shot_performance = stats.get('accuracy', 0)\n",
    "                elif 'manager' in strategy.lower():\n",
    "                    manager_performance = stats.get('accuracy', 0)\n",
    "            \n",
    "            if one_shot_performance > 0 and manager_performance > 0:\n",
    "                if one_shot_performance > manager_performance * 1.5:\n",
    "                    recommendations.append(\"üõ§Ô∏è  ROUTING: Consider routing more questions to one-shot LLM\")\n",
    "                elif manager_performance > one_shot_performance * 1.5:\n",
    "                    recommendations.append(\"üõ§Ô∏è  ROUTING: Manager coordination is more effective\")\n",
    "        \n",
    "        # Technical recommendations\n",
    "        hybrid_metrics = self.last_result.get('hybrid_state_metrics', {})\n",
    "        if hybrid_metrics:\n",
    "            avg_time = hybrid_metrics.get('average_execution_time', 0)\n",
    "            if avg_time > 30:\n",
    "                recommendations.append(\"‚ö° PERFORMANCE: Optimize execution speed - currently slow\")\n",
    "            \n",
    "            context_usage = hybrid_metrics.get('context_bridge_usage', {})\n",
    "            usage_pct = context_usage.get('usage_percentage', 0)\n",
    "            if usage_pct < 0.8:\n",
    "                recommendations.append(\"üåâ INTEGRATION: Improve context bridge consistency\")\n",
    "        \n",
    "        # Error-specific recommendations\n",
    "        results = self.last_result.get('results', [])\n",
    "        eval_errors = [r for r in results if r.get('evaluation_error')]\n",
    "        if eval_errors:\n",
    "            recommendations.append(\"üîç CRITICAL: Fix evaluation errors preventing accurate assessment\")\n",
    "        \n",
    "        # Display recommendations\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        \n",
    "        if not recommendations:\n",
    "            print(\"   üéâ No specific issues detected - excellent performance!\")\n",
    "        \n",
    "        # Final priority guidance\n",
    "        print(f\"\\nüöÄ PRIORITY FOCUS:\")\n",
    "        if execution_rate < 0.9:\n",
    "            print(\"   1. Fix execution failures first\")\n",
    "            print(\"   2. Then focus on accuracy improvement\")\n",
    "        elif accuracy < 0.45:\n",
    "            print(\"   1. Reach GAIA 45% threshold\")\n",
    "            print(\"   2. Level 1 questions are your foundation\")\n",
    "        elif accuracy < 0.60:\n",
    "            print(\"   1. Optimize Level 2 performance\")\n",
    "            print(\"   2. Fine-tune routing and strategies\")\n",
    "        else:\n",
    "            print(\"   1. Maintain excellent performance\")\n",
    "            print(\"   2. Scale testing and monitor consistency\")\n",
    "\n",
    "validator = GAIAValidator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# GAIA Testing Methods Reference\n",
    "\n",
    "## Validator Methods\n",
    "\n",
    "### `validator.quick(config, questions)`\n",
    "**Quick validation test - perfect for development**\n",
    "- `config` (str, default=\"groq\"): Agent configuration \n",
    "  - Options: `\"groq\"`, `\"google\"`, `\"openrouter\"`, `\"ollama\"`, `\"performance\"`, `\"accuracy\"`\n",
    "- `questions` (int, default=5): Number of questions to test\n",
    "- **Returns**: Test results dict\n",
    "- **Use case**: Fast iteration during development\n",
    "\n",
    "```python\n",
    "result = validator.quick('groq', 5)\n",
    "```\n",
    "\n",
    "### `validator.full(config, questions)`\n",
    "**Comprehensive test - for production validation**\n",
    "- `config` (str, default=\"groq\"): Agent configuration\n",
    "- `questions` (int, default=20): Number of questions (20+ recommended for reliable results)\n",
    "- **Returns**: Complete test results with level breakdown\n",
    "- **Use case**: Final validation before deployment\n",
    "\n",
    "```python\n",
    "result = validator.full('groq', 20)\n",
    "```\n",
    "\n",
    "### `validator.compare(configs, questions)`\n",
    "**Compare multiple configurations**\n",
    "- `configs` (list, default=[\"groq\", \"google\"]): List of configurations to compare\n",
    "- `questions` (int, default=10): Questions per configuration\n",
    "- **Returns**: Comparison results with rankings\n",
    "- **Use case**: Choosing the best configuration\n",
    "\n",
    "```python\n",
    "result = validator.compare(['groq', 'google', 'performance'], 10)\n",
    "```\n",
    "\n",
    "### `validator.insights()`\n",
    "**Analyze last test results**\n",
    "- **No parameters**\n",
    "- **Returns**: None (prints analysis)\n",
    "- **Use case**: Get actionable recommendations after any test\n",
    "\n",
    "```python\n",
    "validator.insights()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying Testing Functions\n",
    "\n",
    "### `run_quick_gaia_test(agent_config_name, **kwargs)`\n",
    "**Direct access to quick testing**\n",
    "- `agent_config_name` (str): Configuration name\n",
    "- `num_questions` (int, default=5): Number of questions\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- **Returns**: Evaluation results dict\n",
    "\n",
    "### `run_gaia_test(agent_config_name, dataset_path, max_questions, test_config)`\n",
    "**Complete GAIA test workflow**\n",
    "- `agent_config_name` (str, default=\"groq\"): Configuration name\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- `max_questions` (int, default=20): Maximum questions to test\n",
    "- `test_config` (GAIATestConfig, optional): Advanced test configuration\n",
    "- **Returns**: Complete evaluation results\n",
    "\n",
    "### `compare_agent_configs(config_names, num_questions, dataset_path)`\n",
    "**Compare multiple agent configurations**\n",
    "- `config_names` (List[str]): List of configuration names\n",
    "- `num_questions` (int, default=10): Questions per configuration\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- **Returns**: Comparison results dict\n",
    "\n",
    "### `run_smart_routing_test(agent_config_name, num_questions)`\n",
    "**Test smart routing effectiveness**\n",
    "- `agent_config_name` (str, default=\"performance\"): Configuration name\n",
    "- `num_questions` (int, default=15): Number of questions for analysis\n",
    "- **Returns**: Routing analysis results\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Available Configurations\n",
    "| Config Name | Provider | Model | Use Case |\n",
    "|-------------|----------|-------|----------|\n",
    "| `\"groq\"` | Groq | qwen-qwq-32b | Fast, reliable |\n",
    "| `\"google\"` | Google | gemini-2.0-flash-preview | Balanced performance |\n",
    "| `\"openrouter\"` | OpenRouter | qwen/qwen-2.5-coder-32b-instruct:free | Cost-effective |\n",
    "| `\"ollama\"` | Ollama | qwen2.5-coder:32b | Local deployment |\n",
    "| `\"performance\"` | Groq | qwen-qwq-32b | Optimized for speed |\n",
    "| `\"accuracy\"` | Google | gemini-2.0-flash-preview | Optimized for accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "## Test Result Structure\n",
    "\n",
    "### Standard Result Format\n",
    "```python\n",
    "{\n",
    "    \"overall_performance\": {\n",
    "        \"total_questions\": 20,\n",
    "        \"correct_answers\": 11,\n",
    "        \"accuracy\": 0.55,\n",
    "        \"successful_executions\": 19\n",
    "    },\n",
    "    \"level_performance\": {\n",
    "        \"1\": {\"accuracy\": 0.70, \"correct\": 7, \"total\": 10},\n",
    "        \"2\": {\"accuracy\": 0.44, \"correct\": 4, \"total\": 9},\n",
    "        \"3\": {\"accuracy\": 0.0, \"correct\": 0, \"total\": 1}\n",
    "    },\n",
    "    \"strategy_analysis\": {\n",
    "        \"one_shot_llm\": {\"accuracy\": 0.67, \"total_questions\": 12},\n",
    "        \"manager_coordination\": {\"accuracy\": 0.38, \"total_questions\": 8}\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Comparison Result Format\n",
    "```python\n",
    "{\n",
    "    \"comparison_results\": {\n",
    "        \"groq\": {\"accuracy\": 0.60, \"correct_answers\": 6, \"total_questions\": 10},\n",
    "        \"google\": {\"accuracy\": 0.50, \"correct_answers\": 5, \"total_questions\": 10}\n",
    "    },\n",
    "    \"timestamp\": \"2024-12-19T10:30:00\",\n",
    "    \"test_questions\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Workflow\n",
    "```python\n",
    "# 1. Quick test\n",
    "result = validator.quick('groq', 5)\n",
    "validator.insights()\n",
    "\n",
    "# 2. Full validation  \n",
    "result = validator.full('groq', 20)\n",
    "validator.insights()\n",
    "```\n",
    "\n",
    "### Configuration Comparison\n",
    "```python\n",
    "# Compare multiple configs\n",
    "result = validator.compare(['groq', 'google', 'performance'], 10)\n",
    "validator.insights()\n",
    "```\n",
    "\n",
    "### Advanced Testing\n",
    "```python\n",
    "# Direct function access for custom workflows\n",
    "from agent_testing import run_gaia_test, analyze_failure_patterns\n",
    "\n",
    "result = run_gaia_test('groq', max_questions=50)\n",
    "failure_analysis = analyze_failure_patterns(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Benchmarks\n",
    "\n",
    "### GAIA Accuracy Targets\n",
    "- **45%+**: GAIA benchmark threshold\n",
    "- **50-60%**: Competitive performance\n",
    "- **60%+**: Excellent performance\n",
    "\n",
    "### Execution Time Guidelines\n",
    "- **Quick test (5q)**: ~1-2 minutes\n",
    "- **Full test (20q)**: ~5-8 minutes  \n",
    "- **Comparison (3 configs, 10q each)**: ~10-15 minutes\n",
    "\n",
    "### Recommended Question Counts\n",
    "- **Development**: 5 questions (quick feedback)\n",
    "- **Validation**: 20 questions (reliable results)\n",
    "- **Production**: 50+ questions (comprehensive assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# GAIA Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Run a quick test batch\n",
    "### `validator.quick(config, # questions)`\n",
    "**Quick validation test - perfect for development**\n",
    "- `config` (str, default=\"groq\"): Agent configuration \n",
    "  - Options: `\"groq\"`, `\"google\"`, `\"openrouter\"`, `\"ollama\"`, `\"performance\"`, `\"accuracy\"`\n",
    "- `questions` (int, default=5): Number of questions to test\n",
    "- **Returns**: Test results dict\n",
    "- **Use case**: Fast iteration during development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Evaluation results for the quick test\n",
    "\n",
    "## Underlying Testing Functions\n",
    "\n",
    "### `run_quick_gaia_test(agent_config_name, **kwargs)`\n",
    "**Direct access to quick testing**\n",
    "- `agent_config_name` (str): Configuration name\n",
    "- `num_questions` (int, default=5): Number of questions\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- **Returns**: Evaluation results dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß COMPLETE VALIDATOR FIX WITH ENHANCED INSIGHTS\n",
    "# This will fix your validator and add all enhanced functionality\n",
    "\n",
    "print(\"üöÄ COMPLETE VALIDATOR FIX - ENHANCED INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, let's check what we're working with\n",
    "print(\"üîç CURRENT STATUS CHECK:\")\n",
    "if 'validator' in globals():\n",
    "    print(f\"‚úÖ Validator exists: {type(validator)}\")\n",
    "    if hasattr(validator, 'last_result'):\n",
    "        print(f\"üìä last_result type: {type(validator.last_result)}\")\n",
    "        if validator.last_result is not None:\n",
    "            print(f\"‚úÖ Has test results\")\n",
    "        else:\n",
    "            print(\"‚ùå last_result is None - need to run test\")\n",
    "    else:\n",
    "        print(\"‚ùå No last_result attribute\")\n",
    "else:\n",
    "    print(\"‚ùå No validator found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# ENHANCED INSIGHTS METHODS - All in one place\n",
    "def enhanced_insights(self):\n",
    "    \"\"\"Enhanced insights with rich execution analysis and direct log links\"\"\"\n",
    "    if not hasattr(self, 'last_result') or not self.last_result or 'overall_performance' not in self.last_result:\n",
    "        print(\"‚ùå No results to analyze\")\n",
    "        print(f\"Debug: last_result = {getattr(self, 'last_result', 'MISSING ATTRIBUTE')}\")\n",
    "        if hasattr(self, 'last_result') and self.last_result:\n",
    "            print(f\"Debug: last_result type = {type(self.last_result)}\")\n",
    "            if isinstance(self.last_result, dict):\n",
    "                print(f\"Debug: last_result keys = {list(self.last_result.keys())}\")\n",
    "        \n",
    "        print(\"\\nüí° TO FIX THIS:\")\n",
    "        print(\"1. Run a test first:\")\n",
    "        print(\"   result = validator.quick('groq', 5)\")\n",
    "        print(\"2. Then try enhanced insights:\")\n",
    "        print(\"   validator.enhanced_insights()\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚ú® COMPREHENSIVE INSIGHTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 0. LOG FILE LINKS\n",
    "    self._show_log_file_links()\n",
    "    \n",
    "    # 1. EXECUTION PERFORMANCE ANALYSIS\n",
    "    self._analyze_execution_performance()\n",
    "    \n",
    "    # 2. ACCURACY BREAKDOWN  \n",
    "    self._analyze_accuracy_breakdown()\n",
    "    \n",
    "    # 3. STRATEGY & ROUTING ANALYSIS\n",
    "    self._analyze_strategy_performance()\n",
    "    \n",
    "    # 4. TECHNICAL PERFORMANCE METRICS\n",
    "    self._analyze_technical_performance()\n",
    "    \n",
    "    # 5. ERROR ANALYSIS (if applicable)\n",
    "    self._analyze_errors_and_failures()\n",
    "    \n",
    "    # 6. ACTIONABLE RECOMMENDATIONS\n",
    "    self._generate_actionable_recommendations()\n",
    "\n",
    "def _show_log_file_links(self):\n",
    "    \"\"\"Show direct links to log files for detailed analysis\"\"\"\n",
    "    print(f\"üìÅ LOG FILES & DETAILED DATA\")\n",
    "    \n",
    "    # Try to extract log file paths from the result\n",
    "    execution_file = None\n",
    "    evaluation_file = None\n",
    "    \n",
    "    # Look for execution file in various places\n",
    "    if 'execution_file' in self.last_result:\n",
    "        execution_file = self.last_result['execution_file']\n",
    "    \n",
    "    # Try to find recent log files\n",
    "    try:\n",
    "        import glob\n",
    "        import os\n",
    "        \n",
    "        # Look for recent execution and evaluation files\n",
    "        base_dirs = [\"./test_results\", \"./test_results/logs\", \"./logs\"]\n",
    "        \n",
    "        for base_dir in base_dirs:\n",
    "            if os.path.exists(base_dir):\n",
    "                exec_files = glob.glob(f\"{base_dir}/**/*execution*.json\", recursive=True)\n",
    "                eval_files = glob.glob(f\"{base_dir}/**/*evaluation*.json\", recursive=True)\n",
    "                \n",
    "                if exec_files and not execution_file:\n",
    "                    execution_file = max(exec_files, key=os.path.getctime)\n",
    "                if eval_files and not evaluation_file:\n",
    "                    evaluation_file = max(eval_files, key=os.path.getctime)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Display file links\n",
    "    if execution_file:\n",
    "        print(f\"   üìä Execution Details: {execution_file}\")\n",
    "        print(f\"      üí° Open in notebook: pd.read_json('{execution_file}')\")\n",
    "    \n",
    "    if evaluation_file:\n",
    "        print(f\"   üìà Evaluation Results: {evaluation_file}\")\n",
    "        print(f\"      üí° Open in notebook: pd.read_json('{evaluation_file}')\")\n",
    "    \n",
    "    # Look for CSV logs\n",
    "    try:\n",
    "        csv_files = []\n",
    "        for base_dir in [\"./logs\", \"./test_results/logs\"]:\n",
    "            if os.path.exists(base_dir):\n",
    "                csv_files.extend(glob.glob(f\"{base_dir}/*steps*.csv\"))\n",
    "                csv_files.extend(glob.glob(f\"{base_dir}/*questions*.csv\"))\n",
    "                csv_files.extend(glob.glob(f\"{base_dir}/*evaluation*.csv\"))\n",
    "        \n",
    "        if csv_files:\n",
    "            recent_csvs = sorted(csv_files, key=os.path.getctime, reverse=True)[:3]\n",
    "            print(f\"   üìã Recent CSV Logs:\")\n",
    "            for csv_file in recent_csvs:\n",
    "                file_type = \"Steps\" if \"steps\" in csv_file else \"Questions\" if \"questions\" in csv_file else \"Evaluation\"\n",
    "                print(f\"      {file_type}: {csv_file}\")\n",
    "                print(f\"         üí° Open: pd.read_csv('{csv_file}')\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Quick data access commands\n",
    "    print(f\"\\n   üîß QUICK ACCESS COMMANDS:\")\n",
    "    print(f\"      # View raw results structure\")\n",
    "    print(f\"      validator.last_result.keys()\")\n",
    "    print(f\"      # Access specific data\")\n",
    "    print(f\"      validator.last_result['overall_performance']\")\n",
    "    \n",
    "    if not execution_file and not evaluation_file:\n",
    "        print(f\"   ‚ö†Ô∏è  Log files not automatically detected\")\n",
    "        print(f\"      Check ./test_results/ and ./logs/ directories\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def _analyze_execution_performance(self):\n",
    "    \"\"\"Analyze execution success and performance\"\"\"\n",
    "    overall = self.last_result['overall_performance']\n",
    "    \n",
    "    total = overall['total_questions']\n",
    "    successful = overall['successful_executions']\n",
    "    correct = overall['correct_answers']\n",
    "    accuracy = overall['accuracy']\n",
    "    \n",
    "    print(f\"üîß EXECUTION PERFORMANCE\")\n",
    "    print(f\"   Total Questions: {total}\")\n",
    "    print(f\"   Successful Executions: {successful}/{total} ({successful/total:.1%})\")\n",
    "    print(f\"   Correct Answers: {correct}/{total} ({accuracy:.1%})\")\n",
    "    \n",
    "    # Execution success analysis\n",
    "    if successful == total:\n",
    "        print(f\"   ‚úÖ Perfect execution success - no technical failures\")\n",
    "    elif successful >= total * 0.9:\n",
    "        print(f\"   ‚úÖ Good execution success - minimal technical issues\")\n",
    "    elif successful >= total * 0.7:\n",
    "        print(f\"   ‚ö†Ô∏è  Moderate execution issues - {total - successful} failures\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Significant execution problems - {total - successful} failures\")\n",
    "    \n",
    "    # Answer quality vs execution success\n",
    "    if successful > 0:\n",
    "        answer_quality = correct / successful\n",
    "        print(f\"   üìä Answer Quality (when executed): {answer_quality:.1%}\")\n",
    "        \n",
    "        if answer_quality >= 0.6:\n",
    "            print(f\"   üéØ High answer quality - agent reasoning is strong\")\n",
    "        elif answer_quality >= 0.3:\n",
    "            print(f\"   üìà Moderate answer quality - room for improvement\")\n",
    "        else:\n",
    "            print(f\"   üìâ Low answer quality - needs significant work\")\n",
    "\n",
    "def _analyze_accuracy_breakdown(self):\n",
    "    \"\"\"Detailed accuracy analysis\"\"\"\n",
    "    overall = self.last_result['overall_performance']\n",
    "    accuracy = overall['accuracy']\n",
    "    \n",
    "    print(f\"\\nüéØ ACCURACY ANALYSIS\")\n",
    "    print(f\"   Overall Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    # GAIA benchmark context\n",
    "    if accuracy >= 0.70:\n",
    "        print(f\"   üèÜ EXCEPTIONAL - Top-tier performance!\")\n",
    "    elif accuracy >= 0.60:\n",
    "        print(f\"   üåü EXCELLENT - Competitive performance\")\n",
    "    elif accuracy >= 0.45:\n",
    "        print(f\"   ‚úÖ GOOD - Above GAIA 45% threshold\")\n",
    "    elif accuracy >= 0.30:\n",
    "        print(f\"   ‚ö†Ô∏è  FAIR - Below GAIA threshold, needs improvement\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå POOR - Significant improvements needed\")\n",
    "    \n",
    "    # Level breakdown\n",
    "    levels = self.last_result.get('level_performance', {})\n",
    "    if levels:\n",
    "        print(f\"\\n   üìà PERFORMANCE BY LEVEL:\")\n",
    "        for level in sorted(levels.keys()):\n",
    "            perf = levels[level]\n",
    "            acc = perf['accuracy']\n",
    "            correct = perf['correct']\n",
    "            total = perf['total']\n",
    "            print(f\"      Level {level}: {acc:.1%} ({correct}/{total})\")\n",
    "\n",
    "def _analyze_strategy_performance(self):\n",
    "    \"\"\"Analyze routing and strategy effectiveness\"\"\"\n",
    "    strategy_analysis = self.last_result.get('strategy_analysis', {})\n",
    "    if strategy_analysis:\n",
    "        print(f\"\\nüõ§Ô∏è  STRATEGY & ROUTING ANALYSIS\")\n",
    "        \n",
    "        total_strategies = sum(s.get('total_questions', 0) for s in strategy_analysis.values())\n",
    "        \n",
    "        for strategy, stats in strategy_analysis.items():\n",
    "            acc = stats.get('accuracy', 0)\n",
    "            count = stats.get('total_questions', 0)\n",
    "            percentage = (count / total_strategies * 100) if total_strategies > 0 else 0\n",
    "            print(f\"   {strategy}: {acc:.1%} accuracy ({count}q, {percentage:.0f}%)\")\n",
    "    \n",
    "    # Hybrid state metrics\n",
    "    hybrid_metrics = self.last_result.get('hybrid_state_metrics', {})\n",
    "    if hybrid_metrics:\n",
    "        print(f\"\\nüåâ HYBRID STATE PERFORMANCE\")\n",
    "        \n",
    "        context_usage = hybrid_metrics.get('context_bridge_usage', {})\n",
    "        if context_usage:\n",
    "            usage_pct = context_usage.get('usage_percentage', 0)\n",
    "            print(f\"   Context Bridge: {usage_pct:.1%} usage\")\n",
    "        \n",
    "        avg_time = hybrid_metrics.get('average_execution_time', 0)\n",
    "        if avg_time > 0:\n",
    "            print(f\"   Avg Execution Time: {avg_time:.1f}s per question\")\n",
    "\n",
    "def _analyze_technical_performance(self):\n",
    "    \"\"\"Analyze technical execution metrics\"\"\"\n",
    "    results = self.last_result.get('results', [])\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è  TECHNICAL PERFORMANCE\")\n",
    "    \n",
    "    # Execution time analysis\n",
    "    execution_times = [r.get('execution_time', 0) for r in results if r.get('execution_time')]\n",
    "    if execution_times:\n",
    "        avg_time = sum(execution_times) / len(execution_times)\n",
    "        max_time = max(execution_times)\n",
    "        min_time = min(execution_times)\n",
    "        \n",
    "        print(f\"   Execution Times:\")\n",
    "        print(f\"      Average: {avg_time:.1f}s\")\n",
    "        print(f\"      Range: {min_time:.1f}s - {max_time:.1f}s\")\n",
    "\n",
    "def _analyze_errors_and_failures(self):\n",
    "    \"\"\"Analyze errors and execution failures\"\"\"\n",
    "    results = self.last_result.get('results', [])\n",
    "    failed_results = [r for r in results if not r.get('execution_successful', True)]\n",
    "    eval_errors = [r for r in results if r.get('evaluation_error')]\n",
    "    \n",
    "    if failed_results or eval_errors:\n",
    "        print(f\"\\nüêõ ERROR ANALYSIS\")\n",
    "        \n",
    "        if failed_results:\n",
    "            print(f\"   Execution Failures: {len(failed_results)}\")\n",
    "        \n",
    "        if eval_errors:\n",
    "            print(f\"   Evaluation Errors: {len(eval_errors)}\")\n",
    "            unique_errors = set(r.get('evaluation_error', '') for r in eval_errors)\n",
    "            for error in unique_errors:\n",
    "                if error:\n",
    "                    print(f\"      {error}\")\n",
    "\n",
    "def _generate_actionable_recommendations(self):\n",
    "    \"\"\"Generate specific, actionable recommendations\"\"\"\n",
    "    overall = self.last_result['overall_performance']\n",
    "    accuracy = overall['accuracy']\n",
    "    \n",
    "    print(f\"\\nüí° ACTIONABLE RECOMMENDATIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if accuracy < 0.20:\n",
    "        print(\"   1. üéØ CRITICAL: Focus on basic functionality - accuracy very low\")\n",
    "        print(\"   2. üîß Check agent configuration and tool integration\")\n",
    "    elif accuracy < 0.45:\n",
    "        print(\"   1. üéØ PRIMARY: Work toward GAIA 45% threshold\")\n",
    "        print(\"   2. üìà Focus on Level 1 questions first\")\n",
    "    elif accuracy < 0.60:\n",
    "        print(\"   1. üéØ OPTIMIZE: Good performance - push toward excellence\")\n",
    "        print(\"   2. üìà Level 2 questions are the key improvement area\")\n",
    "    else:\n",
    "        print(\"   1. üèÜ EXCELLENT: Maintain and monitor performance\")\n",
    "        print(\"   2. üìä Test with larger question sets to validate\")\n",
    "\n",
    "# ADD OR UPDATE THE VALIDATOR\n",
    "if 'validator' in globals():\n",
    "    print(\"üîß Updating existing validator with enhanced insights...\")\n",
    "    \n",
    "    # Add all the new methods to your existing validator\n",
    "    validator.enhanced_insights = enhanced_insights.__get__(validator, validator.__class__)\n",
    "    validator._show_log_file_links = _show_log_file_links.__get__(validator, validator.__class__)\n",
    "    validator._analyze_execution_performance = _analyze_execution_performance.__get__(validator, validator.__class__)\n",
    "    validator._analyze_accuracy_breakdown = _analyze_accuracy_breakdown.__get__(validator, validator.__class__)\n",
    "    validator._analyze_strategy_performance = _analyze_strategy_performance.__get__(validator, validator.__class__)\n",
    "    validator._analyze_technical_performance = _analyze_technical_performance.__get__(validator, validator.__class__)\n",
    "    validator._analyze_errors_and_failures = _analyze_errors_and_failures.__get__(validator, validator.__class__)\n",
    "    validator._generate_actionable_recommendations = _generate_actionable_recommendations.__get__(validator, validator.__class__)\n",
    "    \n",
    "    print(\"‚úÖ Enhanced insights added to existing validator!\")\n",
    "    \n",
    "    # Check if we have results\n",
    "    if hasattr(validator, 'last_result') and validator.last_result:\n",
    "        print(\"‚úÖ Test results found - enhanced_insights() ready to use!\")\n",
    "        print(\"üí° Now try: validator.enhanced_insights()\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No test results found\")\n",
    "        print(\"üí° Run a test first: result = validator.quick('groq', 5)\")\n",
    "        print(\"üí° Then use: validator.enhanced_insights()\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No 'validator' found\")\n",
    "    print(\"üí° Create validator first:\")\n",
    "    print(\"   from agent_testing import GAIATestValidator\")\n",
    "    print(\"   validator = GAIATestValidator()\")\n",
    "    print(\"   result = validator.quick('groq', 5)\")\n",
    "    print(\"   validator.enhanced_insights()\")\n",
    "\n",
    "print(f\"\\nüéâ COMPLETE VALIDATOR FIX APPLIED!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Enhanced insights methods added\")\n",
    "print(\"‚úÖ Better error handling and debugging\")\n",
    "print(\"‚úÖ Rich analysis with log file links\")\n",
    "print(\"‚úÖ Comprehensive performance breakdown\")\n",
    "print(\"\")\n",
    "print(\"üöÄ READY TO USE:\")\n",
    "print(\"   validator.enhanced_insights()  # Rich analysis\")\n",
    "print(\"   validator.insights()          # Original basic analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = validator.quick('openrouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.enhanced_insights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Run full gaia test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when ready for full validation\n",
    "result = validator.full('openrouter', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.enhanced_insights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### bug fixing and experimentation spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_existing_infrastructure.py - Use your actual GAIA setup\n",
    "\n",
    "from agent_logic import GAIAAgent, GAIAConfig, extract_file_info_from_task_id\n",
    "from smolagents import CodeAgent, LiteLLMModel\n",
    "import os\n",
    "\n",
    "def test_with_existing_gaia_infrastructure():\n",
    "    \"\"\"Test SmolagAgents with your existing GAIA infrastructure\"\"\"\n",
    "    \n",
    "    # Use your existing config\n",
    "    config = GAIAConfig(\n",
    "        model_provider=\"openrouter\",\n",
    "        model_name=\"google/gemini-2.5-flash\",\n",
    "        temperature=0.1,\n",
    "        enable_context_bridge=True,\n",
    "        debug_mode=True\n",
    "    )\n",
    "    \n",
    "    # Create model using your existing pattern\n",
    "    specialist_model = LiteLLMModel(\n",
    "        model_id=f\"openrouter/{config.model_name}\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        temperature=config.temperature\n",
    "    )\n",
    "    \n",
    "    # Create single test agent\n",
    "    test_agent = CodeAgent(\n",
    "        tools=[],\n",
    "        model=specialist_model,\n",
    "        add_base_tools=True,\n",
    "        additional_authorized_imports=[\n",
    "            \"pandas\", \"openpyxl\", \"numpy\", \"json\", \"matplotlib\", \"seaborn\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Test with actual GAIA task pattern\n",
    "    test_task_id = \"test_excel_123\"\n",
    "    test_question = \"What is the sum of column B in the spreadsheet?\"\n",
    "    \n",
    "    # Create test state like your GAIAState\n",
    "    test_state = {\n",
    "        \"task_id\": test_task_id,\n",
    "        \"question\": test_question,\n",
    "        \"file_path\": os.path.abspath(\"test_data.xlsx\"),\n",
    "        \"file_name\": \"test_data.xlsx\",\n",
    "        \"has_file\": True\n",
    "    }\n",
    "    \n",
    "    # Create test Excel file\n",
    "    import pandas as pd\n",
    "    test_df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [10, 20, 30]})\n",
    "    test_df.to_excel(test_state[\"file_path\"], index=False)\n",
    "    \n",
    "    # Use your enhanced question pattern\n",
    "    enhanced_question = f\"\"\"\n",
    "{test_question}\n",
    "\n",
    "File Information:\n",
    "- File available at: {test_state['file_path']}\n",
    "- File name: {test_state['file_name']}\n",
    "- Access this file directly in your Python code using the file path above\n",
    "\n",
    "Important: Use pandas.read_excel() to load the file and sum column B.\n",
    "\"\"\"\n",
    "    \n",
    "    additional_args = {\n",
    "        'file_path': test_state['file_path'],\n",
    "        'file_name': test_state['file_name'],\n",
    "        'has_file': True\n",
    "    }\n",
    "    \n",
    "    print(f\"üß™ Testing with OpenRouter model: {config.model_name}\")\n",
    "    print(f\"üìÅ Test file: {test_state['file_name']}\")\n",
    "    \n",
    "    try:\n",
    "        result = test_agent.run(\n",
    "            task=enhanced_question,\n",
    "            additional_args=additional_args\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ GAIA infrastructure integration SUCCESS!\")\n",
    "        print(f\"üìä Expected sum: 60 (10+20+30)\")\n",
    "        print(f\"üìä Agent result: {result}\")\n",
    "        \n",
    "        # Check if result contains expected answer\n",
    "        if \"60\" in str(result):\n",
    "            print(\"üéØ Correct answer detected!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Answer may not be correct, but execution succeeded\")\n",
    "            return True\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Integration test FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        if os.path.exists(test_state['file_path']):\n",
    "            os.remove(test_state['file_path'])\n",
    "            print(\"üßπ Cleaned up test file\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_with_existing_gaia_infrastructure()\n",
    "    print(f\"\\n{'üéØ READY FOR FULL INTEGRATION' if success else 'üîß NEEDS DEBUGGING'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-assignment-agent-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
