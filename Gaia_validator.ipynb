{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# SIMPLE GAIA VALIDATOR - NO FLUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Declaring validation class by importing methods from testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_testing import run_quick_gaia_test, run_gaia_test, compare_agent_configs, run_smart_routing_test\n",
    "\n",
    "class GAIAValidator:\n",
    "    def __init__(self):\n",
    "        self.last_result = None\n",
    "        print(\"üéØ GAIA Validator ready\")\n",
    "    \n",
    "    def quick(self, config=\"groq\", questions=5):\n",
    "        result = run_quick_gaia_test(config, num_questions=questions)\n",
    "        self.last_result = result\n",
    "        if result and 'overall_performance' in result:\n",
    "            acc = result['overall_performance']['accuracy']\n",
    "            print(f\"‚úÖ {acc:.1%} accuracy\")\n",
    "        return result\n",
    "    \n",
    "    def full(self, config=\"groq\", questions=20):\n",
    "        result = run_gaia_test(config, max_questions=questions)\n",
    "        self.last_result = result\n",
    "        if result and 'overall_performance' in result:\n",
    "            acc = result['overall_performance']['accuracy']\n",
    "            total = result['overall_performance']['total_questions']\n",
    "            correct = result['overall_performance']['correct_answers']\n",
    "            print(f\"‚úÖ {acc:.1%} accuracy ({correct}/{total})\")\n",
    "            print(f\"GAIA Target: {'‚úÖ MET' if acc >= 0.45 else '‚ùå NOT MET'}\")\n",
    "        return result\n",
    "    \n",
    "    def compare(self, configs=[\"groq\", \"google\"], questions=10):\n",
    "        result = compare_agent_configs(configs, questions)\n",
    "        self.last_result = result\n",
    "        if result and 'comparison_results' in result:\n",
    "            for config, data in result['comparison_results'].items():\n",
    "                if 'accuracy' in data:\n",
    "                    print(f\"{config}: {data['accuracy']:.1%}\")\n",
    "        return result\n",
    "    \n",
    "    def insights(self):\n",
    "        if not self.last_result or 'overall_performance' not in self.last_result:\n",
    "            print(\"‚ùå No results to analyze\")\n",
    "            return\n",
    "        \n",
    "        overall = self.last_result['overall_performance']\n",
    "        acc = overall['accuracy']\n",
    "        \n",
    "        print(f\"\\nüìä INSIGHTS\")\n",
    "        print(f\"Accuracy: {acc:.1%}\")\n",
    "        \n",
    "        if acc >= 0.60:\n",
    "            print(\"üèÜ EXCELLENT\")\n",
    "        elif acc >= 0.45:\n",
    "            print(\"‚úÖ GOOD - Above GAIA threshold\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è NEEDS IMPROVEMENT\")\n",
    "        \n",
    "        # Level breakdown\n",
    "        levels = self.last_result.get('level_performance', {})\n",
    "        if levels:\n",
    "            print(\"By level:\")\n",
    "            for level, perf in levels.items():\n",
    "                print(f\"  Level {level}: {perf['accuracy']:.1%}\")\n",
    "        \n",
    "        # Simple recommendations\n",
    "        if acc < 0.45:\n",
    "            print(\"üí° Focus on Level 1 questions first\")\n",
    "        elif acc < 0.60:\n",
    "            print(\"üí° Good performance - optimize Level 2\")\n",
    "        else:\n",
    "            print(\"üí° Excellent - test with more questions\")\n",
    "\n",
    "validator = GAIAValidator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# GAIA Testing Methods Reference\n",
    "\n",
    "## Validator Methods\n",
    "\n",
    "### `validator.quick(config, questions)`\n",
    "**Quick validation test - perfect for development**\n",
    "- `config` (str, default=\"groq\"): Agent configuration \n",
    "  - Options: `\"groq\"`, `\"google\"`, `\"openrouter\"`, `\"ollama\"`, `\"performance\"`, `\"accuracy\"`\n",
    "- `questions` (int, default=5): Number of questions to test\n",
    "- **Returns**: Test results dict\n",
    "- **Use case**: Fast iteration during development\n",
    "\n",
    "```python\n",
    "result = validator.quick('groq', 5)\n",
    "```\n",
    "\n",
    "### `validator.full(config, questions)`\n",
    "**Comprehensive test - for production validation**\n",
    "- `config` (str, default=\"groq\"): Agent configuration\n",
    "- `questions` (int, default=20): Number of questions (20+ recommended for reliable results)\n",
    "- **Returns**: Complete test results with level breakdown\n",
    "- **Use case**: Final validation before deployment\n",
    "\n",
    "```python\n",
    "result = validator.full('groq', 20)\n",
    "```\n",
    "\n",
    "### `validator.compare(configs, questions)`\n",
    "**Compare multiple configurations**\n",
    "- `configs` (list, default=[\"groq\", \"google\"]): List of configurations to compare\n",
    "- `questions` (int, default=10): Questions per configuration\n",
    "- **Returns**: Comparison results with rankings\n",
    "- **Use case**: Choosing the best configuration\n",
    "\n",
    "```python\n",
    "result = validator.compare(['groq', 'google', 'performance'], 10)\n",
    "```\n",
    "\n",
    "### `validator.insights()`\n",
    "**Analyze last test results**\n",
    "- **No parameters**\n",
    "- **Returns**: None (prints analysis)\n",
    "- **Use case**: Get actionable recommendations after any test\n",
    "\n",
    "```python\n",
    "validator.insights()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Underlying Testing Functions\n",
    "\n",
    "### `run_quick_gaia_test(agent_config_name, **kwargs)`\n",
    "**Direct access to quick testing**\n",
    "- `agent_config_name` (str): Configuration name\n",
    "- `num_questions` (int, default=5): Number of questions\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- **Returns**: Evaluation results dict\n",
    "\n",
    "### `run_gaia_test(agent_config_name, dataset_path, max_questions, test_config)`\n",
    "**Complete GAIA test workflow**\n",
    "- `agent_config_name` (str, default=\"groq\"): Configuration name\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- `max_questions` (int, default=20): Maximum questions to test\n",
    "- `test_config` (GAIATestConfig, optional): Advanced test configuration\n",
    "- **Returns**: Complete evaluation results\n",
    "\n",
    "### `compare_agent_configs(config_names, num_questions, dataset_path)`\n",
    "**Compare multiple agent configurations**\n",
    "- `config_names` (List[str]): List of configuration names\n",
    "- `num_questions` (int, default=10): Questions per configuration\n",
    "- `dataset_path` (str, default=\"./tests/gaia_data\"): Dataset location\n",
    "- **Returns**: Comparison results dict\n",
    "\n",
    "### `run_smart_routing_test(agent_config_name, num_questions)`\n",
    "**Test smart routing effectiveness**\n",
    "- `agent_config_name` (str, default=\"performance\"): Configuration name\n",
    "- `num_questions` (int, default=15): Number of questions for analysis\n",
    "- **Returns**: Routing analysis results\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "### Available Configurations\n",
    "| Config Name | Provider | Model | Use Case |\n",
    "|-------------|----------|-------|----------|\n",
    "| `\"groq\"` | Groq | qwen-qwq-32b | Fast, reliable |\n",
    "| `\"google\"` | Google | gemini-2.0-flash-preview | Balanced performance |\n",
    "| `\"openrouter\"` | OpenRouter | qwen/qwen-2.5-coder-32b-instruct:free | Cost-effective |\n",
    "| `\"ollama\"` | Ollama | qwen2.5-coder:32b | Local deployment |\n",
    "| `\"performance\"` | Groq | qwen-qwq-32b | Optimized for speed |\n",
    "| `\"accuracy\"` | Google | gemini-2.0-flash-preview | Optimized for accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "## Test Result Structure\n",
    "\n",
    "### Standard Result Format\n",
    "```python\n",
    "{\n",
    "    \"overall_performance\": {\n",
    "        \"total_questions\": 20,\n",
    "        \"correct_answers\": 11,\n",
    "        \"accuracy\": 0.55,\n",
    "        \"successful_executions\": 19\n",
    "    },\n",
    "    \"level_performance\": {\n",
    "        \"1\": {\"accuracy\": 0.70, \"correct\": 7, \"total\": 10},\n",
    "        \"2\": {\"accuracy\": 0.44, \"correct\": 4, \"total\": 9},\n",
    "        \"3\": {\"accuracy\": 0.0, \"correct\": 0, \"total\": 1}\n",
    "    },\n",
    "    \"strategy_analysis\": {\n",
    "        \"one_shot_llm\": {\"accuracy\": 0.67, \"total_questions\": 12},\n",
    "        \"manager_coordination\": {\"accuracy\": 0.38, \"total_questions\": 8}\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Comparison Result Format\n",
    "```python\n",
    "{\n",
    "    \"comparison_results\": {\n",
    "        \"groq\": {\"accuracy\": 0.60, \"correct_answers\": 6, \"total_questions\": 10},\n",
    "        \"google\": {\"accuracy\": 0.50, \"correct_answers\": 5, \"total_questions\": 10}\n",
    "    },\n",
    "    \"timestamp\": \"2024-12-19T10:30:00\",\n",
    "    \"test_questions\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Workflow\n",
    "```python\n",
    "# 1. Quick test\n",
    "result = validator.quick('groq', 5)\n",
    "validator.insights()\n",
    "\n",
    "# 2. Full validation  \n",
    "result = validator.full('groq', 20)\n",
    "validator.insights()\n",
    "```\n",
    "\n",
    "### Configuration Comparison\n",
    "```python\n",
    "# Compare multiple configs\n",
    "result = validator.compare(['groq', 'google', 'performance'], 10)\n",
    "validator.insights()\n",
    "```\n",
    "\n",
    "### Advanced Testing\n",
    "```python\n",
    "# Direct function access for custom workflows\n",
    "from agent_testing import run_gaia_test, analyze_failure_patterns\n",
    "\n",
    "result = run_gaia_test('groq', max_questions=50)\n",
    "failure_analysis = analyze_failure_patterns(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Benchmarks\n",
    "\n",
    "### GAIA Accuracy Targets\n",
    "- **45%+**: GAIA benchmark threshold\n",
    "- **50-60%**: Competitive performance\n",
    "- **60%+**: Excellent performance\n",
    "\n",
    "### Execution Time Guidelines\n",
    "- **Quick test (5q)**: ~1-2 minutes\n",
    "- **Full test (20q)**: ~5-8 minutes  \n",
    "- **Comparison (3 configs, 10q each)**: ~10-15 minutes\n",
    "\n",
    "### Recommended Question Counts\n",
    "- **Development**: 5 questions (quick feedback)\n",
    "- **Validation**: 20 questions (reliable results)\n",
    "- **Production**: 50+ questions (comprehensive assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# GAIA Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = validator.quick('openrouter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-assignment-agent-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
