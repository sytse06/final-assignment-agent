# agent_testing.py
# GAIA Agent Testing Framework

import os
import json
import time
import uuid
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
from collections import defaultdict
import difflib

# Import agent system
from agent_logic import GAIAAgent
from agent_interface import get_agent_config

# Import dataset management
try:
    from gaia_dataset_utils import GAIADatasetManager, quick_dataset_check
    DATASET_UTILS_AVAILABLE = True
    print("âœ… Dataset utilities available")
except ImportError:
    print("âš ï¸  Dataset utilities not available - limited testing functionality")
    DATASET_UTILS_AVAILABLE = False

# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class GAIATestConfig:
    """Configuration for GAIA testing"""
    max_questions: int = 20
    dataset_path: str = "./tests/gaia_data"
    save_execution_results: bool = True
    save_evaluation_results: bool = True
    results_directory: str = "./test_results"
    execution_timeout: int = 300  # 5 minutes per question
    retry_failed_questions: bool = True
    max_retries: int = 2
    
    # Result file naming
    execution_file_prefix: str = "gaia_execution"
    evaluation_file_prefix: str = "gaia_evaluation"
    
    # Performance tracking
    track_detailed_metrics: bool = True
    analyze_failure_patterns: bool = True
    
    def __post_init__(self):
        """Ensure results directory exists"""
        Path(self.results_directory).mkdir(parents=True, exist_ok=True)

# ============================================================================
# SIMPLIFIED QUESTION EXECUTOR
# ============================================================================

class GAIAQuestionExecutor:
    """
    Simplified GAIA question executor using hybrid state approach.
    Dramatically reduced complexity while preserving all functionality.
    """
    
    def __init__(self, agent_config: Dict, test_config: GAIATestConfig = None):
        """
        Initialize executor with simplified state-based approach.
        
        Args:
            agent_config: Agent configuration dictionary
            test_config: Test execution configuration
        """
        self.agent_config = agent_config
        self.test_config = test_config or GAIATestConfig()
        
        # Create agent instance (handles all its own configuration)
        print("ğŸ”§ Creating GAIA agent...")
        self.agent = GAIAAgent(agent_config)
        
        # Simple execution tracking
        self.execution_stats = {
            "total_questions": 0,
            "successful_executions": 0,
            "failed_executions": 0,
            "retry_count": 0,
            "total_execution_time": 0.0
        }
        
        print("âœ… GAIAQuestionExecutor initialized with hybrid state approach")
    
    def execute_questions_batch(self, blind_questions: List[Dict]) -> str:
        """
        Execute batch of questions without ground truth (blind testing).
        Simplified approach using state preparation.
        
        Args:
            blind_questions: List of questions without 'Final answer' field
            
        Returns:
            Path to execution results file
        """
        print(f"ğŸš€ Starting batch execution: {len(blind_questions)} questions")
        
        # Generate execution file name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        execution_file = Path(self.test_config.results_directory) / f"{self.test_config.execution_file_prefix}_{timestamp}.json"
        
        execution_results = []
        start_time = time.time()
        
        for i, question_data in enumerate(blind_questions, 1):
            print(f"\nğŸ“‹ Question {i}/{len(blind_questions)}: {question_data.get('task_id', 'unknown')}")
            
            # Execute single question with simplified approach
            result = self._execute_single_question_hybrid(question_data, i)
            execution_results.append(result)
            
            # Update stats
            self.execution_stats["total_questions"] += 1
            if result.get("execution_successful", False):
                self.execution_stats["successful_executions"] += 1
            else:
                self.execution_stats["failed_executions"] += 1
        
        # Calculate total execution time
        total_time = time.time() - start_time
        self.execution_stats["total_execution_time"] = total_time
        
        # Save execution results
        execution_summary = {
            "execution_metadata": {
                "timestamp": timestamp,
                "agent_config": self.agent_config.__dict__ if hasattr(self.agent_config, '__dict__') else self.agent_config,
                "execution_stats": self.execution_stats,
                "total_execution_time": total_time,
                "questions_processed": len(blind_questions)
            },
            "execution_results": execution_results
        }
        
        if self.test_config.save_execution_results:
            with open(execution_file, 'w', encoding='utf-8') as f:
                json.dump(execution_summary, f, indent=2, ensure_ascii=False, default=str)
            print(f"ğŸ’¾ Execution results saved: {execution_file}")
        
        print(f"\nğŸ Batch execution complete:")
        print(f"   âœ… Successful: {self.execution_stats['successful_executions']}")
        print(f"   âŒ Failed: {self.execution_stats['failed_executions']}")
        print(f"   â±ï¸  Total time: {total_time:.1f}s")
        
        return str(execution_file)
    
    def _execute_single_question_hybrid(self, question_data: Dict, question_num: int) -> Dict:
        """
        Simplified single question execution using hybrid state approach.
        
        Args:
            question_data: Question data from GAIADatasetManager
            question_num: Question number for logging
            
        Returns:
            Execution result dictionary
        """
        task_id = question_data.get("task_id", f"unknown_{question_num}")
        question = question_data.get("Question", "")
        
        print(f"ğŸ”„ Processing: {question[:50]}...")
        
        # Prepare initial state (SIMPLE: just populate GAIAState fields)
        initial_state = self._prepare_question_state(question_data)
        
        # Record execution start
        execution_start = time.time()
        
        try:
            # Execute with state (agent handles everything internally)
            print("ğŸ¯ Executing agent with state...")
            result = self.agent.process_question(
                question=question,
                task_id=task_id
            )
            
            execution_time = time.time() - execution_start
            
            # Extract key information
            final_answer = result.get("final_answer", "")
            execution_successful = result.get("execution_successful", False)
            
            print(f"âœ… Execution complete: {final_answer[:30]}..." if final_answer else "âŒ No answer")
            
            # Create execution record (simplified)
            return self._create_execution_record(result, question_data, execution_time)
            
        except Exception as e:
            execution_time = time.time() - execution_start
            error_msg = f"Execution failed: {str(e)}"
            print(f"âŒ {error_msg}")
            
            # Handle retry if configured
            if self.test_config.retry_failed_questions and self.execution_stats["retry_count"] < self.test_config.max_retries:
                print(f"ğŸ”„ Retrying question {question_num}...")
                self.execution_stats["retry_count"] += 1
                return self._execute_single_question_hybrid(question_data, question_num)
            
            # Create error record
            return {
                "task_id": task_id,
                "question": question,
                "final_answer": "",
                "execution_successful": False,
                "execution_time": execution_time,
                "error_message": error_msg,
                "question_metadata": {
                    "level": question_data.get("Level", "unknown"),
                    "file_name": question_data.get("file_name", ""),
                    "has_file": bool(question_data.get("file_name"))
                }
            }
    
    def _prepare_question_state(self, question_data: Dict) -> Dict:
        """
        Simple state preparation from question data.
        Replaces complex tool configuration with clean state population.
        
        Args:
            question_data: Raw question data from dataset
            
        Returns:
            Initial state dictionary for GAIAState
        """
        return {
            "task_id": question_data.get("task_id"),
            "question": question_data.get("Question", ""),
            "level": question_data.get("Level", "unknown"),
            "file_name": question_data.get("file_name", ""),
            "file_path": question_data.get("file_path", ""),
            "has_file": bool(question_data.get("file_name")),
            "steps": []
        }
    
    def _create_execution_record(self, agent_result: Dict, question_data: Dict, execution_time: float) -> Dict:
        """
        Create execution record from agent result and question data.
        
        Args:
            agent_result: Result from agent.process_question()
            question_data: Original question data
            execution_time: Time taken for execution
            
        Returns:
            Standardized execution record
        """
        return {
            "task_id": agent_result.get("task_id"),
            "question": agent_result.get("question"),
            "final_answer": agent_result.get("final_answer", ""),
            "raw_answer": agent_result.get("raw_answer", ""),
            "execution_successful": agent_result.get("execution_successful", False),
            "execution_time": execution_time,
            "total_steps": agent_result.get("total_steps", 0),
            "selected_agent": agent_result.get("selected_agent", "unknown"),
            "complexity": agent_result.get("complexity", "unknown"),
            "question_metadata": {
                "level": question_data.get("Level", "unknown"),
                "file_name": question_data.get("file_name", ""),
                "has_file": bool(question_data.get("file_name")),
                "file_path": question_data.get("file_path", "")
            },
            "performance_metrics": agent_result.get("performance_metrics", {}),
            "file_info": agent_result.get("file_info", {})
        }

# ============================================================================
# PRESERVED EVALUATION SYSTEM
# ============================================================================

class GAIAAnswerEvaluator:
    """
    GAIA answer evaluation system with preserved sophisticated functionality.
    Compares agent answers against ground truth with multiple matching strategies.
    """
    
    def __init__(self, dataset_manager = None):
        """
        Initialize evaluator with ground truth access.
        
        Args:
            dataset_manager: GAIADatasetManager instance for ground truth
        """
        self.dataset_manager = dataset_manager
        self.evaluation_stats = {
            "total_evaluated": 0,
            "exact_matches": 0,
            "fuzzy_matches": 0,
            "no_matches": 0,
            "evaluation_errors": 0
        }
        
        if not dataset_manager and DATASET_UTILS_AVAILABLE:
            print("âš ï¸  No dataset manager provided - limited evaluation capability")
        
        print("âœ… GAIAAnswerEvaluator initialized")
    
    def evaluate_execution_results(self, execution_file: str) -> Dict:
        """
        Evaluate execution results against ground truth.
        
        Args:
            execution_file: Path to execution results JSON file
            
        Returns:
            Comprehensive evaluation results
        """
        print(f"ğŸ“Š Starting evaluation: {execution_file}")
        
        # Load execution results
        with open(execution_file, 'r', encoding='utf-8') as f:
            execution_data = json.load(f)
        
        execution_results = execution_data.get("execution_results", [])
        execution_metadata = execution_data.get("execution_metadata", {})
        
        print(f"ğŸ” Evaluating {len(execution_results)} results...")
        
        # Evaluate each result
        evaluation_results = []
        performance_by_level = defaultdict(lambda: {"correct": 0, "total": 0})
        strategy_performance = defaultdict(lambda: {"correct": 0, "total": 0})
        
        for result in execution_results:
            evaluation = self._evaluate_single_result(result)
            evaluation_results.append(evaluation)
            
            # Track performance by level
            level = result.get("question_metadata", {}).get("level", "unknown")
            performance_by_level[level]["total"] += 1
            if evaluation.get("is_correct", False):
                performance_by_level[level]["correct"] += 1
            
            # Track performance by strategy
            selected_agent = result.get("selected_agent", "unknown")
            complexity = result.get("complexity", "unknown")
            strategy = f"{selected_agent}_{complexity}" if selected_agent != "unknown" else "unknown"
            
            strategy_performance[strategy]["total"] += 1
            if evaluation.get("is_correct", False):
                strategy_performance[strategy]["correct"] += 1
        
        # Calculate overall performance
        total_questions = len(evaluation_results)
        correct_answers = sum(1 for eval_result in evaluation_results if eval_result.get("is_correct", False))
        
        overall_performance = {
            "total_questions": total_questions,
            "correct_answers": correct_answers,
            "accuracy": correct_answers / total_questions if total_questions > 0 else 0.0,
            "successful_executions": sum(1 for result in execution_results if result.get("execution_successful", False))
        }
        
        # Calculate level performance
        level_performance = {}
        for level, stats in performance_by_level.items():
            level_performance[level] = {
                "total": stats["total"],
                "correct": stats["correct"],
                "accuracy": stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            }
        
        # Calculate strategy performance
        strategy_analysis = {}
        for strategy, stats in strategy_performance.items():
            strategy_analysis[strategy] = {
                "total_questions": stats["total"],
                "correct_answers": stats["correct"],
                "accuracy": stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            }
        
        # Compile comprehensive evaluation
        comprehensive_evaluation = {
            "evaluation_metadata": {
                "timestamp": datetime.now().isoformat(),
                "execution_file": execution_file,
                "evaluation_stats": self.evaluation_stats
            },
            "overall_performance": overall_performance,
            "level_performance": level_performance,
            "strategy_analysis": strategy_analysis,
            "execution_metadata": execution_metadata,
            "detailed_results": evaluation_results
        }
        
        # Save evaluation results
        if hasattr(self, 'save_evaluation') and self.save_evaluation:
            self._save_evaluation_results(comprehensive_evaluation, execution_file)
        
        # Print summary
        self._print_evaluation_summary(comprehensive_evaluation)
        
        return comprehensive_evaluation
    
    def _evaluate_single_result(self, execution_result: Dict) -> Dict:
        """
        Evaluate single execution result against ground truth.
        
        Args:
            execution_result: Single execution result
            
        Returns:
            Evaluation result with correctness determination
        """
        task_id = execution_result.get("task_id")
        predicted_answer = execution_result.get("final_answer", "")
        
        if not self.dataset_manager:
            return {
                "task_id": task_id,
                "predicted_answer": predicted_answer,
                "expected_answer": "Unknown",
                "is_correct": False,
                "matching_method": "no_ground_truth",
                "confidence": 0.0,
                "evaluation_error": "No dataset manager available"
            }
        
        try:
            # Get ground truth
            ground_truth = self.dataset_manager.get_ground_truth(task_id)
            expected_answer = ground_truth.get("Final answer", "") if ground_truth else ""
            
            if not expected_answer:
                self.evaluation_stats["evaluation_errors"] += 1
                return {
                    "task_id": task_id,
                    "predicted_answer": predicted_answer,
                    "expected_answer": "",
                    "is_correct": False,
                    "matching_method": "no_ground_truth",
                    "confidence": 0.0,
                    "evaluation_error": "No ground truth found"
                }
            
            # Try GAIA-compliant exact matching first
            exact_match, exact_confidence = self.gaia_answer_matching(predicted_answer, expected_answer)
            
            if exact_match:
                self.evaluation_stats["exact_matches"] += 1
                return {
                    "task_id": task_id,
                    "predicted_answer": predicted_answer,
                    "expected_answer": expected_answer,
                    "is_correct": True,
                    "matching_method": "gaia_exact",
                    "confidence": exact_confidence
                }
            
            # Try fuzzy matching as fallback
            fuzzy_match, fuzzy_confidence = self.fuzzy_answer_matching(predicted_answer, expected_answer)
            
            if fuzzy_match:
                self.evaluation_stats["fuzzy_matches"] += 1
                return {
                    "task_id": task_id,
                    "predicted_answer": predicted_answer,
                    "expected_answer": expected_answer,
                    "is_correct": True,
                    "matching_method": "fuzzy",
                    "confidence": fuzzy_confidence
                }
            
            # No match
            self.evaluation_stats["no_matches"] += 1
            return {
                "task_id": task_id,
                "predicted_answer": predicted_answer,
                "expected_answer": expected_answer,
                "is_correct": False,
                "matching_method": "no_match",
                "confidence": 0.0
            }
            
        except Exception as e:
            self.evaluation_stats["evaluation_errors"] += 1
            return {
                "task_id": task_id,
                "predicted_answer": predicted_answer,
                "expected_answer": "Error",
                "is_correct": False,
                "matching_method": "evaluation_error",
                "confidence": 0.0,
                "evaluation_error": str(e)
            }
    
    def gaia_answer_matching(self, predicted: str, expected: str) -> tuple[bool, float]:
        """
        GAIA-compliant answer matching with exact comparison.
        
        Args:
            predicted: Agent's predicted answer
            expected: Ground truth answer
            
        Returns:
            Tuple of (is_match, confidence)
        """
        if not predicted or not expected:
            return False, 0.0
        
        # Clean both answers
        pred_clean = self._clean_answer_for_gaia(predicted)
        exp_clean = self._clean_answer_for_gaia(expected)
        
        # Exact match
        if pred_clean == exp_clean:
            return True, 1.0
        
        # Case-insensitive exact match
        if pred_clean.lower() == exp_clean.lower():
            return True, 0.95
        
        return False, 0.0
    
    def fuzzy_answer_matching(self, predicted: str, expected: str, threshold: float = 0.8) -> tuple[bool, float]:
        """
        Fuzzy answer matching using similarity metrics.
        
        Args:
            predicted: Agent's predicted answer
            expected: Ground truth answer
            threshold: Minimum similarity threshold
            
        Returns:
            Tuple of (is_match, confidence)
        """
        if not predicted or not expected:
            return False, 0.0
        
        # Calculate similarity
        similarity = difflib.SequenceMatcher(None, predicted.lower(), expected.lower()).ratio()
        
        # Check if above threshold
        is_match = similarity >= threshold
        
        return is_match, similarity
    
    def _clean_answer_for_gaia(self, answer: str) -> str:
        """
        Clean answer according to GAIA formatting rules.
        
        Args:
            answer: Raw answer string
            
        Returns:
            Cleaned answer string
        """
        if not answer:
            return ""
        
        answer = answer.strip()
        
        # Remove common prefixes
        prefixes = ["the answer is", "answer:", "final answer:", "result:", "solution:"]
        answer_lower = answer.lower()
        for prefix in prefixes:
            if answer_lower.startswith(prefix):
                answer = answer[len(prefix):].strip()
                break
        
        # Remove quotes and punctuation
        answer = answer.strip('.,!?:;"\'')
        
        # Handle numbers - remove commas
        if answer.replace('.', '').replace('-', '').replace(',', '').replace(' ', '').isdigit():
            answer = answer.replace(',', '').replace(' ', '')
        
        return answer
    
    def _print_evaluation_summary(self, evaluation: Dict):
        """Print evaluation summary to console"""
        overall = evaluation["overall_performance"]
        
        print(f"\nğŸ† EVALUATION SUMMARY")
        print(f"{'='*50}")
        print(f"ğŸ“Š Overall Performance:")
        print(f"   Questions: {overall['total_questions']}")
        print(f"   Correct: {overall['correct_answers']}")
        print(f"   Accuracy: {overall['accuracy']:.1%}")
        print(f"   Successful Executions: {overall['successful_executions']}")
        
        print(f"\nğŸ“ˆ Performance by Level:")
        for level, perf in evaluation["level_performance"].items():
            print(f"   Level {level}: {perf['correct']}/{perf['total']} ({perf['accuracy']:.1%})")
        
        print(f"\nğŸ¯ Strategy Analysis:")
        for strategy, perf in evaluation["strategy_analysis"].items():
            if perf['total_questions'] > 0:
                print(f"   {strategy}: {perf['correct_answers']}/{perf['total_questions']} ({perf['accuracy']:.1%})")
    
    def _save_evaluation_results(self, evaluation: Dict, execution_file: str):
        """Save evaluation results to file"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        eval_file = Path(execution_file).parent / f"gaia_evaluation_{timestamp}.json"
        
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ Evaluation results saved: {eval_file}")

# ============================================================================
# HIGH-LEVEL TESTING FUNCTIONS (PRESERVED)
# ============================================================================

def run_gaia_test(agent_config_name: str = "groq", max_questions: int = 20, dataset_path: str = "./tests/gaia_data") -> Dict:
    """
    Complete GAIA test workflow: prepare â†’ execute â†’ evaluate.
    
    Args:
        agent_config_name: Agent configuration name
        max_questions: Maximum questions to test
        dataset_path: Path to GAIA dataset
        
    Returns:
        Comprehensive evaluation results
    """
    print(f"ğŸš€ Starting complete GAIA test: {agent_config_name}, {max_questions} questions")
    
    if not DATASET_UTILS_AVAILABLE:
        raise ImportError("Dataset utilities required for GAIA testing. Please install gaia_dataset_utils.py")
    
    # Step 1: Prepare test batch (no ground truth)
    print("ğŸ“š Step 1: Preparing test batch...")
    dataset_manager = GAIADatasetManager(dataset_path)
    blind_questions = dataset_manager.create_test_batch(max_questions, "balanced")
    
    # Step 2: Execute questions blindly
    print("ğŸ¯ Step 2: Executing questions...")
    agent_config = get_agent_config_by_name(agent_config_name)
    executor = GAIAQuestionExecutor(agent_config)
    execution_file = executor.execute_questions_batch(blind_questions)
    
    # Step 3: Evaluate against ground truth
    print("ğŸ“Š Step 3: Evaluating results...")
    evaluator = GAIAAnswerEvaluator(dataset_manager)
    results = evaluator.evaluate_execution_results(execution_file)
    
    print("âœ… Complete GAIA test finished!")
    return results

def run_quick_gaia_test(agent_config_name: str = "groq", num_questions: int = 5) -> Dict:
    """
    Quick GAIA validation with small question set.
    
    Args:
        agent_config_name: Agent configuration name
        num_questions: Number of questions to test
        
    Returns:
        Evaluation results
    """
    print(f"âš¡ Quick GAIA test: {agent_config_name}, {num_questions} questions")
    return run_gaia_test(agent_config_name, num_questions)

def compare_agent_configs(config_names: List[str], num_questions: int = 10) -> Dict:
    """
    Compare multiple agent configurations fairly.
    
    Args:
        config_names: List of configuration names to compare
        num_questions: Number of questions for each config
        
    Returns:
        Comparison results
    """
    print(f"ğŸ”„ Comparing agent configs: {config_names}")
    
    comparison_results = {}
    
    for config_name in config_names:
        print(f"\nğŸ”§ Testing configuration: {config_name}")
        try:
            results = run_gaia_test(config_name, num_questions)
            comparison_results[config_name] = {
                "accuracy": results["overall_performance"]["accuracy"],
                "total_questions": results["overall_performance"]["total_questions"],
                "correct_answers": results["overall_performance"]["correct_answers"],
                "level_performance": results["level_performance"],
                "execution_successful": True
            }
        except Exception as e:
            print(f"âŒ Configuration {config_name} failed: {e}")
            comparison_results[config_name] = {
                "error": str(e),
                "execution_successful": False
            }
    
    # Print comparison summary
    print(f"\nğŸ† CONFIGURATION COMPARISON")
    print(f"{'='*50}")
    for config, results in comparison_results.items():
        if results.get("execution_successful"):
            print(f"{config}: {results['accuracy']:.1%} ({results['correct_answers']}/{results['total_questions']})")
        else:
            print(f"{config}: FAILED - {results.get('error', 'Unknown error')}")
    
    return comparison_results

def run_smart_routing_test(agent_config_name: str = "groq", num_questions: int = 15) -> Dict:
    """
    Test smart routing effectiveness.
    
    Args:
        agent_config_name: Agent configuration name
        num_questions: Number of questions to test
        
    Returns:
        Routing analysis results
    """
    print(f"ğŸ›¤ï¸  Testing smart routing: {agent_config_name}")
    
    results = run_gaia_test(agent_config_name, num_questions)
    
    # Analyze routing decisions
    strategy_analysis = results.get("strategy_analysis", {})
    
    print(f"\nğŸ¯ ROUTING ANALYSIS")
    print(f"{'='*40}")
    for strategy, perf in strategy_analysis.items():
        if perf.get("total_questions", 0) > 0:
            print(f"{strategy}: {perf['accuracy']:.1%} ({perf['total_questions']} questions)")
    
    return {
        "routing_analysis": strategy_analysis,
        "overall_performance": results["overall_performance"]
    }

def analyze_failure_patterns(evaluation_results: Dict) -> Dict:
    """
    Analyze failure patterns to identify improvement opportunities.
    
    Args:
        evaluation_results: Results from evaluate_execution_results()
        
    Returns:
        Failure pattern analysis
    """
    detailed_results = evaluation_results.get("detailed_results", [])
    
    # Categorize failures
    failure_categories = {
        "execution_failures": [],
        "wrong_answers": [],
        "no_answers": [],
        "evaluation_errors": []
    }
    
    for result in detailed_results:
        if result.get("evaluation_error"):
            failure_categories["evaluation_errors"].append(result)
        elif not result.get("is_correct", False):
            if not result.get("predicted_answer"):
                failure_categories["no_answers"].append(result)
            else:
                failure_categories["wrong_answers"].append(result)
    
    # Analyze patterns
    analysis = {
        "failure_summary": {
            "execution_failures": len(failure_categories["execution_failures"]),
            "wrong_answers": len(failure_categories["wrong_answers"]),
            "no_answers": len(failure_categories["no_answers"]),
            "evaluation_errors": len(failure_categories["evaluation_errors"])
        },
        "improvement_suggestions": []
    }
    
    # Generate suggestions
    if len(failure_categories["no_answers"]) > 2:
        analysis["improvement_suggestions"].append("High number of no-answer failures - check agent timeout and error handling")
    
    if len(failure_categories["wrong_answers"]) > len(failure_categories["no_answers"]):
        analysis["improvement_suggestions"].append("More wrong answers than no answers - focus on answer accuracy improvement")
    
    print(f"\nğŸ” FAILURE PATTERN ANALYSIS")
    print(f"{'='*40}")
    for category, count in analysis["failure_summary"].items():
        print(f"{category}: {count}")
    
    if analysis["improvement_suggestions"]:
        print(f"\nğŸ’¡ Suggestions:")
        for suggestion in analysis["improvement_suggestions"]:
            print(f"   â€¢ {suggestion}")
    
    return analysis

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def get_agent_config_by_name(config_name: str) -> Dict:
    """
    Get agent configuration by name.
    Redirects to agent_interface for consistency.
    
    Args:
        config_name: Configuration name
        
    Returns:
        Agent configuration dictionary
    """
    try:
        from agent_interface import get_groq_config, get_google_config, get_openrouter_config
        
        config_map = {
            "groq": get_groq_config,
            "google": get_google_config,
            "openrouter": get_openrouter_config
        }
        
        if config_name in config_map:
            return config_map[config_name]()
        else:
            print(f"âš ï¸  Unknown config name '{config_name}', using groq as fallback")
            return get_groq_config()
            
    except ImportError:
        print("âš ï¸  agent_interface not available, using default config")
        from agent_logic import GAIAConfig
        return GAIAConfig()

def test_file_vs_text_performance(agent_config_name: str = "groq", num_questions: int = 10) -> Dict:
    """
    Compare agent performance on questions with files vs text-only questions.
    
    Args:
        agent_config_name: Agent configuration name
        num_questions: Number of questions per category
        
    Returns:
        Performance comparison results
    """
    print(f"ğŸ“Š Testing file vs text performance: {agent_config_name}")
    
    if not DATASET_UTILS_AVAILABLE:
        print("âŒ Dataset utilities required for file vs text comparison")
        return {}
    
    # Get questions with and without files
    dataset_manager = GAIADatasetManager("./tests/gaia_data")
    
    # Get file questions
    file_questions = [q for q in dataset_manager.metadata if q.get('file_name')][:num_questions]
    
    # Get text-only questions
    text_questions = [q for q in dataset_manager.metadata if not q.get('file_name')][:num_questions]
    
    print(f"ğŸ“ Testing {len(file_questions)} questions with files")
    print(f"ğŸ“ Testing {len(text_questions)} text-only questions")
    
    # Test both categories
    agent_config = get_agent_config_by_name(agent_config_name)
    
    results = {
        "file_questions": {"questions": file_questions, "performance": None},
        "text_questions": {"questions": text_questions, "performance": None}
    }
    
    # Test file questions
    if file_questions:
        executor = GAIAQuestionExecutor(agent_config)
        file_execution = executor.execute_questions_batch(file_questions)
        
        evaluator = GAIAAnswerEvaluator(dataset_manager)
        file_evaluation = evaluator.evaluate_execution_results(file_execution)
        results["file_questions"]["performance"] = file_evaluation["overall_performance"]
    
    # Test text questions
    if text_questions:
        executor = GAIAQuestionExecutor(agent_config)
        text_execution = executor.execute_questions_batch(text_questions)
        
        evaluator = GAIAAnswerEvaluator(dataset_manager)
        text_evaluation = evaluator.evaluate_execution_results(text_execution)
        results["text_questions"]["performance"] = text_evaluation["overall_performance"]
    
    # Print comparison
    print(f"\nğŸ“Š FILE VS TEXT PERFORMANCE")
    print(f"{'='*40}")
    
    if results["file_questions"]["performance"]:
        file_perf = results["file_questions"]["performance"]
        print(f"ğŸ“ With Files: {file_perf['accuracy']:.1%} ({file_perf['correct_answers']}/{file_perf['total_questions']})")
    
    if results["text_questions"]["performance"]:
        text_perf = results["text_questions"]["performance"]
        print(f"ğŸ“ Text Only: {text_perf['accuracy']:.1%} ({text_perf['correct_answers']}/{text_perf['total_questions']})")
    
    return results

def validate_test_environment() -> Dict:
    """
    Validate testing environment and dependencies.
    
    Returns:
        Environment validation results
    """
    print("ğŸ” Validating test environment...")
    
    validation = {
        "agent_logic_available": False,
        "agent_interface_available": False,
        "dataset_utils_available": DATASET_UTILS_AVAILABLE,
        "gaia_dataset_accessible": False,
        "test_directory_writable": False,
        "all_dependencies_ready": False
    }
    
    # Check agent_logic
    try:
        from agent_logic import GAIAAgent
        validation["agent_logic_available"] = True
        print("âœ… agent_logic available")
    except ImportError:
        print("âŒ agent_logic not available")
    
    # Check agent_interface
    try:
        from agent_interface import get_groq_config
        validation["agent_interface_available"] = True
        print("âœ… agent_interface available")
    except ImportError:
        print("âŒ agent_interface not available")
    
    # Check GAIA dataset
    if DATASET_UTILS_AVAILABLE:
        try:
            dataset_manager = GAIADatasetManager("./tests/gaia_data")
            if dataset_manager.metadata:
                validation["gaia_dataset_accessible"] = True
                print(f"âœ… GAIA dataset accessible: {len(dataset_manager.metadata)} questions")
            else:
                print("âŒ GAIA dataset not accessible")
        except Exception as e:
            print(f"âŒ GAIA dataset error: {e}")
    
    # Check test directory
    try:
        test_dir = Path("./test_results")
        test_dir.mkdir(exist_ok=True)
        test_file = test_dir / "test_write.tmp"
        test_file.write_text("test")
        test_file.unlink()
        validation["test_directory_writable"] = True
        print("âœ… Test directory writable")
    except Exception as e:
        print(f"âŒ Test directory not writable: {e}")
    
    # Overall validation
    validation["all_dependencies_ready"] = all([
        validation["agent_logic_available"],
        validation["agent_interface_available"],
        validation["dataset_utils_available"],
        validation["test_directory_writable"]
    ])
    
    if validation["all_dependencies_ready"]:
        print("ğŸ‰ All dependencies ready for testing!")
    else:
        print("âš ï¸  Some dependencies missing - limited testing capability")
    
    return validation

# ============================================================================
# BACKWARDS COMPATIBILITY LAYER
# ============================================================================

class GAIATestExecutor:
    """
    Backwards compatibility wrapper for existing test code.
    Redirects to new hybrid approach.
    """
    
    def __init__(self, agent_config, test_config=None):
        print("ğŸ”„ Using backwards compatibility layer")
        self.executor = GAIAQuestionExecutor(agent_config, test_config)
    
    def execute_questions_batch(self, questions):
        return self.executor.execute_questions_batch(questions)
    
    def execute_single_question(self, question_data, attempt=1):
        return self.executor._execute_single_question_hybrid(question_data, attempt)


# ============================================================================
# MAIN TESTING ENTRY POINTS
# ============================================================================

if __name__ == "__main__":
    print("ğŸ§ª GAIA Testing Framework - Hybrid Architecture")
    print("=" * 60)
    print("âœ… AVAILABLE CLASSES:")
    print("   â”œâ”€â”€ GAIAQuestionExecutor (Simplified execution)")
    print("   â”œâ”€â”€ GAIAAnswerEvaluator (GAIA evaluation)")
    print("   â””â”€â”€ GAIATestExecutor (Backwards compatibility)")
    print("")
    print("ğŸ”§ Key Features:")
    features = [
        "âœ… Simplified state-based execution",
        "âœ… Eliminated complex tool configuration", 
        "âœ… Real-time progress monitoring",
        "âœ… Enhanced error handling",
        "âœ… GAIA-compliant answer matching",
        "âœ… Blind testing workflow (Phase 1 + Phase 2)",
        "âœ… Production-ready evaluation metrics",
        "âœ… Detailed failure pattern analysis",
        "âœ… Multi-configuration comparison tools",
        "âœ… 60% code reduction with preserved functionality"
    ]
    
    for feature in features:
        print(f"  {feature}")

    print(f"\nğŸ“‹ Available Functions:")
    functions = [
        'run_gaia_test',
        'run_quick_gaia_test', 
        'compare_agent_configs',
        'run_smart_routing_test',
        'test_file_vs_text_performance',
        'analyze_failure_patterns',
        'validate_test_environment'
    ]
    
    for func in functions:
        print(f"   â”œâ”€â”€ {func}")

    print(f"\nğŸ’¡ Quick Start:")
    print(f"   from agent_testing import run_quick_gaia_test")
    print(f"   result = run_quick_gaia_test('groq', num_questions=5)")
    print(f"   print(f\"Accuracy: {{result['overall_performance']['accuracy']:.1%}}\")")
    
    print(f"\nğŸ¯ Environment Validation:")
    try:
        validation = validate_test_environment()
        
        if validation["all_dependencies_ready"]:
            print("   âœ… All dependencies ready!")
            print("   âœ… GAIA dataset accessible!" if validation["gaia_dataset_accessible"] else "   âš ï¸  GAIA dataset not found")
            print("   âœ… Agent system available!")
            
            print(f"\nğŸš€ Ready for testing! Try:")
            print(f"   results = run_quick_gaia_test('groq', 5)")
            
        else:
            print("   âš ï¸  Some dependencies missing:")
            for dep, available in validation.items():
                if not available and dep != "all_dependencies_ready":
                    print(f"     âŒ {dep}")
        
    except Exception as e:
        print(f"   âŒ Validation failed: {e}")
        print("   ğŸ’¡ Check that agent_logic and agent_interface are available")

    print(f"\nğŸ“Š Architecture Benefits:")
    print(f"   â€¢ 60% fewer lines of code (2500+ â†’ 1028)")
    print(f"   â€¢ State-driven execution (more reliable)")
    print(f"   â€¢ Simplified tool configuration")
    print(f"   â€¢ Preserved all testing capabilities")
    print(f"   â€¢ Enhanced error reporting")