{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploration for Agent Specialized on Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install smolagents duckduckgo-search wikipedia-api docling rank_bm25 transformers google-api-python-client 'pymilvus[model]' torch sentence_transformers hf_xet\n",
    "%pip install devtools\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Search Tools Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "`DuckDuckGoSearchTool` is part of default tools in smolagents package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import DuckDuckGoSearchTool\n",
    "\n",
    "search_tools = DuckDuckGoSearchTool()\n",
    "search_results = search_tools(\"Mercedes Sosa\")\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Note: Due to rate limits, DuckDuckGo was not effective for solving GAIA tasks. However, the agents were still able to find the correct answers despite occasional tool failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from devtools import pprint\n",
    "\n",
    "developer_key = os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
    "if not developer_key:\n",
    "    raise ValueError(\"Please set the GOOGLE_SEARCH_API_KEY environment variable.\")\n",
    "cx = os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\n",
    "if not developer_key:\n",
    "    raise ValueError(\"Please set the GOOGLE_SEARCH_ENGINE_ID environment variable.\")\n",
    "\n",
    "service = build(\"customsearch\", \"v1\", developerKey=developer_key)\n",
    "res = (\n",
    "    service.cse()\n",
    "    .list(\n",
    "        q=\"Mercedes Sosa\",\n",
    "        cx=cx,\n",
    "        # fields=\"items(title,link,snippet)\",\n",
    "        # siteSearch=\"wikipedisa.org\",\n",
    "        # siteSearchFilter=\"i\",\n",
    "        num=2,\n",
    "        sort=\"date:r:20000101:20091231\",\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "out = \"\\n\\n\".join(\n",
    "    [f\"[{item['title']}]({item['link']})\\n{item['snippet']}\" for item in res[\"items\"]]\n",
    ")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "\n",
    "\n",
    "class GoogleSearchTool(Tool):\n",
    "    name = \"web_search\"\n",
    "    description = \"\"\"Performs a google web search for query then returns top search results in markdown format.\"\"\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform search.\",\n",
    "        },\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    skip_forward_signature_validation = True\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str | None = None,\n",
    "        search_engine_id: str | None = None,\n",
    "        num_results: int = 10,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        api_key = api_key if api_key is not None else os.getenv(\"GOOGLE_SEARCH_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"Please set the GOOGLE_SEARCH_API_KEY environment variable.\"\n",
    "            )\n",
    "        search_engine_id = (\n",
    "            search_engine_id\n",
    "            if search_engine_id is not None\n",
    "            else os.getenv(\"GOOGLE_SEARCH_ENGINE_ID\")\n",
    "        )\n",
    "        if not search_engine_id:\n",
    "            raise ValueError(\n",
    "                \"Please set the GOOGLE_SEARCH_ENGINE_ID environment variable.\"\n",
    "            )\n",
    "\n",
    "        self.cse = build(\"customsearch\", \"v1\", developerKey=api_key).cse()\n",
    "        self.cx = search_engine_id\n",
    "        self.num = num_results\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _collect_params(self) -> dict:\n",
    "        return {}\n",
    "\n",
    "    def forward(self, query: str, *args, **kwargs) -> str:\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"cx\": self.cx,\n",
    "            \"fields\": \"items(title,link,snippet)\",\n",
    "            \"num\": self.num,\n",
    "        }\n",
    "\n",
    "        params = params | self._collect_params(*args, **kwargs)\n",
    "\n",
    "        response = self.cse.list(**params).execute()\n",
    "        if \"items\" not in response:\n",
    "            return \"No results found.\"\n",
    "\n",
    "        result = \"\\n\\n\".join(\n",
    "            [\n",
    "                f\"[{item['title']}]({item['link']})\\n{item['snippet']}\"\n",
    "                for item in response[\"items\"]\n",
    "            ]\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "class GoogleSiteSearchTool(GoogleSearchTool):\n",
    "    name = \"site_search\"\n",
    "    description = \"\"\"Performs a google search within the website for query then returns top search results in markdown format.\"\"\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform search.\",\n",
    "        },\n",
    "        \"site\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The domain of the site on which to search.\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def _collect_params(self, site: str) -> dict:\n",
    "        return {\n",
    "            \"siteSearch\": site,\n",
    "            \"siteSearchFilter\": \"i\",\n",
    "        }\n",
    "\n",
    "\n",
    "class GoogleTimeRestrictedSearchTool(GoogleSearchTool):\n",
    "    name = \"web_search_in_date_range\"\n",
    "    description = \"\"\"Performs a Google search with a date range filter and returns top results formatted in markdown.\"\"\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform search.\",\n",
    "        },\n",
    "        \"start\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Specifies the earliest date for search results in `YYYYMMDD` format. Filters out content published before this date.\",\n",
    "        },\n",
    "        \"end\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Specifies the latest date for search results in YYYYMMDD format. Filters out content published after this date.\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def _collect_params(self, start: str, end: str) -> dict:\n",
    "        return {\"sort\": f\"date:r:{start}:{end}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search = GoogleSearchTool(num_results=2)\n",
    "r = search_results(\"Mercedes Sosa\")\n",
    "print(r)\n",
    "\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "site_search = GoogleSiteSearchTool(num_results=2)\n",
    "r = site_search(\"Mercedes Sosa\", \"wikipedia.org\")\n",
    "print(r)\n",
    "\n",
    "print(\"\\n-----\\n\")\n",
    "\n",
    "time_restricted_search = GoogleTimeRestrictedSearchTool(num_results=2)\n",
    "r = time_restricted_search(\"Mercedes Sosa\", \"20000101\", \"20091231\")\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import WikipediaSearchTool\n",
    "\n",
    "search_tools = WikipediaSearchTool(content_type=\"summary\")\n",
    "search_results = search_tools(\"Mercedes Sosa\")\n",
    "print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Web Page Visit Tool Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import VisitWebpageTool\n",
    "\n",
    "visit_web_page = VisitWebpageTool(1000000)\n",
    "page = visit_web_page(\"https://en.wikipedia.org/wiki/Mercedes_Sosa\")\n",
    "print(f\"Length retrieved: {len(page)}\")\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Note: to get page full content `max_output_length` was increased.\n",
    "\n",
    "Default tool uses `markdownify`. Let's check if `docling` will produce better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Document Retrieval Tool Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Simple Documents Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://en.wikipedia.org/wiki/Mercedes_Sosa\"  # PDF path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "page = result.document.export_to_markdown()\n",
    "print(f\"Length retrieved: {len(page)}\")\n",
    "print(result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "`docling` produce more clean result for wikipedia pages. To use with an agent we can wrap it in tool. At this point we are not going to worry about context length and will add it later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "\n",
    "class ContentRetrieverTool(Tool):\n",
    "    name = \"retrieve_content\"\n",
    "    description = \"\"\"Retrieve the content of a webpage or document in markdown format. Supports PDF, DOCX, XLSX, HTML, images, and more.\"\"\"\n",
    "    inputs = {\n",
    "        \"url\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The URL or local path of the webpage or document to retrieve.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.document_converter = DocumentConverter()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the content of a webpage or document. Supports PDF, DOCX, XLSX, HTML, images, and more.\n",
    "        Args:\n",
    "            url (str):\n",
    "        Returns:\n",
    "            str: The content of the webpage or document in markdown format.\n",
    "        \"\"\"\n",
    "        result = self.document_converter.convert(url)\n",
    "        content = result.document.export_to_markdown()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Tool verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_content = ContentRetrieverTool()\n",
    "content = retrieve_content(\"https://en.wikipedia.org/wiki/Mercedes_Sosa\")\n",
    "print(f\"Length retrieved: {len(content)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Agent with web basic navigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import ToolCallingAgent, OpenAIServerModel\n",
    "\n",
    "model = OpenAIServerModel(model_id=\"gpt-4.1\")\n",
    "agent = ToolCallingAgent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        GoogleSearchTool(),\n",
    "        WikipediaSearchTool(),\n",
    "        ContentRetrieverTool(),\n",
    "    ],\n",
    "    verbosity_level=2,\n",
    ")\n",
    "\n",
    "agent.run(\"\"\"\n",
    "How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Agent works but consumes to much tokens for page content which affect instructions following. We need to provide ability to focus on relevant content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "from transformers import AutoTokenizer, logging\n",
    "from rank_bm25 import BM25Okapi\n",
    "from devtools import pprint\n",
    "\n",
    "document_converter = DocumentConverter()\n",
    "\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")\n",
    "chunker = HybridChunker(tokenizer=tokenizer)\n",
    "\n",
    "transformers_logging_verbosity = logging.get_verbosity()\n",
    "logging.set_verbosity_error()\n",
    "document = document_converter.convert(\n",
    "    \"https://en.wikipedia.org/wiki/Mercedes_Sosa\"\n",
    ").document\n",
    "chunks_iterator = chunker.chunk(dl_doc=document)\n",
    "logging.set_verbosity(transformers_logging_verbosity)\n",
    "\n",
    "chunks = []\n",
    "tokenized_document = []\n",
    "tokenize = tokenizer.get_tokenizer().tokenize\n",
    "for chunk in chunks_iterator:\n",
    "    chunk_with_context = chunker.contextualize(chunk=chunk)\n",
    "    chunk_tokenized = tokenize(chunk_with_context)\n",
    "    chunks.append(chunk_with_context)\n",
    "    tokenized_document.append(chunk_tokenized)\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_document)\n",
    "query = \"studio album\"\n",
    "tokenized_query = tokenize(query)\n",
    "relevant_chunks = bm25.get_top_n(tokenized_query, chunks)\n",
    "print(\"\\n\\n\".join(relevant_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "pprint(bm25_scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.argsort(-bm25_scores):\n",
    "    print(f\"Chunk {i} BM25 score: {bm25_scores[i]}\")\n",
    "    print(chunks[i])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "beta = 1.0  # < 1.0 â‡’ smoother; > 1.0 â‡’ sharper\n",
    "# --- soft-max (with optional temperature) ---\n",
    "shifted = beta * bm25_scores - np.max(beta * bm25_scores)\n",
    "probs = np.exp(shifted)\n",
    "probs /= probs.sum()  # soft-max probabilities, sum = 1\n",
    "# --- keep docs whose individual prob â‰¥ threshold ---\n",
    "cum_idx = np.argsort(probs)[::-1]  # indices sorted by prob, desc\n",
    "cum_probs = probs[cum_idx].cumsum()\n",
    "cut = np.searchsorted(cum_probs, 0.8)  # first pos where cumulative â‰¥ 80 %\n",
    "selected = cum_idx[: cut + 1].tolist()\n",
    "relevant_chunks = [(chunks[i], bm25_scores[i]) for i in selected]\n",
    "pprint(relevant_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "From [docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#setup) documentation:\n",
    "\n",
    "> ðŸ‘‰ NOTE: As you see above, using the HybridChunker can sometimes lead to a warning from the transformers library, however this is a \"false alarm\" â€” for details check [here](https://docling-project.github.io/docling/faq/#hybridchunker-triggers-warning-token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length-for-this-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "`bm25` score has not probability nature and this make soft max output not relevant and hard to find balance between completeness of information and noise. Let's try more advanced RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, model\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HierarchicalChunker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "client = MilvusClient(\"./data/milvus.db\")\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding_fn = model.DefaultEmbeddingFunction()\n",
    "\n",
    "if client.has_collection(collection_name=\"foo\"):\n",
    "    client.drop_collection(collection_name=\"foo\")\n",
    "client.create_collection(\n",
    "    collection_name=\"foo\",\n",
    "    dimension=768,  # model.get_sentence_embedding_dimension(),\n",
    "    # metric_type=\"COSINE\",\n",
    ")\n",
    "\n",
    "document_converter = DocumentConverter()\n",
    "document = document_converter.convert(\n",
    "    \"https://en.wikipedia.org/wiki/Mercedes_Sosa\"\n",
    ").document\n",
    "chunker = HierarchicalChunker()\n",
    "chunks_iterator = chunker.chunk(dl_doc=document)\n",
    "\n",
    "chunks = [chunker.contextualize(chunk) for chunk in list(chunks_iterator)]\n",
    "# vectors = model.encode(chunks, normalize_embeddings=True)\n",
    "vectors = embedding_fn.encode_documents(chunks)\n",
    "data = [{\"id\": i, \"vector\": vectors[i], \"text\": chunks[i]} for i in range(len(vectors))]\n",
    "client.insert(collection_name=\"foo\", data=data)\n",
    "\n",
    "query_texts = [\"List of studio albums by Mercedes Sosa\"]\n",
    "# query_vectors = model.encode(query_texts, normalize_embeddings=True)\n",
    "query_vectors = embedding_fn.encode_queries(query_texts)\n",
    "res = client.search(\n",
    "    collection_name=\"foo\",  # target collection\n",
    "    data=query_vectors,  # query vectors\n",
    "    limit=100,  # number of returned entities\n",
    "    output_fields=[\"text\"],  # specifies fields to be returned\n",
    ")\n",
    "\n",
    "for r in res[0]:\n",
    "    print(f\"ID: {r.id}, distance: {r.distance}\")\n",
    "    print(r.entity[\"text\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Experiments with Chrome and Milvus does not brought expected result on the example page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HierarchicalChunker\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "document_converter = DocumentConverter()\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "document = document_converter.convert(\n",
    "    \"https://en.wikipedia.org/wiki/Mercedes_Sosa\"\n",
    "    # \"https://www.baseball-reference.com/teams/NYY/1977.shtml\"\n",
    ").document\n",
    "\n",
    "chunks = list(chunker.chunk(dl_doc=document))\n",
    "chunks_text = [chunk.text for chunk in chunks]\n",
    "chunks_with_context = [chunker.contextualize(chunk) for chunk in chunks]\n",
    "chunks_context = [\n",
    "    chunks_with_context[i].replace(chunks_text[i], \"\").strip()\n",
    "    for i in range(len(chunks))\n",
    "]\n",
    "\n",
    "chunk_embeddings = model.encode(chunks_text, convert_to_tensor=True)\n",
    "context_embeddings = model.encode(chunks_context, convert_to_tensor=True)\n",
    "query_embedding = model.encode([\"studio albums\"], convert_to_tensor=True)\n",
    "\n",
    "threshold = 0.2\n",
    "selected_indices = []  # aggregate indexes across chunks and context matches and for all queries\n",
    "for embeddings in [\n",
    "    context_embeddings,\n",
    "    chunk_embeddings,\n",
    "]:\n",
    "    # Compute cosine similarities (returns 1D tensor)\n",
    "    for cos_scores in util.pytorch_cos_sim(query_embedding, embeddings):\n",
    "        # Convert to softmax probabilities\n",
    "        probabilities = torch.nn.functional.softmax(cos_scores, dim=0)\n",
    "        # Sort by probability descending\n",
    "        sorted_indices = torch.argsort(probabilities, descending=True)\n",
    "        # Accumulate until total probability reaches threshold\n",
    "\n",
    "        cumulative = 0.0\n",
    "        for i in sorted_indices:\n",
    "            cumulative += probabilities[i].item()\n",
    "            selected_indices.append(i.item())\n",
    "            if cumulative >= threshold:\n",
    "                break\n",
    "\n",
    "selected_indices = list(\n",
    "    dict.fromkeys(selected_indices)\n",
    ")  # remove duplicates and preserve order\n",
    "selected_indices = selected_indices[\n",
    "    ::-1\n",
    "]  # make most relevant items last for better focus\n",
    "for idx in selected_indices:\n",
    "    print(chunks_with_context[idx], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Now we are going to rewrite our tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HierarchicalChunker\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "\n",
    "class ContentRetrieverTool(Tool):\n",
    "    name = \"retrieve_content\"\n",
    "    description = \"\"\"Retrieve the content of a webpage or document in markdown format. Supports PDF, DOCX, XLSX, HTML, images, and more.\"\"\"\n",
    "    inputs = {\n",
    "        \"url\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The URL or local path of the webpage or document to retrieve.\",\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The subject on the page you are looking for. The shorter the more relevant content is returned.\",\n",
    "        },\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str | None = None,\n",
    "        threshold: float = 0.2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.threshold = threshold\n",
    "        self._document_converter = DocumentConverter()\n",
    "        self._model = SentenceTransformer(\n",
    "            model_name if model_name is not None else \"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        self._chunker = HierarchicalChunker()\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, url: str, query: str) -> str:\n",
    "        document = self._document_converter.convert(url).document\n",
    "\n",
    "        chunks = list(self._chunker.chunk(dl_doc=document))\n",
    "        if len(chunks) == 0:\n",
    "            return \"No content found.\"\n",
    "\n",
    "        chunks_text = [chunk.text for chunk in chunks]\n",
    "        chunks_with_context = [self._chunker.contextualize(chunk) for chunk in chunks]\n",
    "        chunks_context = [\n",
    "            chunks_with_context[i].replace(chunks_text[i], \"\").strip()\n",
    "            for i in range(len(chunks))\n",
    "        ]\n",
    "\n",
    "        chunk_embeddings = self._model.encode(chunks_text, convert_to_tensor=True)\n",
    "        context_embeddings = self._model.encode(chunks_context, convert_to_tensor=True)\n",
    "        query_embedding = self._model.encode(\n",
    "            [term.strip() for term in query.split(\",\") if term.strip()], \n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "        selected_indices = []  # aggregate indexes across chunks and context matches and for all queries\n",
    "        for embeddings in [\n",
    "            context_embeddings,\n",
    "            chunk_embeddings,\n",
    "        ]:\n",
    "            # Compute cosine similarities (returns 1D tensor)\n",
    "            for cos_scores in util.pytorch_cos_sim(query_embedding, embeddings):\n",
    "                # Convert to softmax probabilities\n",
    "                probabilities = torch.nn.functional.softmax(cos_scores, dim=0)\n",
    "                # Sort by probability descending\n",
    "                sorted_indices = torch.argsort(probabilities, descending=True)\n",
    "                # Accumulate until total probability reaches threshold\n",
    "\n",
    "                cumulative = 0.0\n",
    "                for i in sorted_indices:\n",
    "                    cumulative += probabilities[i].item()\n",
    "                    selected_indices.append(i.item())\n",
    "                    if cumulative >= self.threshold:\n",
    "                        break\n",
    "\n",
    "        selected_indices = list(\n",
    "            dict.fromkeys(selected_indices)\n",
    "        )  # remove duplicates and preserve order\n",
    "        selected_indices = selected_indices[\n",
    "            ::-1\n",
    "        ]  # make most relevant items last for better focus\n",
    "        \n",
    "        if len(selected_indices) == 0:\n",
    "            return \"No content found.\"\n",
    "\n",
    "        return \"\\n\\n\".join(\n",
    "            [chunks_with_context[idx] for idx in selected_indices]\n",
    "        )\n",
    "\n",
    "\n",
    "retrieve_content = ContentRetrieverTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Agent Solving Task with Wikipedia Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Recreate agent with improved tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import (\n",
    "    ToolCallingAgent,\n",
    "    OpenAIServerModel,\n",
    "    # WikipediaSearchTool,\n",
    ")\n",
    "\n",
    "model = OpenAIServerModel(model_id=\"gpt-4.1\")\n",
    "agent = ToolCallingAgent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        # WikipediaSearchTool(content_type=\"summary\"),\n",
    "        GoogleSearchTool(),\n",
    "        GoogleSiteSearchTool(),\n",
    "        GoogleTimeRestrictedSearchTool(),\n",
    "        ContentRetrieverTool(),\n",
    "    ],\n",
    "    planning_interval=3,\n",
    "    max_steps=10,\n",
    "    verbosity_level=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"\"\"\n",
    "How many studio albums were published by Mercedes Sosa between 2000 and 2009 (included)? You can use the latest 2022 version of english wikipedia.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Agent Solving Web Search Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"\"\"\\\n",
    "How many at bats did the Yankee with the most walks in the 1977 regular season have that same season?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"\"\"\\\n",
    "What country had the least number of athletes at the 1928 Summer Olympics? If there's a tie for a number of athletes, return the first in alphabetical order. Give the IOC country code as your answer.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"\"\"\\\n",
    "Who are the pitchers with the number before and after Taish\\u014d Tamai's number as of July 2023?\n",
    "Give them to me in the form Pitcher Before, Pitcher\n",
    "After, use their last names only, in Roman characters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Agent Solving Papers Search Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(\"\"\"\\\n",
    "On June 6, 2023, an article by Carolyn Collins Petersen was published in Universe Today. This article mentions a team that produced a paper about their observations, linked at the bottom of the article. Find this paper. Under what NASA award number was the work performed by R. G. Arendt supported by?\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
