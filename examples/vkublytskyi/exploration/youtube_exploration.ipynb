{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploration of Working with Youtube Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "⚠️ Warning: This notebook utilizes unofficial Python libraries to access YouTube content. Retrieved content must not be stored or redistributed. All code and tools are intended strictly for educational and research purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install smolagents youtube-transcript-api yt-dlp av webvtt-py pysrt torch torchaudio transformers openai\n",
    "%pip install devtools\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## YouTube Video Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from devtools import pprint\n",
    "\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "snippets = ytt_api.fetch(\"1htKBjuUWec\")\n",
    "pprint(snippets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\n",
    "for s in snippets:\n",
    "    transcript += f\"[{s.start}]\\n{s.text}\\n[{s.start + s.duration}]\\n\"\n",
    "transcript = transcript.strip()\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import tool\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "\n",
    "\n",
    "@tool\n",
    "def transcribe_youtube_vide(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe a YouTube video.\n",
    "    Args:\n",
    "        url (str): The URL of the YouTube video.\n",
    "    Returns:\n",
    "        str: The transcript of the video.\n",
    "    \"\"\"\n",
    "    video_id = parse_qs(urlparse(url).query).get(\"v\", [None])[0]\n",
    "    if not video_id:\n",
    "        raise ValueError(\"Invalid YouTube URL.\")\n",
    "\n",
    "    snippets = YouTubeTranscriptApi.fetch(video_id)\n",
    "\n",
    "    transcript = \"\"\n",
    "    for s in snippets:\n",
    "        transcript += f\"[{s.start}]\\n{s.text}\\n[{s.start + s.duration}]\\n\"\n",
    "\n",
    "    return transcript.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## YouTube Video Image Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Download YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import yt_dlp\n",
    "import av\n",
    "import requests\n",
    "import pysrt\n",
    "import webvtt\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def stream_youtube_video(url, capture_interval_sec=5):\n",
    "    \"\"\"\n",
    "    Streams a YouTube video and captures one frame every `capture_interval_sec` seconds.\n",
    "\n",
    "    Args:\n",
    "        url (str): YouTube video URL.\n",
    "        capture_interval_sec (int or float): Time between captured frames in seconds.\n",
    "    \"\"\"\n",
    "    # Get direct video URL from yt-dlp\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"skip_download\": True,\n",
    "        \"format\": \"bestvideo[ext=mp4][height<=360]+bestaudio[ext=m4a]/best[height<=360]\",\n",
    "        \"forceurl\": True,\n",
    "        \"noplaylist\": True,\n",
    "        \"writesubtitles\": True,\n",
    "        \"writeautomaticsub\": True,\n",
    "        \"subtitleslangs\": [\"en\"],\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "\n",
    "    title = info.get(\"title\")\n",
    "    description = info.get(\"description\")\n",
    "    subtitles = info.get(\"subtitles\", {})\n",
    "    auto_captions = info.get(\"automatic_captions\", {})\n",
    "    caption_tracks = subtitles.get(\"en\") or auto_captions.get(\"en\") or []\n",
    "\n",
    "    structured_captions = []\n",
    "\n",
    "    srt_track = next((track for track in caption_tracks if track[\"ext\"] == \"srt\"), None)\n",
    "    vtt_track = next((track for track in caption_tracks if track[\"ext\"] == \"vtt\"), None)\n",
    "\n",
    "    if srt_track:\n",
    "        response = requests.get(srt_track[\"url\"])\n",
    "        response.raise_for_status()\n",
    "        srt_data = response.content.decode(\"utf-8\")\n",
    "\n",
    "        def to_sec(t):\n",
    "            return t.hours * 3600 + t.minutes * 60 + t.seconds + t.milliseconds / 1000\n",
    "\n",
    "        structured_captions = [\n",
    "            {\n",
    "                \"start\": to_sec(sub.start),\n",
    "                \"end\": to_sec(sub.end),\n",
    "                \"text\": sub.text.strip(),\n",
    "            }\n",
    "            for sub in pysrt.from_str(srt_data)\n",
    "        ]\n",
    "    if vtt_track:\n",
    "        response = requests.get(vtt_track[\"url\"])\n",
    "        response.raise_for_status()\n",
    "        vtt_data = response.text\n",
    "\n",
    "        vtt_file = StringIO(vtt_data)\n",
    "\n",
    "        def to_sec(t):\n",
    "            \"\"\"Convert 'HH:MM:SS.mmm' to float seconds\"\"\"\n",
    "            h, m, s = t.split(\":\")\n",
    "            s, ms = s.split(\".\")\n",
    "            return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "\n",
    "        for caption in webvtt.read_buffer(vtt_file):\n",
    "            structured_captions.append(\n",
    "                {\n",
    "                    \"start\": to_sec(caption.start),\n",
    "                    \"end\": to_sec(caption.end),\n",
    "                    \"text\": caption.text.strip(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    video_format = next(\n",
    "        f\n",
    "        for f in info[\"formats\"]\n",
    "        if f.get(\"vcodec\") != \"none\" and f.get(\"height\") == 360\n",
    "    )\n",
    "    video_url = video_format[\"url\"]\n",
    "\n",
    "    # Use ffmpeg to stream video to stdout\n",
    "    ffmpeg_cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        video_url,\n",
    "        \"-f\",\n",
    "        \"matroska\",  # container format\n",
    "        \"-\",\n",
    "    ]\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL\n",
    "    )\n",
    "\n",
    "    container = av.open(process.stdout)\n",
    "    stream = container.streams.video[0]\n",
    "    time_base = stream.time_base\n",
    "\n",
    "    next_capture_time = 0\n",
    "\n",
    "    imgs = []\n",
    "    for frame in container.decode(stream):\n",
    "        if frame.pts is None:\n",
    "            continue\n",
    "\n",
    "        timestamp = float(frame.pts * time_base)\n",
    "        if timestamp >= next_capture_time:\n",
    "            img = frame.to_image()  # PIL image\n",
    "            print(f\"IMG: {len(imgs)}\")\n",
    "            display(img)\n",
    "            imgs.append(img)\n",
    "            next_capture_time += capture_interval_sec\n",
    "\n",
    "    process.terminate()\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"captions\": structured_captions,\n",
    "        \"frames\": imgs,\n",
    "    }\n",
    "\n",
    "\n",
    "video = stream_youtube_video(\n",
    "    # \"https://www.youtube.com/watch?v=1htKBjuUWec\",\n",
    "    \"https://www.youtube.com/watch?v=L1vXCYZAYYM\",\n",
    "    capture_interval_sec=2,\n",
    ")\n",
    "\n",
    "print(video[\"title\"])\n",
    "print(video[\"description\"])\n",
    "for caption in video[\"captions\"]:\n",
    "    print(f\"{caption['start']} - {caption['end']}:\\n{caption['text']}\")\n",
    "imgs = video[\"frames\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Zero-shot Object Detection and Grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, pipeline\n",
    "\n",
    "checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\n",
    "\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "image = imgs[41].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import ImageDraw\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def process_with_owl(image, text_queries):\n",
    "    inputs = processor(text=text_queries, images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs, threshold=0.1, target_sizes=target_sizes\n",
    "        )[0]\n",
    "\n",
    "    # Display results\n",
    "\n",
    "    image_labeled = imgs[41].copy()\n",
    "    draw = ImageDraw.Draw(image_labeled)\n",
    "\n",
    "    scores = results[\"scores\"].tolist()\n",
    "    labels = results[\"labels\"].tolist()\n",
    "    boxes = results[\"boxes\"].tolist()\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "        draw.text(\n",
    "            (xmin, ymin), f\"{text_queries[label]}: {round(score, 2)}\", fill=\"white\"\n",
    "        )\n",
    "\n",
    "    print(f\"Text queries: {text_queries}\")\n",
    "    print(f\"Labels: {Counter(labels)}\")\n",
    "    display(image_labeled)\n",
    "\n",
    "\n",
    "process_with_owl(image, [\"emperor penguin\", \"adelie penguin\", \"petrel\"])\n",
    "process_with_owl(image, [\"penguin\", \"petrel\"])\n",
    "process_with_owl(image, [\"bird\"])\n",
    "process_with_owl(image, [\"bird species\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Challenges:\n",
    "- require predefined and crafted text queries (labels)\n",
    "- can label same object twice with different probability\n",
    "- hard to choose proper threshold for open questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Image Captioning and Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### BLIP2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# model_id = \"Salesforce/blip2-opt-2.7b\"\n",
    "model_id = \"Salesforce/blip2-flan-t5-xl\"\n",
    "# model_id = \"Salesforce/blip2-opt-2.7b-coco\"\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "processor = Blip2Processor.from_pretrained(model_id)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_id)\n",
    "model.eval().to(device)\n",
    "\n",
    "image = imgs[41].copy().convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def process_with_blip2(case, image, text=None):\n",
    "    inputs = processor(image, text=text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    if device.type == \"mps\":\n",
    "        inputs = {\n",
    "            k: v.to(torch.float32) if v.dtype.is_floating_point else v\n",
    "            for k, v in inputs.items()\n",
    "        }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "        # out = model.generate(**inputs, min_new_tokens=50, max_new_tokens=100)\n",
    "\n",
    "    print(f\"{case}: {text}|{processor.decode(out[0], skip_special_tokens=True)}\")\n",
    "\n",
    "\n",
    "display(image)\n",
    "process_with_blip2(\"Captioning\", image)\n",
    "process_with_blip2(\"Completion\", image, \"Here are birds of species \")\n",
    "process_with_blip2(\"VQA\", image, \"What bird species are here?\")\n",
    "process_with_blip2(\"VQA\", image, \"What birds are here?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Setting Up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    "    BlipForQuestionAnswering,\n",
    ")\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "blip_id = \"Salesforce/blip-image-captioning-base\"\n",
    "# blip_id = \"Salesforce/blip-image-captioning-large\"\n",
    "blip_vqa_id = \"Salesforce/blip-vqa-base\"\n",
    "# blip_vqa_id = \"Salesforce/blip-vqa-capfilt-large\"\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(blip_id)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(blip_id).to(device)\n",
    "blip_vqa_model = BlipForQuestionAnswering.from_pretrained(blip_vqa_id).to(device)\n",
    "\n",
    "image = imgs[41].copy().convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)\n",
    "\n",
    "\n",
    "def process_with_blip(model, case, image, text=None):\n",
    "    blip_inputs = blip_processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "    blip_inputs = {k: v.to(device) for k, v in blip_inputs.items()}\n",
    "    if device.type == \"mps\":\n",
    "        blip_inputs = {\n",
    "            k: v.to(torch.float32) if v.dtype.is_floating_point else v\n",
    "            for k, v in blip_inputs.items()\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            generated_ids = model.generate(**blip_inputs)\n",
    "            # generated_ids = model.generate(**blip_inputs, min_new_tokens=250, max_new_tokens=1000)\n",
    "        except TypeError:\n",
    "            print((f\"{case}: FAILED!\"))\n",
    "            return\n",
    "\n",
    "        response = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        print(f\"{case}: {text}|{'\\n'.join(response)}\")\n",
    "        return response\n",
    "\n",
    "\n",
    "process_with_blip(blip_model, \"BLIP Caption\", image)\n",
    "process_with_blip(blip_model, \"BLIP Completion\", image, \"Birds species here are \")\n",
    "process_with_blip(\n",
    "    blip_model,\n",
    "    \"BLIP Question Answering\",\n",
    "    image,\n",
    "    \"What birds are here?\",\n",
    ")\n",
    "process_with_blip(blip_vqa_model, \"BLIP VQA Caption\", image)\n",
    "process_with_blip(\n",
    "    blip_vqa_model, \"BLIP VQA Completion\", image, \"Birds species here are \"\n",
    ")\n",
    "process_with_blip(\n",
    "    blip_vqa_model,\n",
    "    \"BLIP VQA Question Answering\",\n",
    "    image,\n",
    "    \"What birds are here?\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Batch question processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_inputs = blip_processor(\n",
    "    images=[img.copy().convert(\"RGB\") for img in imgs],\n",
    "    text=[\"What birds are here?\"] * len(imgs),\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "blip_inputs = {k: v.to(device) for k, v in blip_inputs.items()}\n",
    "if device.type == \"mps\":\n",
    "    blip_inputs = {\n",
    "        k: v.to(torch.float32) if v.dtype.is_floating_point else v\n",
    "        for k, v in blip_inputs.items()\n",
    "    }\n",
    "generated_ids = blip_vqa_model.generate(**blip_inputs, max_new_tokens=50)\n",
    "\n",
    "labels = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "answer = \"\\n\".join(labels)\n",
    "print(f\"BLIP VQA Answer:\\n{answer}\")\n",
    "\n",
    "labels = list(set(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Objects Grounding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForZeroShotObjectDetection,\n",
    ")\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "dino_id = \"IDEA-Research/grounding-dino-base\"\n",
    "dino_processor = AutoProcessor.from_pretrained(dino_id)\n",
    "dino_model = AutoModelForZeroShotObjectDetection.from_pretrained(dino_id).to(device)\n",
    "\n",
    "image = imgs[41].copy().convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def process_with_dino(image, prompt=None):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    inputs = dino_processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = dino_model(**inputs)\n",
    "\n",
    "    target_sizes = torch.tensor([image.size[::-1]]).to(device)\n",
    "    results = dino_processor.post_process_grounded_object_detection(\n",
    "        outputs, target_sizes=target_sizes, threshold=0.3\n",
    "    )[0]\n",
    "\n",
    "    # Display the results\n",
    "\n",
    "    image_labeled = image.copy()\n",
    "    draw = ImageDraw.Draw(image_labeled)\n",
    "\n",
    "    scores = results[\"scores\"].tolist()\n",
    "    detected_labels = results[\"text_labels\"]\n",
    "    boxes = results[\"boxes\"]\n",
    "\n",
    "    print(f\"Objects Detected: {len(boxes)}\")\n",
    "    print(f\"Labels: {Counter(detected_labels)}\")\n",
    "    for box, score, label in zip(boxes, scores, detected_labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "        draw.text((xmin + 2, ymin), f\"{label}: {round(score, 2)}\", fill=\"red\")\n",
    "\n",
    "    display(image_labeled)\n",
    "    return boxes\n",
    "\n",
    "\n",
    "boxes = process_with_dino(image, \"bird\")\n",
    "boxes = process_with_dino(image, \"bird.\")\n",
    "try:\n",
    "    boxes = process_with_dino(image, \". \".join(labels) + \".\")\n",
    "except NameError:\n",
    "    pass  # BLIP VQA Answer not available\n",
    "boxes = process_with_dino(image, \"Emperor Penguin. Adelie Penguins. Giant Petrel.\")\n",
    "boxes = process_with_dino(image, \"emperor penguin. adelie penguins. giant petrel.\")\n",
    "boxes = process_with_dino(image, \"penguin. petrel.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Objects Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Setting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "clip_id = \"openai/clip-vit-base-patch32\"\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_id)\n",
    "clip_model = CLIPModel.from_pretrained(clip_id).to(device)\n",
    "\n",
    "image = imgs[41].copy().convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def process_with_clip(image, labels):\n",
    "    labeled_boxes = []\n",
    "\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
    "        cropped = image.crop((xmin, ymin, xmax, ymax))\n",
    "\n",
    "        clip_inputs = clip_processor(\n",
    "            text=labels, images=cropped, return_tensors=\"pt\", padding=True\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**clip_inputs)\n",
    "            logits = outputs.logits_per_image[0]\n",
    "            probs = logits.softmax(dim=0)\n",
    "            best_idx = probs.argmax().item()\n",
    "            label = labels[best_idx]\n",
    "            score = probs[best_idx].item()\n",
    "            labeled_boxes.append((box, label, score))\n",
    "\n",
    "    # Display the results\n",
    "\n",
    "    image_labeled = image.copy()\n",
    "    draw = ImageDraw.Draw(image_labeled)\n",
    "\n",
    "    labels_detected = []\n",
    "    for box, label, score in labeled_boxes:\n",
    "        labels_detected.append(label)\n",
    "        xmin, ymin, xmax, ymax = map(int, box.tolist())\n",
    "        draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "        draw.text((xmin, ymin), f\"{label}: {score:.2f}\", fill=\"white\")\n",
    "\n",
    "    print(f\"Labels: {Counter(labels_detected)}\")\n",
    "    display(image_labeled)\n",
    "\n",
    "\n",
    "print(\"Actual Labels:\")\n",
    "process_with_clip(image, [\"adelie penguin\", \"emperor penguin\", \"giant petrel\"])\n",
    "print(\"Generalized Labels:\")\n",
    "process_with_clip(image, [\"penguin\", \"petrel\"])\n",
    "try:\n",
    "    print(\"BLIP VQA Labels:\")\n",
    "    boxes = process_with_clip(image, labels)\n",
    "except NameError:\n",
    "    pass  # BLIP VQA Answer not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Video Comprehension with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "base64_frames = []\n",
    "for img in imgs:\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")\n",
    "    encoded = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    base64_frames.append(encoded)\n",
    "\n",
    "\n",
    "def process_with_openai(frames_subset):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": (\n",
    "                            \"\"\"\\\n",
    "    These are some frames from a video that I want to upload.\n",
    "\n",
    "    Video description:\n",
    "    Emperor Penguin Chicks and Adelie Penguins stand up to Giant Petrel\n",
    "    The emperor penguin chicks are left to fend for themselves and stand up against a giant petrel with the help of a feisty Adelie.\n",
    "    From our programme 'Spy in the Snow' for the BBC.\n",
    "\n",
    "    Name all bird species to be on camera simultaneously.\n",
    "    \"\"\"\n",
    "                        ),\n",
    "                    },\n",
    "                    *[\n",
    "                        {\n",
    "                            \"type\": \"input_image\",\n",
    "                            \"image_url\": f\"data:image/jpeg;base64,{frame}\",\n",
    "                        }\n",
    "                        for frame in frames_subset\n",
    "                    ],\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(response.output_text)\n",
    "\n",
    "\n",
    "print(\"Expected: only 2 bird species are visible at the same time.\")\n",
    "print(\"Model Response: \")\n",
    "process_with_openai(base64_frames[30:39])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Expected: all 3 bird species are visible at the same time.\")\n",
    "print(\"Model Response: \")\n",
    "process_with_openai(base64_frames[30:45])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## YouTube Tool Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "from openai import OpenAI\n",
    "from tools import SpeechRecognitionTool\n",
    "from io import BytesIO\n",
    "import yt_dlp\n",
    "import av\n",
    "import torchaudio\n",
    "import subprocess\n",
    "import requests\n",
    "import base64\n",
    "from devtools import pprint\n",
    "\n",
    "\n",
    "class YoutubeVideoTool(Tool):\n",
    "    name = \"youtube_video\"\n",
    "    description = \"\"\"Process the video and return the requested information from it.\"\"\"\n",
    "    inputs = {\n",
    "        \"url\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The URL of the YouTube video.\",\n",
    "        },\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The question to answer.\",\n",
    "        },\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_quality: int = 360,\n",
    "        frames_interval: int | float | None = 2,\n",
    "        chunk_duration: int | float | None = 20,\n",
    "        speech_recognition_tool: SpeechRecognitionTool | None = None,\n",
    "        client: OpenAI | None = None,\n",
    "        model_id: str = \"gpt-4.1-mini\",\n",
    "        debug: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.video_quality = video_quality\n",
    "        self.speech_recognition_tool = speech_recognition_tool\n",
    "        self.frames_interval = frames_interval\n",
    "        self.chunk_duration = chunk_duration\n",
    "\n",
    "        self.client = client or OpenAI()\n",
    "        self.model_id = model_id\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, url: str, query: str):\n",
    "        \"\"\"\n",
    "        Process the video and return the requested information.\n",
    "        Args:\n",
    "            url (str): The URL of the YouTube video.\n",
    "            query (str): The question to answer.\n",
    "        Returns:\n",
    "            str: Answer to the query.\n",
    "        \"\"\"\n",
    "        answer = \"\"\n",
    "        for chunk in self._split_video_into_chunks(url):\n",
    "            prompt = self._prompt(\n",
    "                chunk,\n",
    "                query,\n",
    "                answer,\n",
    "            )\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                input=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"input_text\",\n",
    "                                \"text\": prompt,\n",
    "                            },\n",
    "                            *[\n",
    "                                {\n",
    "                                    \"type\": \"input_image\",\n",
    "                                    \"image_url\": f\"data:image/jpeg;base64,{frame}\",\n",
    "                                }\n",
    "                                for frame in self._base64_frames(chunk[\"frames\"])\n",
    "                            ],\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "            answer = response.output_text\n",
    "            if self.debug:\n",
    "                print(\n",
    "                    f\"CHUNK {chunk['start']} - {chunk['end']}:\\n\\n{prompt}\\n\\nANSWER:\\n{answer}\"\n",
    "                )\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def _prompt(self, chunk, query, aggregated_answer):\n",
    "        prompt = [\n",
    "            f\"\"\"\\\n",
    "These are some frames of a video that I want to upload.\n",
    "I will ask a question about the entire video, but I will only last part of it.\n",
    "\n",
    "VIDEO TITLE:\n",
    "{chunk[\"title\"]}\n",
    "\n",
    "VIDEO DESCRIPTION:\n",
    "{chunk[\"description\"]}\n",
    "\n",
    "FRAMES SUBTITLES:\n",
    "{chunk[\"captions\"]}\"\"\"\n",
    "        ]\n",
    "\n",
    "        if aggregated_answer:\n",
    "            prompt.append(f\"\"\"\\\n",
    "Here is the answer to the same question based on the previous video parts:\n",
    "                          \n",
    "BASED ON PREVIOUS PARTS:\n",
    "{aggregated_answer}\"\"\")\n",
    "\n",
    "        prompt.append(f\"\"\"\\\n",
    "Now, please answer the question.\n",
    "                      \n",
    "QUESTION:\n",
    "{query}\"\"\")\n",
    "\n",
    "        return \"\\n\\n\".join(prompt)\n",
    "\n",
    "    def _split_video_into_chunks(\n",
    "        self, url: str, with_captions: bool = True, with_frames: bool = True\n",
    "    ):\n",
    "        video = self._process_video(\n",
    "            url, with_captions=with_captions, with_frames=with_frames\n",
    "        )\n",
    "        video_duration = video[\"duration\"]\n",
    "        chunk_duration = self.chunk_duration or video_duration\n",
    "\n",
    "        chunk_start = 0.0\n",
    "        while chunk_start < video_duration:\n",
    "            chunk_end = min(chunk_start + chunk_duration, video_duration)\n",
    "            chunk = self._get_video_chunk(video, chunk_start, chunk_end)\n",
    "            yield chunk\n",
    "            chunk_start += chunk_duration\n",
    "\n",
    "    def _get_video_chunk(self, video, start, end):\n",
    "        chunk_captions = [\n",
    "            c for c in video[\"captions\"] if c[\"start\"] <= end and c[\"end\"] >= start\n",
    "        ]\n",
    "        chunk_frames = [\n",
    "            f\n",
    "            for f in video[\"frames\"]\n",
    "            if f[\"timestamp\"] >= start and f[\"timestamp\"] <= end\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"title\": video[\"title\"],\n",
    "            \"description\": video[\"description\"],\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"captions\": \"\\n\".join([c[\"text\"] for c in chunk_captions]),\n",
    "            \"frames\": chunk_frames,\n",
    "        }\n",
    "\n",
    "    def _process_video(\n",
    "        self, url: str, with_captions: bool = True, with_frames: bool = True\n",
    "    ):\n",
    "        lang = \"en\"\n",
    "        info = self._get_video_info(url, lang)\n",
    "\n",
    "        if with_captions:\n",
    "            captions = self._extract_captions(\n",
    "                lang, info.get(\"subtitles\", {}), info.get(\"automatic_captions\", {})\n",
    "            )\n",
    "            if not captions and self.speech_recognition_tool:\n",
    "                audio_url = self._select_audio_format(info[\"formats\"])\n",
    "                audio = self._capture_audio(audio_url)\n",
    "                waveform, sample_rate = torchaudio.load(audio)\n",
    "                assert sample_rate == 16000\n",
    "                waveform_np = waveform.squeeze().numpy()\n",
    "                captions = self.speech_recognition_tool.transcribe(waveform_np)\n",
    "        else:\n",
    "            captions = []\n",
    "\n",
    "        if with_frames:\n",
    "            video_url = self._select_video_format(info[\"formats\"], 360)[\"url\"]\n",
    "            frames = self._capture_video_frames(video_url, self.frames_interval)\n",
    "        else:\n",
    "            frames = []\n",
    "\n",
    "        return {\n",
    "            \"id\": info[\"id\"],\n",
    "            \"title\": info[\"title\"],\n",
    "            \"description\": info[\"description\"],\n",
    "            \"duration\": info[\"duration\"],\n",
    "            \"captions\": captions,\n",
    "            \"frames\": frames,\n",
    "        }\n",
    "\n",
    "    def _get_video_info(self, url: str, lang: str):\n",
    "        ydl_opts = {\n",
    "            \"quiet\": True,\n",
    "            \"skip_download\": True,\n",
    "            \"format\": \"bestvideo[ext=mp4][height<=360]+bestaudio[ext=m4a]/best[height<=360]\",\n",
    "            \"forceurl\": True,\n",
    "            \"noplaylist\": True,\n",
    "            \"writesubtitles\": True,\n",
    "            \"writeautomaticsub\": True,\n",
    "            \"subtitlesformat\": \"vtt\",\n",
    "            \"subtitleslangs\": [lang],\n",
    "        }\n",
    "\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "\n",
    "        return info\n",
    "\n",
    "    def _extract_captions(self, lang, subtitles, auto_captions):\n",
    "        caption_tracks = subtitles.get(lang) or auto_captions.get(lang) or []\n",
    "\n",
    "        structured_captions = []\n",
    "\n",
    "        srt_track = next(\n",
    "            (track for track in caption_tracks if track[\"ext\"] == \"srt\"), None\n",
    "        )\n",
    "        vtt_track = next(\n",
    "            (track for track in caption_tracks if track[\"ext\"] == \"vtt\"), None\n",
    "        )\n",
    "\n",
    "        if srt_track:\n",
    "            import pysrt\n",
    "\n",
    "            response = requests.get(srt_track[\"url\"])\n",
    "            response.raise_for_status()\n",
    "            srt_data = response.content.decode(\"utf-8\")\n",
    "\n",
    "            def to_sec(t):\n",
    "                return (\n",
    "                    t.hours * 3600 + t.minutes * 60 + t.seconds + t.milliseconds / 1000\n",
    "                )\n",
    "\n",
    "            structured_captions = [\n",
    "                {\n",
    "                    \"start\": to_sec(sub.start),\n",
    "                    \"end\": to_sec(sub.end),\n",
    "                    \"text\": sub.text.strip(),\n",
    "                }\n",
    "                for sub in pysrt.from_str(srt_data)\n",
    "            ]\n",
    "        if vtt_track:\n",
    "            import webvtt\n",
    "            from io import StringIO\n",
    "\n",
    "            response = requests.get(vtt_track[\"url\"])\n",
    "            response.raise_for_status()\n",
    "            vtt_data = response.text\n",
    "\n",
    "            vtt_file = StringIO(vtt_data)\n",
    "\n",
    "            def to_sec(t):\n",
    "                \"\"\"Convert 'HH:MM:SS.mmm' to float seconds\"\"\"\n",
    "                h, m, s = t.split(\":\")\n",
    "                s, ms = s.split(\".\")\n",
    "                return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000\n",
    "\n",
    "            for caption in webvtt.read_buffer(vtt_file):\n",
    "                structured_captions.append(\n",
    "                    {\n",
    "                        \"start\": to_sec(caption.start),\n",
    "                        \"end\": to_sec(caption.end),\n",
    "                        \"text\": caption.text.strip(),\n",
    "                    }\n",
    "                )\n",
    "        return structured_captions\n",
    "\n",
    "    def _select_video_format(self, formats, video_quality):\n",
    "        video_format = next(\n",
    "            f\n",
    "            for f in formats\n",
    "            if f.get(\"vcodec\") != \"none\" and f.get(\"height\") == video_quality\n",
    "        )\n",
    "        return video_format\n",
    "\n",
    "    def _capture_video_frames(self, video_url, capture_interval_sec=None):\n",
    "        ffmpeg_cmd = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\",\n",
    "            video_url,\n",
    "            \"-f\",\n",
    "            \"matroska\",  # container format\n",
    "            \"-\",\n",
    "        ]\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "\n",
    "        container = av.open(process.stdout)\n",
    "        stream = container.streams.video[0]\n",
    "        time_base = stream.time_base\n",
    "\n",
    "        frames = []\n",
    "        next_capture_time = 0\n",
    "        for frame in container.decode(stream):\n",
    "            if frame.pts is None:\n",
    "                continue\n",
    "\n",
    "            timestamp = float(frame.pts * time_base)\n",
    "            if capture_interval_sec is None or timestamp >= next_capture_time:\n",
    "                frames.append(\n",
    "                    {\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"image\": frame.to_image(),  # PIL image\n",
    "                    }\n",
    "                )\n",
    "                if capture_interval_sec is not None:\n",
    "                    next_capture_time += capture_interval_sec\n",
    "\n",
    "        process.terminate()\n",
    "        return frames\n",
    "\n",
    "    def _base64_frames(self, frames):\n",
    "        base64_frames = []\n",
    "        for f in frames:\n",
    "            buffered = BytesIO()\n",
    "            f[\"image\"].save(buffered, format=\"JPEG\")\n",
    "            encoded = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            base64_frames.append(encoded)\n",
    "        return base64_frames\n",
    "\n",
    "    def _select_audio_format(self, formats):\n",
    "        audio_formats = [\n",
    "            f\n",
    "            for f in formats\n",
    "            if f.get(\"vcodec\") == \"none\"\n",
    "            and f.get(\"acodec\")\n",
    "            and f.get(\"acodec\") != \"none\"\n",
    "        ]\n",
    "\n",
    "        if not audio_formats:\n",
    "            raise ValueError(\"No valid audio-only formats found.\")\n",
    "\n",
    "        # Prefer m4a > webm, highest abr first\n",
    "        preferred_exts = [\"m4a\", \"webm\"]\n",
    "\n",
    "        def sort_key(f):\n",
    "            ext_score = (\n",
    "                preferred_exts.index(f[\"ext\"]) if f[\"ext\"] in preferred_exts else 99\n",
    "            )\n",
    "            abr = f.get(\"abr\") or 0\n",
    "            return (ext_score, -abr)\n",
    "\n",
    "        audio_formats.sort(key=sort_key)\n",
    "        return audio_formats[0][\"url\"]\n",
    "\n",
    "    def _capture_audio(self, audio_url) -> BytesIO:\n",
    "        audio_buffer = BytesIO()\n",
    "        ffmpeg_audio_cmd = [\n",
    "            \"ffmpeg\",\n",
    "            \"-i\",\n",
    "            audio_url,\n",
    "            \"-f\",\n",
    "            \"wav\",\n",
    "            \"-acodec\",\n",
    "            \"pcm_s16le\",  # Whisper prefers PCM\n",
    "            \"-ac\",\n",
    "            \"1\",  # Mono\n",
    "            \"-ar\",\n",
    "            \"16000\",  # 16kHz for Whisper\n",
    "            \"-\",\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(\n",
    "            ffmpeg_audio_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(\"ffmpeg failed:\\n\" + result.stderr.decode())\n",
    "\n",
    "        audio_buffer = BytesIO(result.stdout)\n",
    "        audio_buffer.seek(0)\n",
    "        return audio_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Verify the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "speech_recognition_tool = SpeechRecognitionTool()\n",
    "youtube_tool = YoutubeVideoTool(\n",
    "    client=client,\n",
    "    speech_recognition_tool=speech_recognition_tool,\n",
    "    frames_interval=3,\n",
    "    chunk_duration=60,\n",
    "    debug=True,\n",
    ")\n",
    "\n",
    "r = youtube_tool(\n",
    "    # url=\"https://www.youtube.com/watch?v=1htKBjuUWec\",\n",
    "    # query=\"What does Teal'c say in response to the question \\\"Isn't that hot?\\\"?\",\n",
    "    url=\"https://www.youtube.com/watch?v=L1vXCYZAYYM\",\n",
    "    query=\"what is the highest number of bird species to be on camera simultaneously?\",\n",
    ")\n",
    "print(f\"\\n\\nFINAL ANSWER:\\n{r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "## Agent with YouTube Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import ToolCallingAgent, OpenAIServerModel\n",
    "\n",
    "model = model = OpenAIServerModel(model_id=\"gpt-4.1\")\n",
    "\n",
    "speech_recognition_tool = SpeechRecognitionTool()\n",
    "youtube_tool = YoutubeVideoTool(\n",
    "    client=model.client,\n",
    "    speech_recognition_tool=speech_recognition_tool,\n",
    "    frames_interval=3,\n",
    "    chunk_duration=60,\n",
    ")\n",
    "agent = ToolCallingAgent(\n",
    "    model=model,\n",
    "    tools=[youtube_tool],\n",
    ")\n",
    "\n",
    "for t in [\n",
    "    \"In the video https://www.youtube.com/watch?v=L1vXCYZAYYM, what is the highest number of bird species to be on camera simultaneously?\",\n",
    "    \"Examine the video at https://www.youtube.com/watch?v=1htKBjuUWec. What does Teal'c say in response to the question \\\"Isn't that hot?\\\"\",\n",
    "]:\n",
    "    r = agent.run(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
