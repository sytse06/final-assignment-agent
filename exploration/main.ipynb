{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment Agent for HuggingFace Agents Course\n",
    "\n",
    "This notebooks implements agents to solve subset of [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark as described in [Unit 4. Final Project - Create, Test, and Certify Your Agent](https://huggingface.co/learn/agents-course/unit4) of [Agents Course](https://huggingface.co/learn/agents-course) from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of GAIA Challenges for Agent\n",
    "\n",
    "All questions used to validate agents are available through [Agent Evaluation API](https://agents-course-unit4-scoring.hf.space/docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each evaluation question has:\n",
    "- task_id - UUID of the task.\n",
    "- question - Text of the question to be answered by the agent.\n",
    "- level - complexity of the questions. For this assessment its always 1 but GAIA benchmark has 3 levels.\n",
    "- file_name - name of the file related to the task. Files can be fetched from Agent Evaluation API endpoint `/files/{task_id}` with `GET` request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### Gathering Information About Challenges\n",
    "\n",
    "First, we will gather the evaluation challenges and review them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare environment and install all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install tqdm openpyxl ipywidgets\n",
    "%pip install pandas tabulate matplotlib seaborn\n",
    "\n",
    "%pip install anthropic openai google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import utility libraries and set up environment variables from `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create simple models for handling evaluation tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_serializer\n",
    "from typing import Optional\n",
    "from yaml.dumper import SafeDumper\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class LiteralString(str):\n",
    "    \"\"\"Marker for YAML literal style.\"\"\"\n",
    "\n",
    "\n",
    "def _literal_str_representer(dumper, data):\n",
    "    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n",
    "\n",
    "\n",
    "SafeDumper.add_representer(LiteralString, _literal_str_representer)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TaskAttachment(BaseModel):\n",
    "    url: str\n",
    "    local_path: str\n",
    "    mime_type: str\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Task(BaseModel):\n",
    "    task_id: str\n",
    "    question: str\n",
    "    attachment: Optional[TaskAttachment] = None\n",
    "\n",
    "    @field_serializer(\"question\")\n",
    "    def _ser_lit_str_val(self, v: str):\n",
    "        # wrap the raw string into LiteralString\n",
    "        return LiteralString(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to analyze tasks and implement agents, we will download evaluation questions along with any related files.\n",
    "\n",
    "All data needed for developing agents will be stored in the `data` folder. This keeps the input data separate from any results or output produced by the agents.\n",
    "\n",
    "More specifically:\n",
    "- All evaluation task data will be placed inside the `data/tasks` folder.\n",
    "- A file named `tasks.yaml` will list all the tasks.\n",
    "- Each subfolder inside the tasks folder will store the data for one specific task. The name of each subfolder will match the `task_id` of that task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "TASKS_DIR = f\"{DATA_DIR}/tasks\"\n",
    "\n",
    "AGENT_EVALUATION_API = \"https://agents-course-unit4-scoring.hf.space/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to fetch tasks lists and associated attachments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_agent_evaluation_api(path: str, stream=True):\n",
    "    \"\"\"\n",
    "    Fetch data from the Agent Evaluation API using GET HTTP Method.\n",
    "    \"\"\"\n",
    "    response = requests.get(\n",
    "        urljoin(AGENT_EVALUATION_API, path),\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "        stream=stream,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_tasks_from_agent_evaluation_api():\n",
    "    \"\"\"\n",
    "    Fetch tasks from the Agent Evaluation API.\n",
    "    \"\"\"\n",
    "    response = get_from_agent_evaluation_api(\"questions\")\n",
    "    tasks = response.json()\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def get_task_attachment_from_agent_evaluation_api(task_id: str, local_path: str):\n",
    "    \"\"\"\n",
    "    Fetch task attachment from the Agent Evaluation API.\n",
    "    \"\"\"\n",
    "\n",
    "    with get_from_agent_evaluation_api(f\"files/{task_id}\", stream=True) as response:\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(local_path, \"wb\") as local_file:\n",
    "            with tqdm.wrapattr(\n",
    "                response.raw,\n",
    "                \"read\",\n",
    "                total=total_size,\n",
    "                unit=\"B\",\n",
    "                unit_scale=True,\n",
    "                desc=f\"ðŸ’¾ Downloading file for task {task_id}\",\n",
    "            ) as raw_read:\n",
    "                shutil.copyfileobj(raw_read, local_file)\n",
    "                tqdm.write(f\"âœ… {local_path}\")\n",
    "        return {\n",
    "            \"url\": response.url,\n",
    "            \"local_path\": os.path.realpath(local_path),\n",
    "            \"mime_type\": response.headers.get(\"content-type\", \"text/plain\"),\n",
    "        }\n",
    "\n",
    "    raise RuntimeError(f\"Unable to fetch task attachment for task_id: {task_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fetch the data from the Agent Evaluation API.\n",
    "\n",
    "**Note:** This step will delete the existing data folder and replace it with fresh data. Make sure to back up anything important before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TASKS_DIR, ignore_errors=True)\n",
    "os.makedirs(TASKS_DIR, exist_ok=True)\n",
    "\n",
    "tasks = []\n",
    "for task_data in get_tasks_from_agent_evaluation_api():\n",
    "    task_id = task_data[\"task_id\"]\n",
    "    question = task_data[\"question\"]\n",
    "    attachment = None\n",
    "    attachment_name = task_data[\"file_name\"]\n",
    "    if attachment_name:\n",
    "        attachment_data = get_task_attachment_from_agent_evaluation_api(\n",
    "            task_id, f\"{TASKS_DIR}/{task_id}/{attachment_name}\"\n",
    "        )\n",
    "        attachment = TaskAttachment(\n",
    "            url=attachment_data[\"url\"],\n",
    "            local_path=f\"file://./{os.path.relpath(attachment_data['local_path'], TASKS_DIR)}\",\n",
    "            mime_type=attachment_data[\"mime_type\"],\n",
    "        )\n",
    "\n",
    "    task = Task(task_id=task_id, question=question, attachment=attachment)\n",
    "    tasks.append(task)\n",
    "\n",
    "tqdm.write(f\"ðŸ’¡ {len(tasks)} tasks discovered!\")\n",
    "with open(f\"{TASKS_DIR}/tasks.yaml\", \"w\") as tasks_file:\n",
    "    yaml.safe_dump([task.model_dump() for task in tasks], tasks_file, sort_keys=False)\n",
    "    tqdm.write(f\"âœ…  {tasks_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to understand the tasks during analysis, weâ€™ll display the questions directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Image, Audio\n",
    "from ipywidgets import HBox, VBox, Layout, Output\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_attachment(att: TaskAttachment):\n",
    "    mime = att.mime_type\n",
    "    path = os.path.realpath(\n",
    "        os.path.join(TASKS_DIR, att.local_path.replace(\"file://\", \"\"))\n",
    "    )\n",
    "    if mime.startswith(\"text/x-python\"):\n",
    "        text = open(path, encoding=\"utf-8\").read()\n",
    "        display(Markdown(f\"```\\n{text}\\n```\"))\n",
    "    elif mime.startswith(\"image/\"):\n",
    "        # img_out = Output(layout=Layout(max_width='200px'))\n",
    "        # with img_out:\n",
    "        display(Image(filename=path))\n",
    "        # display(img_out)\n",
    "    elif mime.startswith(\"audio/\"):\n",
    "        display(Audio(filename=path))\n",
    "    elif os.path.splitext(path)[1] in (\".xls\", \".xlsx\"):\n",
    "        display(pd.read_excel(path))\n",
    "    else:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        display(Markdown(f\"```\\n{text}\\n```\"))\n",
    "\n",
    "\n",
    "def display_task(task: Task, extras: list = []):\n",
    "    # Create Markdown output widget\n",
    "    left_box = Output(layout=Layout(width=\"60%\", padding=\"0 20px 0 0\"))\n",
    "    with left_box:\n",
    "        display(Markdown(f\"**Question:**\\n\\n\\n\\n{task.question}\\n\\n**{task.task_id}**\"))\n",
    "        extra_boxes = []\n",
    "        for extra in extras:\n",
    "            extra_box = Output(\n",
    "                layout=Layout(\n",
    "                    border=\"1px solid #ccc\",\n",
    "                    padding=\"10px\",\n",
    "                    margin=\"10px 0px\",\n",
    "                    gap=\"20px\",\n",
    "                )\n",
    "            )\n",
    "            with extra_box:\n",
    "                display(extra)\n",
    "            extra_boxes.append(extra_box)\n",
    "        if len(extra_boxes) > 0:\n",
    "            display(VBox(extra_boxes))\n",
    "\n",
    "    # Create attachment output widget\n",
    "    right_box = Output(layout=Layout(width=\"40%\"))\n",
    "    if task.attachment is not None:\n",
    "        with right_box:\n",
    "            display_attachment(task.attachment)\n",
    "\n",
    "    # Combine into a horizontal card layout\n",
    "    card = HBox(\n",
    "        [left_box, right_box],\n",
    "        layout=Layout(padding=\"10px\", margin=\"10px 0px\", gap=\"20px\"),\n",
    "    )\n",
    "\n",
    "    display(card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tasks:\n",
    "    display_task(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for Agents\n",
    "\n",
    "To assess the gap between frontier models and a general-purpose agent, we will test the models on evaluation tasks without providing any tools. If a task includes an attachment, it will be referenced in the prompt using a public URL. If the chat API supports file attachments, the file will also be included directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    model: str\n",
    "    answer: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    reasoning_tokens: int = 0\n",
    "    latency: int\n",
    "\n",
    "    @field_serializer(\"answer\")\n",
    "    def _ser_lit_str_val(self, v: str):\n",
    "        return LiteralString(v)\n",
    "\n",
    "\n",
    "def latency_ms(start: float, end: float) -> int:\n",
    "    return int((end - start) * 1000)\n",
    "\n",
    "\n",
    "GAIA_BASE_PROMPT = \"\"\"\n",
    "You are a general AI assistant.\n",
    "\n",
    "I will ask you a question. Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER]. \n",
    "\n",
    "YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings. \n",
    "\n",
    "If you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unless specified otherwise. \n",
    "If you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise. \n",
    "If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\n",
    "\"\"\"\n",
    "\n",
    "assessment_results = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answered(task: Task, model) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the task has been answered by the model.\n",
    "    \"\"\"\n",
    "    if not assessment_results[task]:\n",
    "        return False\n",
    "    for a in assessment_results[task]:\n",
    "        if a.model == model:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks = [Task(\n",
    "#     task_id=\"test\",\n",
    "#     question=\"Who are you? One word only.\",\n",
    "#     attachment=None,\n",
    "# )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import base64\n",
    "import time\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "for model in [\n",
    "    \"claude-3-opus-latest\",\n",
    "    \"claude-3-7-sonnet-latest\",\n",
    "    \"claude-3-7-sonnet-latest-with-thinking\",\n",
    "    \"claude-3-5-haiku-latest\",\n",
    "]:\n",
    "    for t in tqdm(tasks, desc=f\"Answering questions with {model}\"):\n",
    "        if answered(t, model):\n",
    "            continue\n",
    "\n",
    "        msg_prts = [{\"type\": \"text\", \"text\": t.question}]\n",
    "        if t.attachment is not None:\n",
    "            mime = t.attachment.mime_type\n",
    "            path = os.path.realpath(\n",
    "                os.path.join(TASKS_DIR, t.attachment.local_path.replace(\"file://\", \"\"))\n",
    "            )\n",
    "            if os.path.splitext(path)[1] in (\".xls\", \".xlsx\"):\n",
    "                df = pd.read_excel(path)\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"<attached_spreadsheet>\\n{df.to_markdown()}\\n</attached_spreadsheet>\",\n",
    "                    }\n",
    "                )\n",
    "            elif mime.startswith(\"text/\"):\n",
    "                with open(path, encoding=\"utf-8\") as f:\n",
    "                    att_t = f.read()\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"<attachment_content>\\n{att_t}\\n</attachment_content>\",\n",
    "                    }\n",
    "                )\n",
    "            elif mime.startswith(\"image/\"):\n",
    "                with open(path, \"rb\") as img_f:\n",
    "                    b = base64.b64encode(img_f.read())\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": mime,\n",
    "                            \"data\": b.decode(\"utf-8\"),\n",
    "                        },\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                msg_prts.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"Attachment URL: {t.attachment.url}\"}\n",
    "                )\n",
    "\n",
    "        if model.endswith(\"-with-thinking\"):\n",
    "            start_t = time.perf_counter()\n",
    "            response = client.messages.create(\n",
    "                model=model.split(\"-with-thinking\")[0],\n",
    "                thinking={\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "                max_tokens=1024 * 2,\n",
    "                system=GAIA_BASE_PROMPT,\n",
    "                messages=[{\"role\": \"user\", \"content\": msg_prts}],\n",
    "            )\n",
    "            end_t = time.perf_counter()\n",
    "\n",
    "            r_text = \"\"\n",
    "            for r_prt in response.content:\n",
    "                if r_prt.type == \"thinking\":\n",
    "                    r_text += f\"THINKING:\\n{r_prt.thinking}\\n\"\n",
    "                elif r_prt.type == \"redacted_thinking\":\n",
    "                    r_text += \"THINKING: Claude's internal reasoning has been hidden for safety reasons.\\n\"\n",
    "                elif r_prt.type == \"text\":\n",
    "                    r_text += f\"ANSWER:\\n{r_prt.text}\"\n",
    "\n",
    "            assessment_results[t].append(\n",
    "                Answer(\n",
    "                    model=model,\n",
    "                    answer=r_text,\n",
    "                    input_tokens=response.usage.input_tokens,\n",
    "                    output_tokens=response.usage.output_tokens,\n",
    "                    reasoning_tokens=0,\n",
    "                    latency=latency_ms(start_t, end_t),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            start_t = time.perf_counter()\n",
    "            response = client.messages.create(\n",
    "                model=model,\n",
    "                max_tokens=1024,\n",
    "                system=GAIA_BASE_PROMPT,\n",
    "                messages=[{\"role\": \"user\", \"content\": msg_prts}],\n",
    "            )\n",
    "            end_t = time.perf_counter()\n",
    "\n",
    "            assessment_results[t].append(\n",
    "                Answer(\n",
    "                    model=model,\n",
    "                    answer=response.content[0].text,\n",
    "                    input_tokens=response.usage.input_tokens,\n",
    "                    output_tokens=response.usage.output_tokens,\n",
    "                    reasoning_tokens=0,\n",
    "                    latency=latency_ms(start_t, end_t),\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai as gemini\n",
    "from google.genai import types as gemini_types\n",
    "import time\n",
    "\n",
    "client = gemini.Client()\n",
    "config = gemini_types.GenerateContentConfig(system_instruction=GAIA_BASE_PROMPT)\n",
    "\n",
    "for model in [\"gemini-2.5-pro-preview-05-06\", \"gemini-2.0-flash\"]:\n",
    "    for t in tqdm(tasks, desc=f\"Answering questions with {model}\"):\n",
    "        if answered(t, model):\n",
    "            continue\n",
    "\n",
    "        msg_prts = [t.question]\n",
    "        if t.attachment is not None:\n",
    "            mime = t.attachment.mime_type\n",
    "            path = os.path.realpath(\n",
    "                os.path.join(TASKS_DIR, t.attachment.local_path.replace(\"file://\", \"\"))\n",
    "            )\n",
    "            if os.path.splitext(path)[1] in (\".xls\", \".xlsx\"):\n",
    "                df = pd.read_excel(path)\n",
    "                msg_prts.insert(\n",
    "                    0,\n",
    "                    gemini_types.Part.from_bytes(\n",
    "                        data=df.to_csv(index=False).encode(\"utf-8\"),\n",
    "                        mime_type=\"text/csv\",\n",
    "                    ),\n",
    "                )\n",
    "            elif mime.startswith(\"text/\"):\n",
    "                with open(path, encoding=\"utf-8\") as f:\n",
    "                    att_t = f.read()\n",
    "                msg_prts.insert(\n",
    "                    0, f\"<attachment_content>\\n{att_t}\\n</attachment_content>\"\n",
    "                )\n",
    "            elif (\n",
    "                mime.startswith(\"image/\")\n",
    "                or mime.startswith(\"audio/\")\n",
    "                or mime.startswith(\"video/\")\n",
    "            ):\n",
    "                with open(path, \"rb\") as f:\n",
    "                    b = f.read()\n",
    "                msg_prts.insert(\n",
    "                    0,\n",
    "                    gemini_types.Part.from_bytes(\n",
    "                        data=b,\n",
    "                        mime_type=mime,\n",
    "                    ),\n",
    "                )\n",
    "            else:\n",
    "                msg_prts.append(f\"Attachment URL: {t.attachment.url}\")\n",
    "\n",
    "        start_t = time.perf_counter()\n",
    "        response = client.models.generate_content(\n",
    "            model=model,\n",
    "            config=config,\n",
    "            contents=msg_prts,\n",
    "        )\n",
    "        end_t = time.perf_counter()\n",
    "\n",
    "        assessment_results[t].append(\n",
    "            Answer(\n",
    "                model=model,\n",
    "                answer=response.text,\n",
    "                input_tokens=response.usage_metadata.prompt_token_count,\n",
    "                output_tokens=response.usage_metadata.candidates_token_count,\n",
    "                reasoning_tokens=response.usage_metadata.thoughts_token_count\n",
    "                if response.usage_metadata.thoughts_token_count\n",
    "                else 0,\n",
    "                latency=latency_ms(start_t, end_t),\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "for model in [\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4.1\",\n",
    "    \"o4-mini\",\n",
    "    \"o1\",\n",
    "]:\n",
    "    for t in tqdm(tasks, desc=f\"Answering questions with {model}\"):\n",
    "        if answered(t, model):\n",
    "            continue\n",
    "\n",
    "        msg_prts = [{\"type\": \"input_text\", \"text\": t.question}]\n",
    "        if t.attachment is not None:\n",
    "            mime = t.attachment.mime_type\n",
    "            path = os.path.realpath(\n",
    "                os.path.join(TASKS_DIR, t.attachment.local_path.replace(\"file://\", \"\"))\n",
    "            )\n",
    "            if os.path.splitext(path)[1] in (\".xls\", \".xlsx\"):\n",
    "                df = pd.read_excel(path)\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": f\"<attached_spreadsheet>\\n{df.to_markdown()}\\n</attached_spreadsheet>\",\n",
    "                    }\n",
    "                )\n",
    "            elif mime.startswith(\"text/\"):\n",
    "                with open(path, encoding=\"utf-8\") as f:\n",
    "                    att_t = f.read()\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": f\"<attachment_content>\\n{att_t}\\n</attachment_content>\",\n",
    "                    }\n",
    "                )\n",
    "            elif mime.startswith(\"image/\"):\n",
    "                with open(path, \"rb\") as img_f:\n",
    "                    b = base64.b64encode(img_f.read())\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:{mime};base64,{b.decode('utf-8')}\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                msg_prts.append(\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": f\"Attachment URL: {t.attachment.url}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        start_t = time.perf_counter()\n",
    "        response = client.responses.create(\n",
    "            model=model,\n",
    "            instructions=GAIA_BASE_PROMPT,\n",
    "            input=[{\"role\": \"user\", \"content\": msg_prts}],\n",
    "        )\n",
    "        end_t = time.perf_counter()\n",
    "\n",
    "        assessment_results[t].append(\n",
    "            Answer(\n",
    "                model=model,\n",
    "                answer=response.output_text,\n",
    "                input_tokens=response.usage.input_tokens,\n",
    "                output_tokens=response.usage.output_tokens,\n",
    "                reasoning_tokens=response.usage.output_tokens_details.reasoning_tokens,\n",
    "                latency=latency_ms(start_t, end_t),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's persist LLM answers in `./data/tasks/answers.yaml`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{TASKS_DIR}/answers.yaml\", \"w\") as answers_file:\n",
    "    yaml.safe_dump(\n",
    "        {\n",
    "            str(t.task_id): [a.model_dump() for a in aa]\n",
    "            for t, aa in assessment_results.items()\n",
    "        },\n",
    "        answers_file,\n",
    "        sort_keys=False,\n",
    "    )\n",
    "    tqdm.write(f\"âœ…  {answers_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_tasks_from_yaml() -> list[Task]:\n",
    "    \"\"\"\n",
    "    Restore tasks from the YAML file.\n",
    "    \"\"\"\n",
    "    with open(f\"{TASKS_DIR}/tasks.yaml\", \"r\") as tasks_file:\n",
    "        tasks = yaml.safe_load(tasks_file)\n",
    "        return [Task.model_validate(t) for t in tasks]\n",
    "\n",
    "\n",
    "def restore_answers_from_yaml() -> dict[str, list[Answer]]:\n",
    "    \"\"\"\n",
    "    Restore answers from the YAML file.\n",
    "    \"\"\"\n",
    "    with open(f\"{TASKS_DIR}/answers.yaml\", \"r\") as answers_file:\n",
    "        answers = yaml.safe_load(answers_file)\n",
    "        return {\n",
    "            str(t): [Answer.model_validate(a) for a in aa] for t, aa in answers.items()\n",
    "        }\n",
    "\n",
    "\n",
    "tasks = restore_tasks_from_yaml()\n",
    "answers = restore_answers_from_yaml()\n",
    "assessment_results = defaultdict(list)\n",
    "for t in tasks:\n",
    "    assessment_results[t] = answers[t.task_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visual inspection of the answers run the following script:\n",
    "Warning: Response is huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tasks:\n",
    "    extras = []\n",
    "    for a in assessment_results[t]:\n",
    "        extras.append(Markdown(f\"**{a.model}**:\\n\\n{a.answer}\"))\n",
    "    display_task(t, extras=extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Answers Summary\n",
    "\n",
    "To decide on performance of pure LLM we are going to compare it with [ground truth](https://huggingface.co/datasets/gaia-benchmark/GAIA/blob/main/2023/validation/metadata.jsonl).\n",
    "\n",
    "`gaia-validation-metadata.jsonl` is not part of this repository. To run notebook please download it from [its original location](https://huggingface.co/datasets/gaia-benchmark/GAIA/blob/main/2023/validation/metadata.jsonl). Please do not publish it to not compromise benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "validation_dataset = defaultdict(str)\n",
    "with open(f\"{DATA_DIR}/gaia-validation-metadata.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for l in f:\n",
    "        if l.strip():\n",
    "            t = json.loads(l)\n",
    "            validation_dataset[t[\"task_id\"]] = t[\"Final answer\"]\n",
    "\n",
    "fa_marker = \"FINAL ANSWER:\"\n",
    "df_dt = []\n",
    "for t, aa in assessment_results.items():\n",
    "    gt = validation_dataset[t.task_id].strip()\n",
    "    for a in aa:\n",
    "        fa_pos = a.answer.rfind(fa_marker)\n",
    "        fa_exists = fa_pos != -1\n",
    "        fa = a.answer[fa_pos + len(fa_marker) :].strip() if fa_exists else \"\"\n",
    "\n",
    "        df_dt.append(\n",
    "            {\n",
    "                \"Task ID\": t.task_id,\n",
    "                \"Model\": a.model,\n",
    "                \"Answer\": fa,\n",
    "                \"Ground Truth\": gt,\n",
    "                \"Correct\": fa.casefold() == gt.casefold()\n",
    "                or re.sub(r\"\\s+\", \"\", fa.casefold())\n",
    "                == re.sub(r\"\\s+\", \"\", gt.casefold()),\n",
    "                \"Input Tokens\": a.input_tokens,\n",
    "                \"Output Tokens\": a.output_tokens,\n",
    "                \"Reasoning Tokens\": a.reasoning_tokens,\n",
    "                \"Total Tokens\": a.input_tokens + a.output_tokens + a.reasoning_tokens,\n",
    "                \"Latency\": a.latency,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(df_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display results in tabular format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "display(df)\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "tokens = df.pivot(index=\"Task ID\", columns=\"Model\", values=\"Total Tokens\")\n",
    "tokens.plot(kind=\"bar\", title=\"Tokens Used per Task\", figsize=(12, 6))\n",
    "plt.ylabel(\"Tokens\")\n",
    "plt.xlabel(\"Task ID\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "latency = df.pivot(index=\"Task ID\", columns=\"Model\", values=\"Latency\")\n",
    "latency.plot(kind=\"line\", marker=\"x\", title=\"Processing Time per Task\", figsize=(12, 6))\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "plt.xlabel(\"Task ID\")\n",
    "plt.grid(True)\n",
    "labels = latency.index.tolist()\n",
    "positions = range(len(labels))\n",
    "\n",
    "plt.xticks(ticks=positions, labels=labels, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "heatmap_data = df.pivot(index=\"Model\", columns=\"Task ID\", values=\"Correct\")\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    cmap=ListedColormap([\"red\", \"green\"]),\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"label\": \"Correct (1) / Incorrect (0)\"},\n",
    ")\n",
    "plt.title(\"Task-Level Accuracy Heatmap\")\n",
    "plt.xlabel(\"Task ID\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_stats = (\n",
    "    df.groupby(\"Model\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"Input Tokens\": \"mean\",\n",
    "            \"Output Tokens\": \"mean\",\n",
    "            \"Reasoning Tokens\": \"mean\",\n",
    "            \"Total Tokens\": \"mean\",\n",
    "            \"Latency\": \"mean\",\n",
    "            \"Correct\": \"sum\",\n",
    "        }\n",
    "    )\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"Input Tokens\": \"Mean Input Tokens\",\n",
    "            \"Output Tokens\": \"Mean Output Tokens\",\n",
    "            \"Reasoning Tokens\": \"Mean Reasoning Tokens\",\n",
    "            \"Total Tokens\": \"Mean Total Tokens\",\n",
    "            \"Latency\": \"Mean Latency (ms)\",\n",
    "            \"Correct\": \"Correct Answers\",\n",
    "        }\n",
    "    )\n",
    ")\n",
    "display(model_stats)\n",
    "\n",
    "for column in [\"Correct Answers\", \"Mean Latency (ms)\"]:\n",
    "    if \"Tokens\" in column:\n",
    "        continue\n",
    "    sorted_data = model_stats[column].sort_values(ascending=column == \"Correct Answers\")\n",
    "    sorted_data.plot(kind=\"barh\", figsize=(12, 6), title=column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "model_stats.loc[:, model_stats.columns.str.contains(\"Tokens\")].sort_values(\n",
    "    by=\"Mean Total Tokens\", ascending=True\n",
    ").plot(kind=\"barh\", figsize=(12, 6), title=\"Model Token Usage\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_stats = df.groupby(\"Task ID\").agg(\n",
    "    {\n",
    "        \"Input Tokens\": \"mean\",\n",
    "        \"Output Tokens\": \"mean\",\n",
    "        \"Reasoning Tokens\": \"mean\",\n",
    "        \"Total Tokens\": \"mean\",\n",
    "        \"Latency\": \"mean\",\n",
    "        \"Correct\": [\"sum\", \"count\"],\n",
    "    }\n",
    ")\n",
    "task_stats.columns = [\n",
    "    \"Mean Input Tokens\",\n",
    "    \"Mean Output Tokens\",\n",
    "    \"Mean Reasoning Tokens\",\n",
    "    \"Mean Total Tokens\",\n",
    "    \"Mean Latency (ms)\",\n",
    "    \"Correct Count\",\n",
    "    \"Total Submissions\",\n",
    "]\n",
    "task_stats[\"Task Complexity\"] = (\n",
    "    (task_stats[\"Total Submissions\"] - task_stats[\"Correct Count\"])\n",
    "    / task_stats[\"Total Submissions\"]\n",
    ") * 100\n",
    "task_stats = task_stats.drop(columns=[\"Total Submissions\", \"Correct Count\"])\n",
    "task_stats = task_stats.sort_values(\n",
    "    by=[\"Task Complexity\", \"Mean Total Tokens\"], ascending=[False, False]\n",
    ")\n",
    "display(task_stats)\n",
    "\n",
    "for column in [\"Task Complexity\", \"Mean Latency (ms)\"]:\n",
    "    task_stats[column].plot(kind=\"barh\", figsize=(12, 6), title=column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "task_stats.loc[:, task_stats.columns.str.contains(\"Tokens\")].plot(\n",
    "    kind=\"barh\", figsize=(12, 6), title=\"Performance on Task Summary\"\n",
    ")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display tasks in descending complexity order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def find_task_by_id(task_id: str) -> Task:\n",
    "    \"\"\"\n",
    "    Find a task by its ID.\n",
    "    \"\"\"\n",
    "    for t in tasks:\n",
    "        if t.task_id == task_id:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "\n",
    "for t_id in task_stats.index:\n",
    "    t = find_task_by_id(t_id)\n",
    "    if t is None:\n",
    "        continue\n",
    "    t_complexity = task_stats.loc[t_id, \"Task Complexity\"]\n",
    "    display_task(t, extras=[Markdown(f\"**Complexity Score**: {t_complexity:.2f}\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Implementation\n",
    "We are going to use HuggingFace [SmallAgents](https://huggingface.co/docs/smolagents/en/index) framework for quick prototyping of the solution. For exploration of different task types please see dedicated notebooks:\n",
    "- [Chess Exploration](./chess.ipynb)\n",
    "- [Exploration for Agent Specialized on Data Analysis](./data_analyst.ipynb)\n",
    "- [Exploration for Speech Recognition](./speech_recognition.ipynb)\n",
    "- [Exploration for Agent Specialized on Information Retrieval](./information_retrieval.ipynb)\n",
    "- [Exploration of Working with Youtube Content](./youtube_exploration.ipynb)\n",
    "- [Multi-agent Configuration Exploration](./multi_agent.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put agent developed based on explorations above to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import create_general_ai_agent\n",
    "from IPython.display import Image, display\n",
    "\n",
    "gaia = create_general_ai_agent(verbosity=1)\n",
    "display(Image(gaia.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"final-assignment-agent-v1\"\n",
    "agent_assessment_results = defaultdict(list)\n",
    "for t in tqdm(tasks, desc=f\"Answering questions with {model}\"):\n",
    "    if not answered(t, model):\n",
    "        try:\n",
    "            start_t = time.perf_counter()\n",
    "            response = gaia.invoke(\n",
    "                {\n",
    "                    \"task_id\": t.task_id,\n",
    "                    \"question\": t.question,\n",
    "                }\n",
    "            )\n",
    "            end_t = time.perf_counter()\n",
    "\n",
    "            answer = Answer(\n",
    "                model=model,\n",
    "                answer=\"\\n\".join(response[\"steps\"]),\n",
    "                input_tokens=0,  # not available\n",
    "                output_tokens=0,  # not available\n",
    "                reasoning_tokens=0,  # not available\n",
    "                latency=latency_ms(start_t, end_t),\n",
    "            )\n",
    "            assessment_results[t].append(answer)\n",
    "            agent_assessment_results[t].append(answer)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error: {t.task_id} not answered by {model}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's review agent answers and compare them with ground truth and answers of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gaia-validation-metadata.jsonl` is not part of this repository. To run notebook please download it from [its original location](https://huggingface.co/datasets/gaia-benchmark/GAIA/blob/main/2023/validation/metadata.jsonl). Please do not publish it to not compromise benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "validation_dataset = defaultdict(str)\n",
    "with open(f\"{DATA_DIR}/gaia-validation-metadata.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for l in f:\n",
    "        if l.strip():\n",
    "            t = json.loads(l)\n",
    "            validation_dataset[t[\"task_id\"]] = t[\"Final answer\"]\n",
    "\n",
    "fa_marker = \"FINAL ANSWER:\"\n",
    "df_dt = []\n",
    "for t, aa in assessment_results.items():\n",
    "    gt = validation_dataset[t.task_id].strip()\n",
    "    for a in aa:\n",
    "        fa_pos = a.answer.rfind(fa_marker)\n",
    "        fa_exists = fa_pos != -1\n",
    "        fa = a.answer[fa_pos + len(fa_marker) :].strip() if fa_exists else a.answer\n",
    "\n",
    "        df_dt.append(\n",
    "            {\n",
    "                \"Task ID\": t.task_id,\n",
    "                \"Model\": a.model,\n",
    "                \"Answer\": fa,\n",
    "                \"Ground Truth\": gt,\n",
    "                \"Semantically Correct\": fa.casefold() == gt.casefold()\n",
    "                or re.sub(r\"\\s+\", \"\", fa.casefold())\n",
    "                == re.sub(r\"\\s+\", \"\", gt.casefold()),\n",
    "                \"Correct\": fa == gt,\n",
    "                \"Input Tokens\": a.input_tokens,\n",
    "                \"Output Tokens\": a.output_tokens,\n",
    "                \"Reasoning Tokens\": a.reasoning_tokens,\n",
    "                \"Total Tokens\": a.input_tokens + a.output_tokens + a.reasoning_tokens,\n",
    "                \"Latency\": a.latency,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(df_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "# display(df)\n",
    "display(df[(df[\"Model\"] == \"final-assignment-agent-v1\") & (df[\"Correct\"] == False)])\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "# heatmap\n",
    "def classify(row):\n",
    "    if row[\"Correct\"]:\n",
    "        return 2\n",
    "    elif row[\"Semantically Correct\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "df[\"Correctness Level\"] = df.apply(classify, axis=1)\n",
    "\n",
    "model_accuracy = (\n",
    "    df[df[\"Correctness Level\"] == 2]\n",
    "    .groupby(\"Model\")\n",
    "    .size()\n",
    "    .div(df.groupby(\"Model\").size())\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "task_mean_correctness = (\n",
    "    df.groupby(\"Task ID\")[\"Correctness Level\"].mean().sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "heatmap_data = df.pivot(index=\"Model\", columns=\"Task ID\", values=\"Correctness Level\")\n",
    "heatmap_data = heatmap_data.loc[model_accuracy.index, task_mean_correctness.index]\n",
    "\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=False,\n",
    "    cmap=ListedColormap([\"red\", \"#50ad50\", \"green\"]),\n",
    "    vmin=0,\n",
    "    vmax=2,\n",
    "    cbar_kws={\"label\": \"Correctness\"},\n",
    "    linewidths=0.5,\n",
    "    linecolor=\"gray\",\n",
    ")\n",
    "\n",
    "colorbar = plt.gca().collections[0].colorbar\n",
    "colorbar.set_ticks([0.33, 1, 1.66])\n",
    "colorbar.set_ticklabels([\"Incorrect\", \"Semantically Correct\", \"Correct\"])\n",
    "\n",
    "plt.title(\"Task-Level Accuracy Heatmap\")\n",
    "plt.xlabel(\"Task ID\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# latency\n",
    "ax = latency.plot(\n",
    "    kind=\"line\",\n",
    "    marker=\"x\",\n",
    "    title=\"Processing Time per Task\",\n",
    "    figsize=(20, 6),\n",
    "    linewidth=1,\n",
    "    alpha=0.8,\n",
    "    colormap=\"tab10\",\n",
    ")\n",
    "\n",
    "for line, label in zip(ax.get_lines(), latency.columns):\n",
    "    if label == \"final-assignment-agent-v1\":\n",
    "        line.set_linewidth(3)\n",
    "        line.set_alpha(1.0)\n",
    "        line.set_zorder(10)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", title=\"Model\", borderaxespad=0)\n",
    "\n",
    "plt.subplots_adjust(right=0.8)\n",
    "labels = [task for task in latency.index.tolist()]\n",
    "plt.xticks(ticks=range(len(labels)), labels=labels, rotation=90)\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "plt.xlabel(\"Task ID\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from devtools import pprint\n",
    "\n",
    "answers = []\n",
    "fa_marker = \"FINAL ANSWER:\"\n",
    "for t, aa in assessment_results.items():\n",
    "    for a in aa:\n",
    "        if a.model == \"final-assignment-agent-v1\":       \n",
    "            fa_pos = a.answer.rfind(fa_marker)\n",
    "            fa_exists = fa_pos != -1\n",
    "            fa = a.answer[fa_pos + len(fa_marker) :].strip() if fa_exists else a.answer\n",
    "            answers.append({\n",
    "                \"task_id\": t.task_id,\n",
    "                \"submitted_answer\": fa,\n",
    "            })\n",
    "\n",
    "submission_data = {\n",
    "    \"username\": \"vkublytskyi\",\n",
    "    \"agent_code\": \"https://huggingface.co/spaces/vkublytskyi/Final_Assignment_Agent/tree/main\",\n",
    "    \"answers\": answers,\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://agents-course-unit4-scoring.hf.space/submit\", json=submission_data, timeout=60)\n",
    "response.raise_for_status()\n",
    "result_data = response.json()\n",
    "pprint(result_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
