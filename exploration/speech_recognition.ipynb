{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploration for Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install devtools\n",
    "%pip install torch transformers smolagents openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We are going to use `openai/whisper-large-v3-turbo` model from HuggingFace Hub for speech recognition.\n",
    "\n",
    "First, we are going to create valid configuration.\n",
    "\n",
    "Be sure `ffmpeg` is installed in the system (e.g. with `brew install fffmpeg`) or with `pip install static-ffmpeg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from devtools import pprint\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "model.to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    ")\n",
    "\n",
    "result = pipe(\n",
    "    \"data/tasks/1f975693-876d-457b-a649-393859e79bf3/1f975693-876d-457b-a649-393859e79bf3.mp3\"\n",
    ")\n",
    "\n",
    "print(f\"Text:\\n{result['text']}\")\n",
    "print(\"Chunks:\")\n",
    "pprint(result[\"chunks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Working with URL directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(\n",
    "    \"https://agents-course-unit4-scoring.hf.space/files/1f975693-876d-457b-a649-393859e79bf3\"\n",
    ")\n",
    "print(f\"Text:\\n{result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We are going to wrap this code as a smolagents tool. As chunks timestamp are relative to chunk we are going to fix chunk length and recalculate absolute timestamps so it may be matched with other data (e.g. vide frames). Also to clean up output we are going to suppress undesired messages and warnings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, logging\n",
    "import warnings\n",
    "\n",
    "\n",
    "class SpeechRecognitionTool(Tool):\n",
    "    name = \"speech_to_text\"\n",
    "    description = \"\"\"Transcribes speech from audio.\"\"\"\n",
    "\n",
    "    inputs = {\n",
    "        \"audio\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Path to the audio file to transcribe.\",\n",
    "        },\n",
    "        \"with_time_markers\": {\n",
    "            \"type\": \"boolean\",\n",
    "            \"description\": \"Whether to include timestamps in the transcription output. Each timestamp appears on its own line in the format [float, float], indicating the number of seconds elapsed from the start of the audio.\",\n",
    "            \"nullable\": True,\n",
    "            \"default\": False,\n",
    "        },\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    chunk_length_s = 30\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "        model_id = \"openai/whisper-large-v3-turbo\"\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch_dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "        model.to(device)\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        logging.set_verbosity_error()\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\",\n",
    "            category=FutureWarning,\n",
    "            message=r\".*The input name `inputs` is deprecated.*\",\n",
    "        )\n",
    "        cls.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=device,\n",
    "            chunk_length_s=cls.chunk_length_s,\n",
    "            return_timestamps=True,\n",
    "        )\n",
    "\n",
    "        return super().__new__(cls, *args, **kwargs)\n",
    "\n",
    "    def forward(self, audio: str, with_time_markers: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Transcribes speech from audio.\n",
    "\n",
    "        Args:\n",
    "            audio (str): Path to the audio file to transcribe.\n",
    "            with_time_markers (bool): Whether to include timestamps in the transcription output. Each timestamp appears on its own line in the format [float], indicating the number of seconds elapsed from the start of the audio.\n",
    "\n",
    "        Returns:\n",
    "            str: The transcribed text.\n",
    "        \"\"\"\n",
    "        result = self.pipe(audio)\n",
    "        if not with_time_markers:\n",
    "            return result[\"text\"].strip()\n",
    "\n",
    "        txt = \"\"\n",
    "        chunk_length_s = self.chunk_length_s\n",
    "        absolute_offset = 0.0\n",
    "        chunk_offset = 0.0\n",
    "        for chunk in result[\"chunks\"]:\n",
    "            timestamp_start = chunk[\"timestamp\"][0]\n",
    "            timestamp_end = chunk[\"timestamp\"][1]\n",
    "            if timestamp_start < chunk_offset:\n",
    "                absolute_offset += chunk_length_s\n",
    "                chunk_offset = timestamp_start\n",
    "            absolute_start = absolute_offset + timestamp_start\n",
    "\n",
    "            if timestamp_end < timestamp_start:\n",
    "                absolute_offset += chunk_length_s\n",
    "            absolute_end = absolute_offset + timestamp_end\n",
    "            chunk_offset = timestamp_end\n",
    "\n",
    "            chunk_text = chunk[\"text\"].strip()\n",
    "            if chunk_text:\n",
    "                txt += f\"[{absolute_start:.2f}]\\n{chunk_text}\\n[{absolute_end:.2f}]\\n\"\n",
    "        return txt.strip()\n",
    "\n",
    "\n",
    "speech_to_text = SpeechRecognitionTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Verify tool implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = speech_to_text(\n",
    "    audio=\"data/tasks/1f975693-876d-457b-a649-393859e79bf3/1f975693-876d-457b-a649-393859e79bf3.mp3\",\n",
    "    with_time_markers=True,\n",
    ")\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now time to verify if agent can use our tools in GAIA challenges environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import ToolCallingAgent, OpenAIServerModel\n",
    "from tools import GetAttachmentTool\n",
    "\n",
    "get_attachment = GetAttachmentTool()\n",
    "model = OpenAIServerModel(model_id=\"gpt-4.1\")\n",
    "agent = ToolCallingAgent(\n",
    "    model=model,\n",
    "    tools=[get_attachment, speech_to_text],\n",
    ")\n",
    "\n",
    "for task_with_audio_attachment in [\n",
    "    \"1f975693-876d-457b-a649-393859e79bf3\",\n",
    "    \"99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3\",\n",
    "]:\n",
    "    get_attachment.attachment_for(task_with_audio_attachment)\n",
    "    agent.run(\"Transcribe attached audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "And now let's verify full GAIA task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_with_audio_attachment, question in {\n",
    "    \"1f975693-876d-457b-a649-393859e79bf3\": \"\"\"\\\n",
    "Hi, I was out sick from my classes on Friday, so I'm trying to figure out what I need to study for my Calculus mid-term next week. My friend from class sent me an audio recording of Professor Willowbrook giving out the recommended reading for the test, but my headphones are broken :(\n",
    "Could you please listen to the recording for me and tell me the page numbers I'm supposed to go over? I've attached a file called Homework.mp3 that has the recording. Please provide just the page numbers as a comma-delimited list. And please provide the list in ascending order.\n",
    "\"\"\",\n",
    "    \"99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3\": \"\"\"\\\n",
    "Hi, I'm making a pie but I could use some help with my shopping list. I have everything I need for the crust, but I'm not sure about the filling. I got the recipe from my friend Aditi, but she left it as a voice memo and the speaker on my phone is buzzing so I can't quite make out what she's saying. Could you please listen to the recipe and list all of the ingredients that my friend described? I only want the ingredients for the filling, as I have everything I need to make my favorite pie crust. I've attached the recipe as Strawberry pie.mp3.\n",
    "In your response, please only list the ingredients, not any measurements. So if the recipe calls for \"a pinch of salt\" or \"two cups of ripe strawberries\" the ingredients on the list would be \"salt\" and \"ripe strawberries\".\n",
    "Please format your response as a comma separated list of ingredients. Also, please alphabetize the ingredients.\n",
    "\"\"\",\n",
    "}.items():\n",
    "    get_attachment.attachment_for(task_with_audio_attachment)\n",
    "    agent.run(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_speech_recognition (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
